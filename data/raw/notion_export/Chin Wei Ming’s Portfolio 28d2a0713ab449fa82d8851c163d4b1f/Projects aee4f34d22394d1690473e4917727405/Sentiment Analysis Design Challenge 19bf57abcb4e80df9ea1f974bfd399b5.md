# Sentiment Analysis Design Challenge

Timeline: September 9, 2024 â†’ December 20, 2024
Client: 50.040: Natural Language Processing, Prof. Lu Wei
My Role: Machine Learning Engineer, NLP Engineer
Deliverables: Exploration of NLP model architectures., Refinement for improved model performance
Tools: Gensim, Pandas, Python, Pytorch, Transformer (Hugging Face), Vast.ai
Document: ../NLP_final_project_report.pdf

![Screenshot 2025-02-15 at 12.19.25â€¯PM.png](Sentiment%20Analysis%20Design%20Challenge%2019bf57abcb4e80df9ea1f974bfd399b5/Screenshot_2025-02-15_at_12.19.25_PM.png)

## Project Overview

---

Sentiment analysis is a crucial Natural Language Processing (NLP) task used in various industries, from customer feedback analysis to financial market predictions. This project explores different deep learning architectures to enhance sentiment classification performance on theÂ **IMDB movie reviews dataset**, a widely used benchmark for binary sentiment analysis (positive vs. negative reviews). Our final model,Â **LoRA-RoBERTa**, achieved theÂ **highest performance**, with anÂ **F1-score of 0.9407**Â on the test set, demonstrating its ability to accurately capture nuanced sentiment in text. This project highlights the power ofÂ **transformer-based models**Â andÂ **efficient fine-tuning techniques**Â in advancing sentiment analysis.

## Approach & Methodology

---

To systematically improve sentiment classification performance, we experimented with various deep learning architectures, progressively enhancing model accuracy and interpretability.

### **Baseline Models: BiRNN & TextCNN**

We started withÂ **BiRNN (Bidirectional Recurrent Neural Network)**Â andÂ **TextCNN (Text-based Convolutional Neural Network)**Â as our baseline models.

- **BiRNN**Â captured sequential dependencies in text but had limitations in handling long-range dependencies.
- **TextCNN**, leveraging convolutional layers, excelled at recognizing local n-gram patterns but lacked sequential understanding.
- Performance Benchmark:Â **BiRNN achieved 86.48% accuracy, while TextCNN underperformed at 54.94% accuracy**, highlighting the need for further improvements.

### **Ensemble Model: Attention-Based Aggregation**

To leverage the strengths of both BiRNN and TextCNN, we implemented anÂ **ensemble model**Â using anÂ **attention-based weighted aggregation mechanism**:

- **Simple Ensemble:**Â Averaged predictions from BiRNN and TextCNN.
- **Attention-Based Ensemble:**Â Assigned dynamic weights to each modelâ€™s output based on contextual importance.
- **Custom Embeddings:**Â Replaced standard GloVe embeddings with domain-specific Word2Vec embeddings for richer feature representation.
- **Results:**Â This approach improved performance, achievingÂ **92.67% accuracy and an F1-score of 0.9273**, but we aimed for further optimization.

### **RoBERTa-BiLSTM: Contextual Embeddings + Sequential Understanding**

To incorporateÂ **pre-trained language models**, we introducedÂ **RoBERTa**Â to generate deep contextual embeddings andÂ **BiLSTM (Bidirectional Long Short-Term Memory)**Â to capture sequential dependencies.

- **RoBERTa provided robust sentence-level representations**, improving sentiment understanding.
- **BiLSTM processed embeddings bidirectionally**, retaining contextual dependencies across longer text spans.
- **Results:**Â This model further improved accuracy toÂ **93.92% with an F1-score of 0.9393**.

### **LoRA-RoBERTa: Efficient Transformer Fine-Tuning (Final Model)**

Given computational constraints, we fine-tunedÂ **RoBERTa-large**Â usingÂ **Low-Rank Adaptation (LoRA)**, a parameter-efficient tuning method.

- **Why LoRA?**Â Instead of updating all 355M parameters in RoBERTa-large, LoRA fine-tuned only a subset of trainable parameters, significantly reducing memory requirements while maintaining high performance.
- **Optimization Techniques:**Â UsedÂ **AdamW optimizer, gradient clipping, and linear learning rate scheduling**Â for stable training.
- **Final Results:**Â **LoRA-RoBERTa achieved the best performance with an F1-score of 0.9407**, outperforming all previous models.

## Results

---

Through systematic experimentation, we observed consistent improvements in sentiment classification performance as we transitioned from traditional deep learning models toÂ **transformer-based architectures**.

| Model | Accuracy | Precision | Recall | F1 score |
| --- | --- | --- | --- | --- |
| **BiRNN (Baseline)** | 86.48% | 83.85% | 90.34% | 86.97% |
| **TextCNN (Baseline)** | 54.94% | 52.61% | 98.88% | 68.67% |
| **Ensemble Model** | 92.67% | 91.90% | 93.58% | 92.73% |
| **RoBERTa-BiLSTM** | 93.92% | 93.68% | 94.18% | 93.93% |
| **LoRA-RoBERTa** | 94.08% | 94.25% | 93.88% | 94.07% |

ðŸ“ŒÂ **Key Takeaways:**

- **BiRNN performed well but struggled with long-range dependencies.**
- **TextCNN underperformed due to its lack of sequential context.**
- **The Ensemble model improved performance**Â by combining both architectures with attention-based weighting.
- **RoBERTa-BiLSTM outperformed the ensemble model**Â by leveraging contextual embeddings with sequential processing.
- **LoRA-RoBERTa achieved the best results, demonstrating the effectiveness of transformer-based fine-tuning.**

## Challenges

---

ðŸ”¹Â **Computational Constraints:**

- **Challenge:**Â TrainingÂ **RoBERTa-large**Â was computationally expensive.
- **Solution:**Â UsedÂ **Low-Rank Adaptation (LoRA)**Â to fine-tune only a subset of parameters, reducing memory usage while maintaining high performance.

ðŸ”¹Â **Dataset Preprocessing Issues:**

- **Challenge:**Â The IMDB dataset required extensive text cleaning and handling of imbalanced classes.
- **Solution:**Â ImplementedÂ **NLTK-based preprocessing**Â (lemmatization, stopword removal) andÂ **balanced batches** using PyTorch DataLoader.

ðŸ”¹Â **Hyperparameter Optimization:**

- **Challenge:**Â Finding the best learning rate, batch size, and hidden dimensions was time-intensive.
- **Solution:**Â UsedÂ **Grid Search**Â to optimize hyperparameters efficiently.