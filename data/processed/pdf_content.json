[
  {
    "id": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f_Final_Presentation",
    "source_file": "/Users/weimingchin/Desktop/weiming_chatbot/data/raw/notion_export/Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/Final_Presentation.pdf",
    "type": "pdf",
    "title": "CPF’s Internal",
    "category": "project_report",
    "raw_content": "\n--- Page 1 ---\nFORECASTERS PRESENT...\nCPF’s Internal\nDashboard\nFaith Lim, Chin Wei Ming, Janice Yohana,\nAyu Permata, Parigya Arya\n1/30\n\n--- Page 2 ---\nHave you sent an\nenquiry to CPF before?\n2/30\n\n--- Page 3 ---\nHow long did it take\nCPF to reply you?\n3/30\n\n--- Page 4 ---\nAbout our Client\nCPF’s Correspondence Department\nOperates within four teams\nEach team consists of 8 Customer Service Executives\n(CSEs), 10 Temporary Staff (temps), and 4 Customer\nService Associates (CSAs).\nSupervisor\nOnly Agents (CSE, CSA, Temps) reply to enquiries.\nEfficiently handles customer enquiries on CPF’S\nWebsite, ensuring prompt and effective resolution\nTeam Leader\nof customer issues.\nEach agent role handles different\nCSE Temps CSA\ncategories and number of enquiries.\n4/30\n\n--- Page 5 ---\nFraming\nthe Problem\nUnderstanding the Client’s\nPain Points & Problem Context\n5/30\n\n--- Page 6 ---\nA Glimpse into...\nCPF’s Correspondence Department\nYour target closure\nTODAY’S CASES STATUS\nrate for today is:\nCSE - 30\nOpen Balance 3200\nCSA - 20\nOUR GOAL IS TO REACH\nTemp - 15\nPredicted New Cases 500\nHEALTHY STATUS!\nHealth Status Amber\nCSE CSA Temp\nSUPERVISOR TEAM LEADER\n6/30\n\n--- Page 7 ---\nA Glimpse into...\nCPF’s Correspondence Department\nOne Week Later...\nTODAY’S CASES STATUS\nWHAT?? HOW DID Open Balance 5000\nOPEN BALANCE GET\nPredicted New Cases 400\nSO HIGH IN A WEEK?\nWE UNDERESTIMATED\nTHE NUMBER OF NEW Health Status Red\nCASES COMING IN...\nCSE CSA Temp\nSUPERVISOR TEAM LEADER\n7/30\n\n--- Page 8 ---\nA Glimpse into...\nSTATUS ACTION\nCSE/Team Leader: To clear a daily\nCPF’s Correspondence Department\naverage 35 or more cases\nRed\nCSA: To clear a daily average 25 or\nmore cases\nOKAY WELL, WE DON’T HAVE A\nCHOICE! WE WOULD NEED TO\nTODAY’S CASES STATUS\nSTEP IN TO CLOSE MORE CASES\nAND STEP UP OUR GAME!\nOpen Balance 5000\nPredicted New Cases 400\nHealth Status Red\nSUPERVISOR TEAM LEADER CSE CSA Temp\n8/30\n\n--- Page 9 ---\nA Glimpse into...\nCPF’s Correspondence Department\nI WISH THERE WAS A MORE\nEFFICIENT WAY TO FORECAST\nAND SIMULATE OPEN, CLOSED\nAND NEW CASES FOR BETTER\nMANPOWER ALLOCATION...\nSUPERVISOR\n9/30\n\n--- Page 10 ---\nSUPERVISOR’S MAIN PAIN POINTS\n“THE KEY PLAYER, MAIN DASHBOARD USER”\nTarget Case Closure Rates are being\nassigned manually.\nHMM, I WISH THERE WAS\nUnable to foresee the outcome of seasonal\nA WAY TO FORESEE\nsurges in enquiries, hence enquiry\nCASES COMING TO TAKE\nforecasting is inaccurate. ACTIONS AND MEET THE\nENQUIRY DEMAND....\nUnable to efficiently visualise forecasted\nvalues on a single platform.\n10/30\n\n--- Page 11 ---\nOur Main Goals\nImprove CPF’s current enquiry forecasting\n01\nsolution by accounting for factors aside from\nhistorical data and ensuring that it is dynamic.\nDemand management by simulating the\n02\nnumber of enquiries that each team and\nindividual agent need to answer to ensure\ndeadlines are met.\nVisualise the above requirements on the CPF\n03\nDashboard, using appropriate graphs.\n11/30\n\n--- Page 12 ---\nHow may we improve the current CPF\ndashboard to increase accuracy of\nenquiry forecasting and assist CPF\nSupervisors and Team Leaders in\noptimising Manpower Allocation?\n12/30\n\n--- Page 13 ---\nOur Systems\nDynamics Solution\nUsing a combination of time series\nforecasting and mathematical equations.\n13/30\n\n--- Page 14 ---\nOur Systems Dynamics Simulation Model\nLEGEND\nKey Variables Inflows & Outflows Predicted Variables Actual Variables\nRequirements &\nOutflow Math Equation\nConstraints\nSimulated Open\nBalance (Day 1)\nOptimal Enquiry Allocation\nNumber of\nthrough simulation of\nManpower Available\nSimulated Closed Target Case Closure Rate\nClosed Cases\nCases\nAgent Productivity\nIncoming Cases Incoming Cases\nSimulated Open\nBalance (Day 2)\nOpen Balance Open Balance\nForecasted Incoming\nCases\nExpected Change in\nEnquiry Volume\n14/30\n\n--- Page 15 ---\nForecasting\nPredicted Variables\nusing ARIMA\nDeriving an effective forecasting solution for\npredicted variables in our Systems\nDynamics Model\n15/30\n\n--- Page 16 ---\nOpen Balance Forecasting\nWeekly and Monthly\nHyperparameters: p = 7, d = 1, q = 1 Hyperparameters: p = 7, d = 1, q = 1\nMSE 10584.3, MAE 91.4, RMSE 102.9, Error % 3.7% MSE 31263.7, MAE 148.7, RMSE 176.8, Error % 5.6%\nTraining Data: 60 Days Training Data: 100 Days\n16/30\n\n--- Page 17 ---\nNew Cases Forecasting\nWeekly and Monthly\nHyperparameters: p = 7, d = 1, q = 2 Hyperparameters: p = 12, d = 1, q = 0\nMSE 1995.1, MAE 35.6, RMSE 44.7, Error % 7.4% MSE 6559.9, MAE 71.1, RMSE 81.2, Error % 16.5%\nTraining Data: 60 Days Training Data: 100 Days\n17/30\n\n--- Page 18 ---\nCSA Average Case Closure Forecasting\nDaily and Monthly\nHyperparameters: p = 12, d = 1, q = 0 Hyperparameters: p = 14, d = 1, q = 0\nMSE 12.7, MAE 2.8, RMSE 3.6, Error % 15.5% MSE 34.2, MAE 4.9, RMSE 5.9, Error % 27.1%\nTraining Data: 60 Days Training Data: 100 Days\n18/30\n\n--- Page 19 ---\nDynamic Forecasting\nARIMA Re-Runs when a new actual value is added\nAfter Hyperparameter Finetuning\nUse the previous 60 days to predict\nthe next 7 days\nUse the previous 100 days to predict\nthe next 30 days\nARIMA re-runs when a new actual\nvalue is added for that week/month\n19/30\n\n--- Page 20 ---\nSimulating\nKey Variables\nAn in-depth breakdown of the mathematical\nequations that run the simulation\n20/30\n\n--- Page 21 ---\nWeekly Simulation Demonstration\nToday’s Date: Monday\nTime: 9:00am Manpower Inputted: 8 CSEs, 4 CSAs, 10 Temps\nMon Tues Wed Thurs Fri Sat Sun\nOpening Balance 2730 2569 2429 2351 2439 2593 2715\nNew Cases 460 377 417 392 232 243 427\nTotal Cases Closed 472 366 11 93 422 367 349\nValues to be displayed on the dashboard\nLEGEND ARIMA Predicted Values Simulation Mathematical Results 21/30\n\n--- Page 22 ---\nWeekly Simulation Demonstration\nToday’s Date: Tuesday\nTime: 9:00am\nManpower Inputted: 8 CSEs, 4 CSAs, 10 Temps\nUsed for the next\n7 days’ calculation\nMon Tues Wed Thurs Fri Sat Sun Mon\nOpening Balance 1234 1022 785 651 905 1100 869 660\nNew Cases 112 235 232 265 289 191 159 135\nTotal Cases Closed 213 472 366 11 93 422 367 349\nValues to be displayed on the dashboard\nLEGEND Actual Values Inputted ARIMA Predicted Values Simulation Mathematical Results 22/30\n\n--- Page 23 ---\nWeekly Simulation Demonstration\nToday’s Date: Tuesday\nTime: 9:00am\nManpower Inputted: 8 CSEs, 4 CSAs, 10 Temps Surge in Cases Input: 2%\nUsed for the next\n7 days’ calculation\nMon Tues Wed Thurs Fri Sat Sun Mon\nOpening Balance 1234 1022 785 651 905 1100 869 660\nNew Cases 112 235 232 265 289 191 159 135\nTotal Cases Closed 213 472 366 11 93 422 367 349\nValues to be displayed on the dashboard\nLEGEND Actual Values Inputted ARIMA Predicted Values Simulation Mathematical Results 23/30\n\n--- Page 24 ---\nSimulating Targeted Case Closure Rate\nWeekly Simulation\n[(Today’s Actual Open Balance - Target End Balance on Day 7) +\nTotal Predicted New Cases for the remaining days of the week]\nTotal Cases to\n=\nClose per Day\nRemaining Days of the Week\nTotal Cases to Close per Day\nNo. of Cases to\n=\nClose per CSE\nNumber of CSEs + (Number of CSAs x 0.67) +\n(Number of Temps x 0.54)\nNo. of Cases to\n= No. of Cases to Close per CSE x 0.67\nClose per CSA\nNo. of Cases to\n= No. of Cases to Close per CSA x 0.54\nClose per Temp\nLEGEND Actual Values Inputted Target Values Inputted ARIMA Predicted Values 24/30\n\n--- Page 25 ---\nHold On To\nYour Seats...\nCheck out our project video!\n25/30\n\n--- Page 26 ---\n26/30\n\n--- Page 27 ---\nImprovements Made for CPF’s Business\nHow did our Client-Approved Solution improve CPF’s Processes?\nCPF Current Forecasting Model Our Improved Forecasting Model\nForecasting new cases purely on Forecasting CPF’s open balance, new cases and closed cases based\nhistorical data on both historical data & policy changes -> increased accuracy\nData Driven approach for Enquiry Allocation by simulating Target\nTarget Case Closure Rate assigned\nCase Closure Rate -> saves time & effort\nmanually through Supervisor’s\nintuition with no statistical basis\nData Driven Approach for Demand Management -> increased\nto prove their request\naccuracy and efficiency\nOpen Balance and Manpower Open Balance and Demand Management integrated into one\nAllocation done separately platform -> increased convenience & saves time\n27/30\n\n--- Page 28 ---\nA Live Demo of\nour Final Product\nPresenting CPF’s Improved\nInternal Dashboard\nStreamlit app link :\nsds-cpf-dashboard.streamlit.app\n28/30\n\n--- Page 29 ---\nThank you!\nHave any questions?\n29/30\n\n--- Page 30 ---\nReferences\nAwan, A. A. (n.d.). Time Series analysis: ARIMA Models in Python - KDNuggets. KDnuggets.\nhttps://www.kdnuggets.com/2023/08/times-series-analysis-arima-models-python.html\nBajaj, A. (2023, August 18). ARIMA & SARIMA: Real-World Time Series Forecasting. neptune.ai. https://neptune.ai/blog/arima-\nsarima-real-world-time-series-forecasting-guide\nHayes, A. (2024, April 6). Autoregressive Integrated Moving Average (ARIMA) Prediction Model. Investopedia.\nhttps://www.investopedia.com/terms/a/autoregressive-integrated-moving-average-arima.asp\n8.1 Simple exponential smoothing | Forecasting: Principles and Practice (3rd ed). (n.d.). https://otexts.com/fpp3/ses.html\nBrownlee, J. (2020, April 12). A gentle introduction to exponential smoothing for time series forecasting in Python.\nMachineLearningMastery.com. https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-\npython/\nExponential smoothing - statsmodels 0.15.0 (+251). (n.d.).\nhttps://www.statsmodels.org/dev/examples/notebooks/generated/exponential_smoothing.html\n30/30\n",
    "cleaned_content": "[PAGE BREAK]\nFORECASTERS PRESENT...\nCPF’s Internal\nDashboard\nFaith Lim, Chin Wei Ming, Janice Yohana,\nAyu Permata, Parigya Arya\n1/30\n[PAGE BREAK]\nHave you sent an\nenquiry to CPF before?\n2/30\n[PAGE BREAK]\nHow long did it take\nCPF to reply you?\n3/30\n[PAGE BREAK]\nAbout our Client\nCPF’s Correspondence Department\nOperates within four teams\nEach team consists of 8 Customer Service Executives\n(CSEs), 10 Temporary Staff (temps), and 4 Customer\nService Associates (CSAs).\nSupervisor\nOnly Agents (CSE, CSA, Temps) reply to enquiries.\nEfficiently handles customer enquiries on CPF’S\nWebsite, ensuring prompt and effective resolution\nTeam Leader\nof customer issues.\nEach agent role handles different\nCSE Temps CSA\ncategories and number of enquiries.\n4/30\n[PAGE BREAK]\nFraming\nthe Problem\nUnderstanding the Client’s\nPain Points & Problem Context\n5/30\n[PAGE BREAK]\nA Glimpse into...\nCPF’s Correspondence Department\nYour target closure\nTODAY’S CASES STATUS\nrate for today is:\nCSE - 30\nOpen Balance 3200\nCSA - 20\nOUR GOAL IS TO REACH\nTemp - 15\nPredicted New Cases 500\nHEALTHY STATUS!\nHealth Status Amber\nCSE CSA Temp\nSUPERVISOR TEAM LEADER\n6/30\n[PAGE BREAK]\nA Glimpse into...\nCPF’s Correspondence Department\nOne Week Later...\nTODAY’S CASES STATUS\nWHAT?? HOW DID Open Balance 5000\nOPEN BALANCE GET\nPredicted New Cases 400\nSO HIGH IN A WEEK?\nWE UNDERESTIMATED\nTHE NUMBER OF NEW Health Status Red\nCASES COMING IN...\nCSE CSA Temp\nSUPERVISOR TEAM LEADER\n7/30\n[PAGE BREAK]\nA Glimpse into...\nSTATUS ACTION\nCSE/Team Leader: To clear a daily\nCPF’s Correspondence Department\naverage 35 or more cases\nCSA: To clear a daily average 25 or\nmore cases\nOKAY WELL, WE DON’T HAVE A\nCHOICE! WE WOULD NEED TO\nTODAY’S CASES STATUS\nSTEP IN TO CLOSE MORE CASES\nAND STEP UP OUR GAME!\nOpen Balance 5000\nPredicted New Cases 400\nHealth Status Red\nSUPERVISOR TEAM LEADER CSE CSA Temp\n8/30\n[PAGE BREAK]\nA Glimpse into...\nCPF’s Correspondence Department\nI WISH THERE WAS A MORE\nEFFICIENT WAY TO FORECAST\nAND SIMULATE OPEN, CLOSED\nAND NEW CASES FOR BETTER\nMANPOWER ALLOCATION...\nSUPERVISOR\n9/30\n[PAGE BREAK]\nSUPERVISOR’S MAIN PAIN POINTS\n“THE KEY PLAYER, MAIN DASHBOARD USER”\nTarget Case Closure Rates are being\nassigned manually.\nHMM, I WISH THERE WAS\nUnable to foresee the outcome of seasonal\nA WAY TO FORESEE\nsurges in enquiries, hence enquiry\nCASES COMING TO TAKE\nforecasting is inaccurate. ACTIONS AND MEET THE\nENQUIRY DEMAND....\nUnable to efficiently visualise forecasted\nvalues on a single platform.\n10/30\n[PAGE BREAK]\nOur Main Goals\nImprove CPF’s current enquiry forecasting\nsolution by accounting for factors aside from\nhistorical data and ensuring that it is dynamic.\nDemand management by simulating the\nnumber of enquiries that each team and\nindividual agent need to answer to ensure\ndeadlines are met.\nVisualise the above requirements on the CPF\nDashboard, using appropriate graphs.\n11/30\n[PAGE BREAK]\nHow may we improve the current CPF\ndashboard to increase accuracy of\nenquiry forecasting and assist CPF\nSupervisors and Team Leaders in\noptimising Manpower Allocation?\n12/30\n[PAGE BREAK]\nOur Systems\nDynamics Solution\nUsing a combination of time series\nforecasting and mathematical equations.\n13/30\n[PAGE BREAK]\nOur Systems Dynamics Simulation Model\nLEGEND\nKey Variables Inflows & Outflows Predicted Variables Actual Variables\nRequirements &\nOutflow Math Equation\nConstraints\nSimulated Open\nBalance (Day 1)\nOptimal Enquiry Allocation\nNumber of\nthrough simulation of\nManpower Available\nSimulated Closed Target Case Closure Rate\nClosed Cases\nCases\nAgent Productivity\nIncoming Cases Incoming Cases\nSimulated Open\nBalance (Day 2)\nOpen Balance Open Balance\nForecasted Incoming\nCases\nExpected Change in\nEnquiry Volume\n14/30\n[PAGE BREAK]\nForecasting\nPredicted Variables\nusing ARIMA\nDeriving an effective forecasting solution for\npredicted variables in our Systems\nDynamics Model\n15/30\n[PAGE BREAK]\nOpen Balance Forecasting\nWeekly and Monthly\nHyperparameters: p = 7, d = 1, q = 1 Hyperparameters: p = 7, d = 1, q = 1\nMSE 10584.3, MAE 91.4, RMSE 102.9, Error % 3.7% MSE 31263.7, MAE 148.7, RMSE 176.8, Error % 5.6%\nTraining Data: 60 Days Training Data: 100 Days\n16/30\n[PAGE BREAK]\nNew Cases Forecasting\nWeekly and Monthly\nHyperparameters: p = 7, d = 1, q = 2 Hyperparameters: p = 12, d = 1, q = 0\nMSE 1995.1, MAE 35.6, RMSE 44.7, Error % 7.4% MSE 6559.9, MAE 71.1, RMSE 81.2, Error % 16.5%\nTraining Data: 60 Days Training Data: 100 Days\n17/30\n[PAGE BREAK]\nCSA Average Case Closure Forecasting\nDaily and Monthly\nHyperparameters: p = 12, d = 1, q = 0 Hyperparameters: p = 14, d = 1, q = 0\nMSE 12.7, MAE 2.8, RMSE 3.6, Error % 15.5% MSE 34.2, MAE 4.9, RMSE 5.9, Error % 27.1%\nTraining Data: 60 Days Training Data: 100 Days\n18/30\n[PAGE BREAK]\nDynamic Forecasting\nARIMA Re-Runs when a new actual value is added\nAfter Hyperparameter Finetuning\nUse the previous 60 days to predict\nthe next 7 days\nUse the previous 100 days to predict\nthe next 30 days\nARIMA re-runs when a new actual\nvalue is added for that week/month\n19/30\n[PAGE BREAK]\nSimulating\nKey Variables\nAn in-depth breakdown of the mathematical\nequations that run the simulation\n20/30\n[PAGE BREAK]\nWeekly Simulation Demonstration\nToday’s Date: Monday\nTime: 9:00 am Manpower Inputted: 8 CSEs, 4 CSAs, 10 Temps\nMon Tues Wed Thurs Fri Sat Sun\nOpening Balance 2730 2569 2429 2351 2439 2593 2715\nNew Cases 460 377 417 392 232 243 427\nTotal Cases Closed 472 366 11 93 422 367 349\nValues to be displayed on the dashboard\nLEGEND ARIMA Predicted Values Simulation Mathematical Results 21/30\n[PAGE BREAK]\nWeekly Simulation Demonstration\nToday’s Date: Tuesday\nTime: 9:00 am\nManpower Inputted: 8 CSEs, 4 CSAs, 10 Temps\nUsed for the next\n7 days’ calculation\nMon Tues Wed Thurs Fri Sat Sun Mon\nOpening Balance 1234 1022 785 651 905 1100 869 660\nNew Cases 112 235 232 265 289 191 159 135\nTotal Cases Closed 213 472 366 11 93 422 367 349\nValues to be displayed on the dashboard\nLEGEND Actual Values Inputted ARIMA Predicted Values Simulation Mathematical Results 22/30\n[PAGE BREAK]\nWeekly Simulation Demonstration\nToday’s Date: Tuesday\nTime: 9:00 am\nManpower Inputted: 8 CSEs, 4 CSAs, 10 Temps Surge in Cases Input: 2%\nUsed for the next\n7 days’ calculation\nMon Tues Wed Thurs Fri Sat Sun Mon\nOpening Balance 1234 1022 785 651 905 1100 869 660\nNew Cases 112 235 232 265 289 191 159 135\nTotal Cases Closed 213 472 366 11 93 422 367 349\nValues to be displayed on the dashboard\nLEGEND Actual Values Inputted ARIMA Predicted Values Simulation Mathematical Results 23/30\n[PAGE BREAK]\nSimulating Targeted Case Closure Rate\nWeekly Simulation\n[(Today’s Actual Open Balance - Target End Balance on Day 7) +\nTotal Predicted New Cases for the remaining days of the week]\nTotal Cases to\nClose per Day\nRemaining Days of the Week\nTotal Cases to Close per Day\nNo. of Cases to\nClose per CSE\nNumber of CSEs + (Number of CSAs x 0.67) +\n(Number of Temps x 0.54)\nNo. of Cases to\n= No. of Cases to Close per CSE x 0.67\nClose per CSA\nNo. of Cases to\n= No. of Cases to Close per CSA x 0.54\nClose per Temp\nLEGEND Actual Values Inputted Target Values Inputted ARIMA Predicted Values 24/30\n[PAGE BREAK]\nHold On To\nYour Seats...\nCheck out our project video!\n25/30\n[PAGE BREAK]\n26/30\n[PAGE BREAK]\nImprovements Made for CPF’s Business\nHow did our Client-Approved Solution improve CPF’s Processes?\nCPF Current Forecasting Model Our Improved Forecasting Model\nForecasting new cases purely on Forecasting CPF’s open balance, new cases and closed cases based\nhistorical data on both historical data & policy changes -> increased accuracy\nData Driven approach for Enquiry Allocation by simulating Target\nTarget Case Closure Rate assigned\nCase Closure Rate -> saves time & effort\nmanually through Supervisor’s\nintuition with no statistical basis\nData Driven Approach for Demand Management -> increased\nto prove their request\naccuracy and efficiency\nOpen Balance and Manpower Open Balance and Demand Management integrated into one\nAllocation done separately platform -> increased convenience & saves time\n27/30\n[PAGE BREAK]\nA Live Demo of\nour Final Product\nPresenting CPF’s Improved\nInternal Dashboard\nStreamlit app link :\nsds-cpf-dashboard.streamlit.app\n28/30\n[PAGE BREAK]\nThank you!\nHave any questions?\n29/30\n[PAGE BREAK]\nReferences\nAwan, A. A. (n.d.). Time Series analysis: ARIMA Models in Python - KDNuggets. KDnuggets.\nhttps://www.kdnuggets.com/2023/08/times-series-analysis-arima-models-python.html\nBajaj, A. (2023, August 18). ARIMA & SARIMA: Real-World Time Series Forecasting. neptune.ai. https://neptune.ai/blog/arima-\nsarima-real-world-time-series-forecasting-guide\nHayes, A. (2024, April 6). Autoregressive Integrated Moving Average (ARIMA) Prediction Model. Investopedia.\nhttps://www.investopedia.com/terms/a/autoregressive-integrated-moving-average-arima.asp\n8.1 Simple exponential smoothing | Forecasting: Principles and Practice (3 rd ed). (n.d.). https://otexts.com/fpp3/ses.html\nBrownlee, J. (2020, April 12). A gentle introduction to exponential smoothing for time series forecasting in Python.\nMachine Learning Mastery.com. https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-\npython/\nExponential smoothing - statsmodels 0.15.0 (+251). (n.d.).\nhttps://www.statsmodels.org/dev/examples/notebooks/generated/exponential_smoothing.html\n30/30",
    "sections": [
      {
        "title": "Content",
        "content": "FORECASTERS PRESENT...\nCPF’s Internal\nDashboard\n"
      },
      {
        "title": "Ayu Permata, Parigya Arya",
        "content": "1/30\n"
      },
      {
        "title": "Content",
        "content": "Have you sent an\nenquiry to CPF before?\n2/30\n"
      },
      {
        "title": "Content",
        "content": "How long did it take\nCPF to reply you?\n3/30\n"
      },
      {
        "title": "Content",
        "content": "About our Client\nCPF’s Correspondence Department\nOperates within four teams\nEach team consists of 8 Customer Service Executives\n(CSEs), 10 Temporary Staff (temps), and 4 Customer\n"
      },
      {
        "title": "Service Associates (CSAs).",
        "content": "Supervisor\n"
      },
      {
        "title": "Only Agents (CSE, CSA, Temps) reply to enquiries.",
        "content": "Efficiently handles customer enquiries on CPF’S\nWebsite, ensuring prompt and effective resolution\n"
      },
      {
        "title": "Team Leader",
        "content": "of customer issues.\nEach agent role handles different\nCSE Temps CSA\ncategories and number of enquiries.\n4/30\n"
      },
      {
        "title": "Content",
        "content": "Framing\nthe Problem\nUnderstanding the Client’s\n"
      },
      {
        "title": "Pain Points & Problem Context",
        "content": "5/30\n"
      },
      {
        "title": "Content",
        "content": "A Glimpse into...\nCPF’s Correspondence Department\nYour target closure\nTODAY’S CASES STATUS\nrate for today is:\nCSE - 30\n"
      },
      {
        "title": "Open Balance 3200",
        "content": "CSA - 20\n"
      },
      {
        "title": "OUR GOAL IS TO REACH",
        "content": "Temp - 15\n"
      },
      {
        "title": "Predicted New Cases 500",
        "content": "HEALTHY STATUS!\n"
      },
      {
        "title": "Health Status Amber",
        "content": "CSE CSA Temp\nSUPERVISOR TEAM LEADER\n6/30\n"
      },
      {
        "title": "Content",
        "content": "A Glimpse into...\nCPF’s Correspondence Department\n"
      },
      {
        "title": "One Week Later...",
        "content": "TODAY’S CASES STATUS\nWHAT?? HOW DID Open Balance 5000\n"
      },
      {
        "title": "Predicted New Cases 400",
        "content": "SO HIGH IN A WEEK?\n"
      },
      {
        "title": "WE UNDERESTIMATED",
        "content": "THE NUMBER OF NEW Health Status Red\nCASES COMING IN...\nCSE CSA Temp\nSUPERVISOR TEAM LEADER\n7/30\n"
      },
      {
        "title": "Content",
        "content": "A Glimpse into...\n"
      },
      {
        "title": "STATUS ACTION",
        "content": "CSE/Team Leader: To clear a daily\nCPF’s Correspondence Department\naverage 35 or more cases\nCSA: To clear a daily average 25 or\nmore cases\nOKAY WELL, WE DON’T HAVE A\nCHOICE! WE WOULD NEED TO\nTODAY’S CASES STATUS\nSTEP IN TO CLOSE MORE CASES\nAND STEP UP OUR GAME!\n"
      },
      {
        "title": "Health Status Red",
        "content": "SUPERVISOR TEAM LEADER CSE CSA Temp\n8/30\n"
      },
      {
        "title": "Content",
        "content": "A Glimpse into...\nCPF’s Correspondence Department\nI WISH THERE WAS A MORE\nEFFICIENT WAY TO FORECAST\nAND SIMULATE OPEN, CLOSED\nAND NEW CASES FOR BETTER\nMANPOWER ALLOCATION...\n"
      },
      {
        "title": "SUPERVISOR",
        "content": "9/30\n"
      },
      {
        "title": "Content",
        "content": "SUPERVISOR’S MAIN PAIN POINTS\n“THE KEY PLAYER, MAIN DASHBOARD USER”\n"
      },
      {
        "title": "Target Case Closure Rates are being",
        "content": "assigned manually.\nHMM, I WISH THERE WAS\nUnable to foresee the outcome of seasonal\n"
      },
      {
        "title": "A WAY TO FORESEE",
        "content": "surges in enquiries, hence enquiry\n"
      },
      {
        "title": "CASES COMING TO TAKE",
        "content": "forecasting is inaccurate. ACTIONS AND MEET THE\nENQUIRY DEMAND....\nUnable to efficiently visualise forecasted\nvalues on a single platform.\n10/30\n"
      },
      {
        "title": "Our Main Goals",
        "content": "Improve CPF’s current enquiry forecasting\nsolution by accounting for factors aside from\nhistorical data and ensuring that it is dynamic.\nDemand management by simulating the\nnumber of enquiries that each team and\nindividual agent need to answer to ensure\ndeadlines are met.\nVisualise the above requirements on the CPF\nDashboard, using appropriate graphs.\n11/30\n"
      },
      {
        "title": "Content",
        "content": "How may we improve the current CPF\ndashboard to increase accuracy of\nenquiry forecasting and assist CPF\nSupervisors and Team Leaders in\noptimising Manpower Allocation?\n12/30\n"
      },
      {
        "title": "Dynamics Solution",
        "content": "Using a combination of time series\nforecasting and mathematical equations.\n13/30\n"
      },
      {
        "title": "Key Variables Inflows & Outflows Predicted Variables Actual Variables",
        "content": "Requirements &\n"
      },
      {
        "title": "Outflow Math Equation",
        "content": "Constraints\n"
      },
      {
        "title": "Simulated Open",
        "content": "Balance (Day 1)\n"
      },
      {
        "title": "Optimal Enquiry Allocation",
        "content": "Number of\nthrough simulation of\n"
      },
      {
        "title": "Closed Cases",
        "content": "Cases\n"
      },
      {
        "title": "Simulated Open",
        "content": "Balance (Day 2)\n"
      },
      {
        "title": "Forecasted Incoming",
        "content": "Cases\n"
      },
      {
        "title": "Enquiry Volume",
        "content": "14/30\n"
      },
      {
        "title": "Content",
        "content": "Forecasting\n"
      },
      {
        "title": "Predicted Variables",
        "content": "using ARIMA\nDeriving an effective forecasting solution for\npredicted variables in our Systems\n"
      },
      {
        "title": "Dynamics Model",
        "content": "15/30\n"
      },
      {
        "title": "Open Balance Forecasting",
        "content": "Weekly and Monthly\nHyperparameters: p = 7, d = 1, q = 1 Hyperparameters: p = 7, d = 1, q = 1\nMSE 10584.3, MAE 91.4, RMSE 102.9, Error % 3.7% MSE 31263.7, MAE 148.7, RMSE 176.8, Error % 5.6%\n"
      },
      {
        "title": "Training Data: 60 Days Training Data: 100 Days",
        "content": "16/30\n"
      },
      {
        "title": "New Cases Forecasting",
        "content": "Weekly and Monthly\nHyperparameters: p = 7, d = 1, q = 2 Hyperparameters: p = 12, d = 1, q = 0\nMSE 1995.1, MAE 35.6, RMSE 44.7, Error % 7.4% MSE 6559.9, MAE 71.1, RMSE 81.2, Error % 16.5%\n"
      },
      {
        "title": "Training Data: 60 Days Training Data: 100 Days",
        "content": "17/30\n"
      },
      {
        "title": "Content",
        "content": "CSA Average Case Closure Forecasting\nDaily and Monthly\nHyperparameters: p = 12, d = 1, q = 0 Hyperparameters: p = 14, d = 1, q = 0\nMSE 12.7, MAE 2.8, RMSE 3.6, Error % 15.5% MSE 34.2, MAE 4.9, RMSE 5.9, Error % 27.1%\n"
      },
      {
        "title": "Training Data: 60 Days Training Data: 100 Days",
        "content": "18/30\n"
      },
      {
        "title": "Dynamic Forecasting",
        "content": "ARIMA Re-Runs when a new actual value is added\n"
      },
      {
        "title": "After Hyperparameter Finetuning",
        "content": "Use the previous 60 days to predict\nthe next 7 days\nUse the previous 100 days to predict\nthe next 30 days\nARIMA re-runs when a new actual\nvalue is added for that week/month\n19/30\n"
      },
      {
        "title": "Content",
        "content": "Simulating\n"
      },
      {
        "title": "Key Variables",
        "content": "An in-depth breakdown of the mathematical\nequations that run the simulation\n20/30\n"
      },
      {
        "title": "Weekly Simulation Demonstration",
        "content": "Today’s Date: Monday\nTime: 9:00 am Manpower Inputted: 8 CSEs, 4 CSAs, 10 Temps\n"
      },
      {
        "title": "Total Cases Closed 472 366 11 93 422 367 349",
        "content": "Values to be displayed on the dashboard\nLEGEND ARIMA Predicted Values Simulation Mathematical Results 21/30\n"
      },
      {
        "title": "Weekly Simulation Demonstration",
        "content": "Today’s Date: Tuesday\nTime: 9:00 am\n"
      },
      {
        "title": "Manpower Inputted: 8 CSEs, 4 CSAs, 10 Temps",
        "content": "Used for the next\n7 days’ calculation\n"
      },
      {
        "title": "Total Cases Closed 213 472 366 11 93 422 367 349",
        "content": "Values to be displayed on the dashboard\nLEGEND Actual Values Inputted ARIMA Predicted Values Simulation Mathematical Results 22/30\n"
      },
      {
        "title": "Weekly Simulation Demonstration",
        "content": "Today’s Date: Tuesday\nTime: 9:00 am\n"
      },
      {
        "title": "Manpower Inputted: 8 CSEs, 4 CSAs, 10 Temps Surge in Cases Input: 2%",
        "content": "Used for the next\n7 days’ calculation\n"
      },
      {
        "title": "Total Cases Closed 213 472 366 11 93 422 367 349",
        "content": "Values to be displayed on the dashboard\nLEGEND Actual Values Inputted ARIMA Predicted Values Simulation Mathematical Results 23/30\n"
      },
      {
        "title": "Weekly Simulation",
        "content": "[(Today’s Actual Open Balance - Target End Balance on Day 7) +\n"
      },
      {
        "title": "Total Cases to",
        "content": "Close per Day\n"
      },
      {
        "title": "Total Cases to Close per Day",
        "content": "No. of Cases to\nClose per CSE\nNumber of CSEs + (Number of CSAs x 0.67) +\n(Number of Temps x 0.54)\nNo. of Cases to\n= No. of Cases to Close per CSE x 0.67\nClose per CSA\nNo. of Cases to\n= No. of Cases to Close per CSA x 0.54\nClose per Temp\nLEGEND Actual Values Inputted Target Values Inputted ARIMA Predicted Values 24/30\n"
      },
      {
        "title": "Your Seats...",
        "content": "Check out our project video!\n25/30\n"
      },
      {
        "title": "Content",
        "content": "26/30\n"
      },
      {
        "title": "Improvements Made for CPF’s Business",
        "content": "How did our Client-Approved Solution improve CPF’s Processes?\nCPF Current Forecasting Model Our Improved Forecasting Model\nForecasting new cases purely on Forecasting CPF’s open balance, new cases and closed cases based\nhistorical data on both historical data & policy changes -> increased accuracy\n"
      },
      {
        "title": "Case Closure Rate -> saves time & effort",
        "content": "manually through Supervisor’s\nintuition with no statistical basis\n"
      },
      {
        "title": "Data Driven Approach for Demand Management -> increased",
        "content": "to prove their request\naccuracy and efficiency\n"
      },
      {
        "title": "Open Balance and Manpower Open Balance and Demand Management integrated into one",
        "content": "Allocation done separately platform -> increased convenience & saves time\n27/30\n"
      },
      {
        "title": "Content",
        "content": "A Live Demo of\nour Final Product\nPresenting CPF’s Improved\n"
      },
      {
        "title": "Internal Dashboard",
        "content": "Streamlit app link :\nsds-cpf-dashboard.streamlit.app\n28/30\n"
      },
      {
        "title": "Content",
        "content": "Thank you!\nHave any questions?\n29/30\n"
      },
      {
        "title": "Content",
        "content": "References\nAwan, A. A. (n.d.). Time Series analysis: ARIMA Models in Python - KDNuggets. KDnuggets.\nhttps://www.kdnuggets.com/2023/08/times-series-analysis-arima-models-python.html\nBajaj, A. (2023, August 18). ARIMA & SARIMA: Real-World Time Series Forecasting. neptune.ai. https://neptune.ai/blog/arima-\nsarima-real-world-time-series-forecasting-guide\nHayes, A. (2024, April 6). Autoregressive Integrated Moving Average (ARIMA) Prediction Model. Investopedia.\nhttps://www.investopedia.com/terms/a/autoregressive-integrated-moving-average-arima.asp\n8.1 Simple exponential smoothing | Forecasting: Principles and Practice (3 rd ed). (n.d.). https://otexts.com/fpp3/ses.html\nBrownlee, J. (2020, April 12). A gentle introduction to exponential smoothing for time series forecasting in Python.\n"
      },
      {
        "title": "Machine Learning Mastery.com. https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-",
        "content": "python/\nExponential smoothing - statsmodels 0.15.0 (+251). (n.d.).\nhttps://www.statsmodels.org/dev/examples/notebooks/generated/exponential_smoothing.html\n30/30\n"
      }
    ],
    "metadata": {
      "title": "CPF’s Internal",
      "category": "project_report",
      "file_name": "Final_Presentation",
      "relative_path": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/Final_Presentation.pdf",
      "page_count": 31,
      "project_name": null,
      "file_size": 5236364,
      "last_modified": 1749024948.3369808
    },
    "word_count": 1439,
    "page_count": 31
  },
  {
    "id": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f_Final_Report",
    "source_file": "/Users/weimingchin/Desktop/weiming_chatbot/data/raw/notion_export/Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/Final_Report.pdf",
    "type": "pdf",
    "title": "Final Technical Report",
    "category": "project_report",
    "raw_content": "\n--- Page 1 ---\n60.006: Spatial Design Studio\nFinal Technical Report\nUrban Redevelopment Authority x SUTD\nAditya Kumar 1006300\nCaitlin Chiang 1006537\nChin Wei Ming 1006264\nTimothy Wee 1005545\nWang Zixuan 1006391\n\n--- Page 2 ---\nTable of Contents\nExecutive Summary 3\nProject Overview 4\n1.1 Company Introduction 4\n1.2 Public Engagement Session 4\n1.3 Proposed Plan 5\nResearch 6\n2.1 Interview with URA 6\n2.2 Conventional Approach 7\n2.3 URA AI Engagement Framework 8\n2.4 Problem Scoping 10\n2.5 Problem Statement 10\n2.6 Proposed Approach 11\nDesign and Development of Application 13\n3.1 System Architecture 13\n3.2 Admin Interface (Create) 15\n3.3 User Interface 20\n3.4 Admin Interface (View & Analyze Data) 28\n3.5 Artificial Intelligence Models 37\nEngagement Session and Results 44\n4.1 Choosing the Site for Engagement 44\n4.2 Choosing of Participants for the Engagement 44\n4.3 Hosting of the Engagement 44\n4.4 Data Collection 45\n4.5 Results Analysis 47\n4.6 Additional Findings 50\nObservations 50\nInterviews 50\n4.7 Future Work from Findings 51\nConclusion 52\n\n--- Page 3 ---\nExecutive Summary\nThe Urban Redevelopment Authority (URA) in Singapore aims to enhance urban planning\nthrough inclusive public engagement. This project introduces an innovative AI-driven\napplication to streamline and enrich engagement sessions. The application provides tools for\nvisualizing public feedback as actionable design inputs, bridging the gap between traditional\nmanual processes and modern digital frameworks.\nThe system integrates generative AI technologies for tasks like inpainting, image generation,\nprompt upscaling, and topic categorization. It supports both users and administrators by offering\ninteractive interfaces for inputting ideas, analyzing responses, and visualizing outcomes. Testing\ninvolved real-world participant trials comparing conventional and AI-based workflows,\ndemonstrating improved satisfaction, engagement, and output quality with the AI approach.\nKey results include:\n● Enhanced participant satisfaction and engagement with the AI workflow.\n● Simplified and accelerated data processing through integrated AI pipelines.\n● Increased transparency in linking public feedback to urban design solutions.\nThe application is a scalable, user-centric solution that addresses challenges in traditional\nengagement methods, paving the way for more inclusive and impactful urban planning.\n\n--- Page 4 ---\nProject Overview\n1.1 Company Introduction\nThe Urban Redevelopment Authority is the national land-use planning and conservation\nauthority in Singapore. Established to guide Singapore's physical development, URA is tasked\nwith formulating strategies that address the country's land constraints while ensuring sustainable\ngrowth and liveability. Over the years, URA has played a pivotal role in shaping Singapore’s\ntransformation into a modern, vibrant, and highly liveable city-state. Its efforts are guided by a\nlong-term vision to balance economic development, environmental sustainability, and quality of\nlife. URA’s work spans various domains, including urban planning, heritage conservation,\ninfrastructure development, and placemaking initiatives.\nURA’s approach to urban planning is underpinned by a data-driven and community-centered\nphilosophy. Through careful consideration of current and future trends, URA crafts development\nstrategies that address pressing challenges such as housing needs, mobility, and climate\nresilience. By integrating innovative solutions into its planning processes, URA ensures that\nSingapore remains adaptable to global changes while continuing to thrive as a competitive and\ninclusive city.\n1.2 Public Engagement Session\nURA leverages the Master Plan as a statutory land-use blueprint to guide Singapore’s\ndevelopment over the medium to long term, typically spanning 10 to 15 years. The Master Plan’s\nprimary objective is to optimize limited land resources, supporting sustainable growth while\nenhancing the living environment for residents. Reviewed every five years, this dynamic plan\ntranslates long-term strategies into actionable, detailed development plans.\nPublic engagement is an integral part of URA’s planning process. Through these sessions, URA\ngathers insights, feedback, and aspirations from the community to ensure that urban development\naligns with the needs and desires of residents, businesses, and other stakeholders. These sessions\nact as a bridge between planners and the public, fostering dialogue and mutual understanding.\nPublic engagement sessions not only provide a platform for participants to voice concerns and\nshare ideas but also encourage collaboration in the design of urban spaces. By involving diverse\ncommunity groups, URA ensures inclusivity, amplifying under-represented voices in the\nplanning process. This participatory approach cultivates a sense of ownership and belonging\namong residents, strengthening their connection to their living spaces.\nThe insights and feedback collected during these sessions play a crucial role in shaping URA’s\npolicies and projects. They help ensure that the Master Plan remains responsive to the evolving\n\n--- Page 5 ---\nneeds of Singapore’s population, maintaining its relevance and effectiveness in supporting\nsustainable urban development.\n1.3 Proposed Plan\nTo further URA’s mission of creating livable spaces, this project aims to enhance public\nengagement through innovative approaches. By introducing interactive platforms and leveraging\nadvanced technologies, the project seeks to facilitate more meaningful and participatory\nfeedback processes. These insights will be systematically translated into actionable outcomes to\ninform tangible spatial interventions within urban spaces. Serving as a bridge between URA’s\nvision and the voices of the public, this project reinforces a collaborative and inclusive approach\nto urban development.\n\n--- Page 6 ---\nResearch\n2.1 Interview with URA\nAn interview session was conducted with representatives from the URA to gain deeper insights\ninto their current engagement methods and the challenges they face. Several critical insights\ngathered from the discussion, offering a comprehensive understanding of their approaches and\nareas for improvement.\nTypes of Engagement Sessions\nURA adopts both conventional and AI-enhanced approaches to gather public feedback.\nConventional engagement methods include workshops and in-person discussions, while the more\nrecent integration of generative AI tools has allowed for the visualization of ideas through image\ngeneration. This dual approach reflects URA’s commitment to embracing innovative\ntechnologies alongside traditional practices to achieve more effective public participation.\nPreparation of Engagement Sessions\nThe preparation process for engagement sessions is tailored to the specific site or scenario being\naddressed. Project teams design public engagement frameworks and craft questions that align\nwith the objectives of each session. This targeted approach ensures that the feedback collected is\nrelevant and actionable, providing meaningful insights for urban planning decisions.\nTranslating Public Opinions into Actionable Outcomes\nKey takeaways from public feedback are summarized to address participants' concerns.\nHowever, no immediate commitments are made during the sessions. Instead, the feedback\nundergoes further study and evaluation to ensure that decisions are inclusive, well-informed and\nreflective of the broader community’s needs.\nPublic Perception of AI Implementations\nThe perception of AI implementations among the public varies across demographic groups.\nYounger participants adapt quickly to AI-based tools, demonstrating a high level of comfort and\nengagement. Meanwhile, older generations feel empowered by being included in the process,\nthough some require additional assistance to effectively participate. This highlights the need for\naccessible and inclusive tools to cater to all age groups.\nUser Profiles in Decision-Making\nUser profiles also play a significant role in decision-making processes. Individuals who live in\nthe area of the engagement site or are part of groups directly related to the topic are given greater\npriority in the feedback evaluation. These stakeholders are considered key contributors due to\ntheir vested interest in the project outcomes, ensuring that their input carries substantial weight in\nshaping decisions.\n\n--- Page 7 ---\n2.2 Conventional Approach\nFigure 1: Conventional Engagement Process Approach\nThe Urban Redevelopment Authority (URA) traditionally adapts a manual, pen-and-paper-based\nprocess. Figure 1 illustrates the workflow, starting from the use of sticky notes for ideation and\nsharing, to curating a public exhibition of the outcome of the workshop. At the start of the\nworkshop, participants are strategically grouped, taking into account factors such as\ndemographics, backgrounds, and familiarity with the site. This approach ensures the formation of\nwell-balanced groups for effective results.\nSticky Note + Share\nWithin their groups, participants are encouraged to brainstorm and propose ideas for improving\nthe given site. They are free to explore any topics and write down as many ideas as possible on\nsticky notes. Following this, each participant takes turns sharing their ideas with the group.\nGroup + Select\nSubsequently, the group organises similar ideas into clusters and collaboratively selects the best\nideas to represent their collective vision.\nSketch Proposals\nParticipants are then tasked with visualising their group’s ideas through sketches. Pen and paper\nare provided, and time is allocated for them to create their drawings.\nCross Share\nAfterward, participants engage in a cross-sharing session, where they present their sketches and\nideas to members of other groups, fostering collaborative exchange and diverse perspectives.\nSummarise\nThe workshop facilitators will collect all the sketches and sticky notes from the workshop and\nsummarise the results, consolidating the insights and ideas generated.\n\n--- Page 8 ---\nDevelop + Exhibit\nLastly, they will utilise the summarised results to develop an exhibition that showcases the\nworkshop’s outcome. This involves curating the collected sketches and ideas into a cohesive\nnarrative, creating visual displays that highlight key themes. The exhibition serves to engage\nstakeholders, gather further feedback, and demonstrate the potential for future site improvements\nbased on the participants contribution.\n2.3 URA AI Engagement Framework\nFigure 2: AI Hybrid Engagement Process Approach\nAlongside the conventional approach, URA has also embraced a more digitalised AI engagement\nframework in response to the rise of AI tools. They leverage existing technologies, including\nChatGPT and MidJourney, to enhance analytical insights and improve the visualization.\nQuestion and Answer (Mentimeter)\nURA designs site-specific questions to encourage meaningful engagement, combining\nmultiple-choice questions (MCQs) and open-ended queries. Predefined answers simplify the\nresponse process, while open-ended questions allow participants to elaborate on their thoughts.\nMentimeter, an interactive tool, is used to collect real-time textual responses efficiently and\neffectively.\nBrainstorming Ideas (Mentimeter)\n\n--- Page 9 ---\nParticipants use the questions as a starting point to brainstorm and share their ideas about what\nthey envision or prioritize. This collaborative process enables them to expand on their initial\nthoughts and contribute creative and diverse perspectives via Mentimeter.\nUpvote individual Ideas (Mentimeter)\nResponses to open-ended questions are shared with the community, allowing participants to read\nand evaluate others’ ideas. Through Mentimeter, they upvote the ideas they find most compelling\nor relevant. This voting process ensures that the most popular and meaningful suggestions are\nhighlighted for deeper analysis, making the engagement more inclusive and community-driven.\nExport Response Data (Excel)\nThe collected responses, along with their upvote counts, are exported to Excel format for further\nprocessing.\nPrompt Engineering (ChatGPT-4)\nThe exported data undergoes a two-stage prompt engineering process using ChatGPT-4, designed\nto amplify community input and identify key themes for analysis:\nFirst level: Replication Based on Upvote Weight\nIn the first stage, ChatGPT-4 processes the Excel data, which includes textual feedback and the\ncorresponding upvotes counts. Each response is duplicated in proportion to its upvote count,\nensuring that ideas with higher community support are given greater emphasis in the subsequent\nanalysis. This weighting mechanism ensures that the most popular and relevant ideas are not\ndiluted or overlooked during theme extraction and prompt generation.\nSecond level: Topic Modeling with LDA\nFollowing replication, the enhanced dataset undergoes a topic modeling process using Latent\nDIrichlet Allocation (LDA). This step identifies key themes or clusters from the feedback,\ncapturing the broad areas of concern or interest expressed by the participants. LDA ensures that\nthe extracted themes are distinct, non-overlapping, and reflective of diverse aspects of the\ncommunity’s input.\nGenerate Image Prompts (ChatGPT-4)\nBased on the identified themes, ChatGPT-4 formulates distinct and detailed prompts for image\ngeneration. These prompts incorporate critical keywords and elements derived from the\nfeedback, ensuring that the generated images align closely with the participants’ input and reflect\ntheir original intentions.\nGenerate Image (MidJourney)\n\n--- Page 10 ---\nThe finalized prompts are processed using MidJourney, an AI-based tool that creates visual\nrepresentations of the ideas. These images provide a tangible and immersive way to visualize\npublic feedback, making the engagement process more interactive and impactful.\n2.4 Problem Scoping\nEffective public engagement is a critical component of urban planning, as it ensures that\ncommunity needs and aspirations are integrated into the design and development process.\nHowever, both conventional and AI-driven engagement approaches face challenges that need to\nbe addressed for more impactful outcomes. The three identified issues in the engagement process\nare as follows:\nManual Toggling Between Applications\nThe current AI approach is innovative, but it suffers from inefficiencies caused by the manual\ntoggling between multiple applications at different stages of the workflow. For instance, public\nfeedback collected through Mentimeter is exported to Excel for data organization, then processed\nthrough ChatGPT-4 for prompt engineering, and finally input into MidJourney for visualization.\nThis fragmented process not only consumes time but also increases the likelihood of errors,\ncreating a barrier to seamless integration of AI technologies into the engagement framework.\nDisconnection Between Public Feedback and Urban Design\nIn both conventional and AI approaches, there exists a gap between the insights gathered from\npublic feedback and their translation into actionable urban design solutions. While feedback is\ncollected and categorized effectively, the connection between these insights and the resulting\nurban development plans often remains unclear. This disconnection can lead to public\ndisengagement, as stakeholders may feel that their contributions are not directly influencing\nplanning outcomes. Addressing this issue requires mechanisms to bridge the gap between\nparticipatory input and urban design implementation.\nDifficulty in Translating Public Input into Actionable Outcomes\nBoth conventional and AI engagement methods face challenges in transforming diverse and\noften unstructured public input into clear, actionable outcomes. The complexity of integrating\nqualitative data, such as community preferences and concerns, into measurable urban planning\nobjectives makes the process inherently difficult. This challenge is exacerbated when public\ninput is abstract, subjective, or fragmented, necessitating advanced tools and methodologies to\nsynthesize these ideas into actionable designs and strategies.\n2.5 Problem Statement\nThe identified challenges - manual toggling between applications, the disconnection between\npublic feedback and urban design, and the difficulty in translating public input into actionable\n\n--- Page 11 ---\noutcomes - highlight inefficiencies and gaps in the current engagement processes. These issues\nhinder the ability to fully integrate community input into urban planning strategies.\nBuilding on these challenges, the problem can be framed as follows:\n“How might we integrate Artificial Intelligence (AI) to enhance user engagement, enabling\nusers to express desired space changes and distill actionable outcomes from their feedback?”\n2.6 Proposed Approach\nBuilding on the problem statement and URA's existing engagement framework, the proposed\napproach seeks to revolutionize public engagement in urban planning by leveraging AI\ntechnologies and an inclusive, user-centric solution. The goal is to empower users to contribute\nmeaningfully while streamlining the workflow for administrators, fostering a deeper connection\nbetween public feedback and actionable outcomes.\nGuiding Principles\n1. Empowering Public Engagement\nThe solution will enable users to visually express desired space changes, helping them\nsee how their input directly influences urban design. This fosters transparency and builds\na stronger connection between public feedback and decision-making processes.\n2. Leveraging AI for Actionable Outcomes\nBy using AI, the system will transform public feedback into real-time visualizations of\nurban space changes. This approach will help translate user input into actionable design\nproposals, while also tracking user preferences and identifying emerging trends to inform\ninclusive and sustainable decisions.\nKey Features of the Proposed Solution\nFor Users\n● A user-friendly web application designed to cater to all age groups and demographics.\n● Interactive features guide users through a seamless journey of translating their ideas into\nAI-generated images, enabling them to express preferences for space changes visually\nand intuitively.\n● Ensures inclusivity by accommodating diverse user needs and levels of digital literacy.\nFor Admins (URA)\n● An administrative extension that streamlines engagement sessions through customizable\ncontrols for displays, questions, and interactions.\n\n--- Page 12 ---\n● Analytical tools to aggregate and summarize public feedback, offering insights into\nemerging trends and priorities.\n● Enhanced decision-making through data-driven summaries of user input and\nAI-generated design proposals.\nThis proposed approach establishes a direct and efficient connection between public input and\nurban planning, addressing the problem statement's core challenges and creating a framework for\nsustainable and inclusive urban development.\n\n--- Page 13 ---\nDesign and Development of Application\n3.1 System Architecture\nTechnologies Used\nAll moving parts of the application are within one codebase that contains both the frontend and\nbackend of the application.\nApplication Portion Technologies Used Reason\nFrontend TypeScript, React.js, CSS, Allows full flexibility in\nand Material-UI creation of user interfaces\nBackend Python Python handles all API calls\nin a high speed without\nintroducing complexity\nDatabase Firebase Effective for prototyping, and\npresents a flexible low-cost\ndatabase structure\nImage Hosting Cloudinary Free of charge, and effective\nfor prototyping due to the fast\nspeed and high quality of\nimages.\nApplication Hosting Frontend: GitHub Pages Flexibility to setup a CI/CD\npipeline\nBackend: 72 Core Dual Xeon,\n64GB RAM, Ubuntu + ngrok\nTunnel to Web\nTable 1 Technologies Used in System Building\n\n--- Page 14 ---\nDatabase UML Diagram\nFigure 3: Database UML Diagram\nThe application was built mainly on four types of resources: engagements, questionnaires,\ngenerations, and users. This structure was chosen to ensure that the system is as customizable\nand dynamic as it can be. Here is the breakdown for each resource:\n1. Engagements: Represents the engagement / event that URA holds.\n○ imageUrl: Saved url of the engagement’s main image (the image users will be\nable to edit with)\n○ imageCaption: Context given to the main image such that context can be passed\nto the backend APIs for better generation results.\n○ location: Google maps link of the location of engagement\n○ title: Title or name of the engagement\n2. Questionnaires: Represents the questions that are linked to each engagement. These\nquestions can come in various forms, and having it as a separate entity allows for admins\nto dynamically add or remove questions from an engagement. Furthermore, the\nquestionnaire links up to the specific engagement in the database by being nested in this\nformat: questionnaires / engagementId / questions\n○ questions: All the questions within that questionnaire; where each question entity\nincludes the question string, as well as the type and the choices that it can come\nwith\n\n--- Page 15 ---\n3. Generations: Represents the image generations that users submit.\n○ engagementId: ID link to the engagement event that the generation belongs to\n○ category: The category that the image falls under, created by a backend API\n○ coordinates: An array of coordinates that represent the highlighted portions of the\nimage if inpainting is used\n○ originalPrompt: The original prompt that the user inputted\n○ upscaledPrompt: The upscaled prompt that the backend API generated, further\nimproving the original prompt that the user submitted.\n○ userId: ID link to the user who created the generation\n○ voters: An array of IDs that represent the users who voted for this particular\nimage generation\n4. Users: The users that sign up to the application; they are not connected to a specific\nengagement since one user can attend multiple engagements.\n○ ageGroup, email, gender, name, postalCode: Particulars of user\n○ preferences: Represents the answers of the user to particular questionnaires\n3.2 Admin Interface (Create)\nFigure 4: User Workflow of Admin Interface (Create)\nThe admin dashboard has two workflows: the first being the process of creating the engagement\nthat users will interact with.\nThe process starts with the admin inputting the title of the engagement as well as the Google\nMaps link of the location. Then they proceed to upload the image that will be shown to all users\nduring the image generation section. This is accompanied by a context description which they\nhave to input as well. Lastly, the admin has to create the questionnaire in regards to that specific\nengagement that will be shown after the user signs up to the application. The questionnaire can\ninclude questions that are free-response type, or multi-select.\nInterface\n1. Admin Landing Page\n\n--- Page 16 ---\nThe admin begins on a landing page displaying an overview of all ongoing engagement\nsessions. To plan a new session, the admin can simply click the \"Create New Engagement\nSession\" button to start the process. The team wanted a page that the admins can see all\ndifferent engagement sessions at a glance, to avoid repetition of engagement sessions that\nthey have already created in the past. This is to also ensure quick access to any of the\nengagement sessions that they want to continue working on.\n2. Create Title Page\nUpon clicking the “Create New Engagement Session” button, the admin is prompted to\nadd a title for the session and has the option to input a Google Maps link. This link allows\nusers of the app to view the site directly in Google Maps Street View. The team designed\nthis such that users will all be evaluating the exact same coordinates for a fair\nengagement, otherwise different users may evaluate different parts of the assigned\nlocation.\n\n--- Page 17 ---\n3. Upload Site Image with Contextual Information\nThe admin can then upload images of the actual site, which will serve as the base for\nparticipants to generate their ideas. Additionally, the admin is prompted to provide\ncontextual information for each uploaded image. The contextual information added will\nbe helpful, especially to participants who haven’t visited the site before. This is to also\nhelp inform the AI model in the backend about more context on the image its processing.\n\n--- Page 18 ---\n4. Create Questionnaire\nNext, the admin can add questions to gather responses from users. They have the option\nto include either multiple-choice questions or open-ended questions.\na. Multiple Choice Question: Participants are allowed to quickly select predefined\nanswers, requiring less time to brainstorm and craft their answers. With a Multiple\nChoice Question, we are able to gather the public's opinion in a more structural\nway as all the answers are uniform. We are also able to prompt the users to think\nabout certain themes or issues predefined by the urban planners.\nb. Open Ended Question: With Open Ended Questions, the goal is to try to collect\nas many responses as possible for the participants, these answers will then be\nuseful for the admin’s analysis. Participants are able to share their personalised\nresponses, and urban planners are able to gain a more comprehensive\n\n--- Page 19 ---\nunderstanding of public sentiment.\n5. End Create New Engagement Journey\nOnce the admin completes all the steps, the newly created engagement session will\nappear in the admin's home view and will be ready for participants to engage with.\n\n--- Page 20 ---\n3.3 User Interface\nFigure 5: User Workflow of User Interface\nThe user interface has a straight-forward workflow. The team wanted to ensure that the user\ndidn’t have to toggle between a lot of screens, and just allow for a smooth linear progression\nfrom the start of the application to the end.\nThe user starts with being welcomed and introduced to the application, then proceeds to fill in\ntheir particulars, the questionnaire, and finally meet the characters that will provide feedback on\ntheir generation. The bulk of the workflow happens during the image workshop.\nThe image workshop has two tabs: one that allows the user to in-paint, and the other to input the\nprompt. If the user decides to click on the tools and in-paint, they are still required to input a\nprompt. The system will detect that the in-painting option has been picked, and it will send the\nrequest to our backend with models realistic-vision-v5-1-inpainting, and GPT 4o-mini.\nImplementation and information on all the models mentioned in the workflows will be detailed\nlater in the report at Section 3.5. Essentially, the second model will upscale the prompt that the\n\n--- Page 21 ---\nuser inputs, to make it more descriptive. Then this upscaled prompt will be passed in the first\nmodel, alongside the in-painted image. The model returns back the generated image which will\nthen be critiqued by the same GPT 4o-mini in which we have prompted with the different\ncharacters.\nIf the system has not detected any tools being used, that means that no in-painting has been done.\nIf this is the case, the model we are passing the image to is realvis-xl-v4. This serves as an\nimage-to-image generation. The prompt is still upscaled before that by GPT 4o-mini, and the\nsame critique workflow applies after.\nOnce the image generation has been completed, the user can repeat the entire workflow again to\ngenerate another image, and so on. When they are satisfied with the final generated image, they\ncan then end their journey.\nAfter the end of the image workshop session, users will be directed to an image feed that they\ncan scroll through to view all other generations by other participants, as well as view each post\nindividually.\nThe next section will detail each of the processes described in this overview, as well as showcase\nthe user interfaces that are displayed in each step.\nInterface\nFor the interface, the goal here is to create a platform that is effective and intuitive to all user\nprofiles -- ranging from young students to older folks that may not be as exposed to technology.\nThe following are the various screens that the user will be exposed to:\n1. User Welcome\nThe user will be first welcomed with a series of screens that introduces them to what the\napplication is about. On the last of the screens, the user will be able to click a ‘View Location in\nMaps’ button that will lead them to a direct link of the Google Map coordinates of the site. This\nis such that the user can explore the area in a 3D street view, and understand the context of the\nlocation they’re engaging about.\n\n--- Page 22 ---\n2. User Particulars\nThe user will be led to a screen where they can input their essential information. The reason why\nthe team decided to collect postal codes is for the possibility that URA may want to start\nweighing the opinions of the users based on who actually lives in the area or not.\n\n--- Page 23 ---\n3. Questionnaire\nThe questionnaire is a series of questions curated by the admin, that collects the user preference\ndata. This is to allow the admin to find out more about the user’s thoughts on the location and\nwhy they may generate certain topics. The choices presented are displayed in the form of large\nbuttons, as the target users may have shaky hands (especially older folks), which will give them\ndifficulty pressing smaller radio-buttons.\n\n--- Page 24 ---\n4. Meet the Characters\nThis screen introduces the various characters that will be critiquing the user’s image generation\nlater on. The purpose of this is to inform the user on the various considerations that they may\nmiss out on whilst generating their image; considerations include topics such as accessibility,\nsafety, play, sustainability, and more. The team has decided to attach generated cartoon images\nfor each character, to give personality and allow the users to feel more connected to them.\n5. Image Workshop and Character Feedback\nThis is the main section where all user image generations happen in the application. Here, the\nuser is greeted with a prompt input that they can fill in. Clicking the sparkle icon at the right\ncorner of the input section will then generate the image based on the prompt. If no inpainting has\nbeen done, then the system will call the image to image function to generate the image.\nOtherwise, if the user decides to use the tools and start inpainting on the image, the system will\ncall the inpainting function, in order to apply the prompt input only in that specific area. For the\n\n--- Page 25 ---\ninpainting option, brush size can be adjusted to better suit the user’s needs. Users are able to\ndownload the image as well in order to save all their iterations.\nAll prompts in the input section will be upscaled in the backend, as users may input vague\ndescriptions that will cause the image generation to not produce accurate results. The team aimed\nto relieve the user from using too much cognitive load in curating the perfect prompt, and allow\nthem to instead prompt anything that they want and let their creativity flow.\nOnce the image is generated, a scrollable feedback modal will pop up. The user is required to\nscroll and read through everything before clicking the ‘Acknowledge’ button that will close the\nmodal. This is to encourage users to be more aware of the various issues that they need to\nconsider when generating such images.\nWhen the modal closes, the user can then scroll through all the iterations that they have created\nvia the image carousel buttons.\nThe session will be completed once the button ‘End Journey’ is pressed. This button is separated\nfrom the generation button, as users are able to keep on generating new images or end their\nsession. This design choice was not the best, as the users during the actual engagement had\ndifficulty distinguishing between the two buttons, and would accidentally click end journey --\nthinking they’re able to generate more images.\n\n--- Page 26 ---\n6. Generation Feed\nUsers are able to scroll through and view the various generations viewed by the other\nparticipants in the engagement session. This is to foster community interaction and provide\ninsight on what others in the community are thinking about.\nUsers can also like the posts that they enjoy via clicking the heart icon on the top right. The\nability to like generations are informative for the URA admins to know which designs do the\nmajority of the participants agree / resonate with.\n\n--- Page 27 ---\n7. Individual Generation Post\nThe last screen that users can see is the individual post details. This page can be reached by\nclicking a post from the feed. The information showcased includes the image generated, the user\nhandle, the upscaled prompt, as well as a heart icon that the user can click to like the post.\n\n--- Page 28 ---\n3.4 Admin Interface (View & Analyze Data)\nFigure 6: User Workflow of Admin Interface (View & Analyze Data)\nAs mentioned earlier in Section 3.2 in the document, the admin dashboard has two workflows.\nThe second one is meant for the admin to view all the analytics and responses of the engagement.\nAfter the engagement session has been conducted, the admin can then view three types of data.\nFirst, the home analytics section showcases a range of user data in a summarized manner. It\ndisplays the participants list, regions of where the users come from, user profiles, and more\ncondensed information that can come in useful for them. For instance, the heatmap provides the\nparts of the image where the users have in-painted the most, indicating the areas the public most\n\n--- Page 29 ---\nwanted changing. There is also a community image that can be generated here, which is\nshowcased to the participants to allow them to see their creations as part of the bigger picture.\nThe model gpt-4o-mini processes all the saved upscaled prompts of the users during the\ngenerations, and combines them to form one descriptive prompt inclusive of all the ideas. This\nprompt is passed in realvis-xl-v4, which generates the image. A word cloud of the most popular\nwords are displayed in this section as well.\nSecond, the questionnaire section showcases a summary of all the question responses of the\nparticipants. Bar graphs display the multi-select answers, but the free responses have their own\nsection. Free responses are further categorized, as vague questions will elicit responses that span\nacross multiple topics. The team uses the models BERTopic and gpt-4o-mini to break down\nresponses to its respective domains.\nLastly, the image generations section displays all generations created by the participants of that\nengagement. Admins can further analyze a generation by viewing its individual details alongside\nthe profile of the user who generated that image.\nInterface\n1. Admin Landing Page\nUpon accessing the Admin Dashboard, the admin can view an overview of all\nengagement sessions. They can click on any session to view its results and analytics.\n\n--- Page 30 ---\n2. Engagement Session - Home View\nUpon selecting a specific session, the admin is directed to the engagement session’s\ndashboard home view. This dashboard offers comprehensive insights, including the\nparticipant list, regional distribution based on postal codes, demographics, highlighted\nareas, community image generation, and the most popular words. These analytics are\ncrucial for the admin to extract meaningful insights from public engagement sessions.\na. Participants List: The admin can view a complete list of participants who\ntook part in the engagement session and has the option to export all\nparticipant information as a CSV file.\n\n--- Page 31 ---\nb. Location and Demographics: The admin can view the percentage of\nparticipants residing in different regions of Singapore, determined by the\npostal codes provided. This information is valuable as opinions from\nparticipants closer to the site carry more weight. Additionally, the\ndashboard includes age and gender data, helping the admin identify the\nprimary user group of the engagement session.\nc. Image In-Painting: The admin can visualize the areas highlighted most\nby participants on the base image. This helps identify the most popular\nareas for change and insights into participants' preferences and\nsuggestions.\nd. Community Image Generation: The admin can generate a community\nimage that encapsulates the collective ideas and sentiments of participants,\ncreating a single visual representation of the group's insights and\nperspectives. This community image is generated by collecting an array of\nuser’s prompts, and using GPT-4o-mini to create a community prompt that\nencapsulates the overall participant’s ideas. The Community Image is then\nbeing generated with a Stable Diffusion model from img2img, by\n\n--- Page 32 ---\ninputting the community prompt generated.\ne. Most Popular Words: The admin can view the most frequently\nmentioned words from participants' image prompts, providing insights into\nthe textual elements of users' ideas and feedback.\n3. Question and Answer View\nThe admin can view all participant responses from the engagement session, enabling a\ndeeper understanding of public opinions and insights.\na. Multiple Choice Questions: The responses will be reflected in a bar chart.\nAdmin is able to analyse the most popular answer as well as the number of votes\n\n--- Page 33 ---\nfor each answer with ease.\nb. Open Ended Questions: Admin is able to see all answers from the participants\nfrom the Open Ended Question. These answers are categorised via the model\ngpt-04-mini, to a category for each of the admin’s further analysis. Admin is able\nto get a sense of the topics which the public is mainly interested in.\n\n--- Page 34 ---\n4. AI Image Generations View\nIn this tab, the admin can review all ideation outcomes submitted by participants, with\neach output automatically categorized into relevant topics and organized by themes for\nefficient navigation and analysis. The dashboard leverages categorization techniques to\ngroup participant’s outputs into categories. This structured view allows the admin to\nanalyze participant ideas more effectively and draw actionable conclusions from the\n\n--- Page 35 ---\nengagement session data.\na. Assigning Categories: Each generation by the participants will then be\ncategorized with our topic modelling model, the admin is then able to see\ngenerations sorted by topics, this eliminates the need for the admin to go\nthrough each image one by one to look for specific themes or topics.\n\n--- Page 36 ---\nb. Participant Output Information: By clicking on a specific image, the\nadmin is able to view the participant’s image generation, the upscaled\nversion of the prompt, the number of upvotes, participant demographics,\nand their responses to any Q&A prompts. Admin is then able to analyse in\n\n--- Page 37 ---\ndetail each user’s train of thought that led to the generated image.\n3.5 Artificial Intelligence Models\nAs mentioned in the various parts of the interface walkthroughs, a crucial part of allowing the\nteam’s vision to come to life was with the use of AI models. This section will be broken down\ninto the various models used throughout the project.\nAll these models are running via a backend Flask application in the main codebase. These\nendpoints are being called in the frontend to connect it with the interface. The structure of which\nthese responses are returned, are all determined based on what is the most convenient way to\nmap out the data in conjunction with React JavaScript.\nInpainting (Editing of Image)\nOne of the crucial sections of our application was the inpainting. To achieve this, we tried\ndifferent pipelines and methodologies.\n1. Hugging Face Diffusers: AutoPipeline for Inpainting\nWe initially were ambitious and used Hugging Face’s AutoPipelineForInpainting, a part of the\nDiffusers library. This pipeline is designed to perform inpainting tasks by taking an input image,\na mask image, and a textual prompt describing the modifications.\n\n--- Page 38 ---\nWe tested several models with this pipeline, including Stability AI Stable Diffusion 2\nInpainting (stabilityai/stable-diffusion-2-inpainting), Runway ML Stable Diffusion v1-5\n(runwayml/stable-diffusion-v1-5), and Diffusers Stable Diffusion XL Inpainting 1.0\n(diffusers/stable-diffusion-xl-1.0-inpainting-0.1).\nThis pipeline processes these inputs and generates a modified image that blends the masked\nregions with the rest of the content. However, this method required a powerful GPU for efficient\nprocessing. Even with an NVIDIA RTX 3090, the process took over 35 seconds for a single\nimage. This was a crucial metric when choosing the right inpainting pipeline as it really takes the\nusers out of the engagement process.\nWhile the outputs of the stable-diffusion-2 and stable-diffusion-xl-1.0-inpainting were highly\nrealistic, the computational cost made this approach unsuitable for large-scale or user-facing\napplications.\n2. OpenAI DALL·E 2\nAfter encountering the limitations with the Hugging Face Diffusers, we decided to test OpenAI’s\nDALL.E 2. As of recently DALL.E 2 allows for inpainting or outpainting an image by specifying\na mask and a text prompt. This approach promised a more accessible API and simplified the\ncomputational requirements, making it a suitable candidate for scalable and user-friendly\napplications.\nTo implement inpainting using DALL·E 2, we leveraged OpenAI's Python API. The process\ninvolved uploading a square Portable Network Graphics (PNG) image as the base, and an alpha\nchannel mask to indicate the areas for modification (transparent regions signifying the editable\nparts). Along with these inputs, a descriptive text prompt detailing the desired edits was\nprovided. The API processed the inputs and returned a URL to the edited image.\nHowever, the quality of DALL.E 2’s outputs was not as expected. For simple scenes, it produced\nacceptable results, but for complex scenes, it often failed to deliver with most generations turning\nblanks or with random artifacts.\n3. Realistic Vision v5.1 (Getimg.ai)\nGetimg.ai stood out for its competitive pricing, with inpainting generations costing as little as\n$0.00075 per image, making it highly cost-effective for large-scale applications. Its lightning-fast\nperformance ensured near-instant results, even for high-resolution images. Furthermore, the\nplatform provided access to a wide array of models tailored to different tasks, such as inpainting,\n\n--- Page 39 ---\ntext-to-image, and image-to-image transformations. This versatility allowed us to experiment\nwith multiple models and choose the one that best suited our requirements.\nTo integrate Getimg.ai into our workflow, we used its inpainting API via Python. The Realistic\nVision v5.1 (from the stable-diffusion/inpaint family) Inpainting model was selected due to its\nability to maintain consistency in complex scenes and its efficient processing capabilities,\naligning with our requirements for scalability and quality.\nThe process involved resizing the input image and mask to a uniform size (512x512), encoding\nthem into base64 format, and sending them to the API along with a descriptive text prompt. The\nAPI processed these inputs and returned a URL to the edited image, which was simply used as\nthe source for the <img> tag in the frontend. The API’s adjustable parameters, such as strength,\nguidance, and steps, allowed us to refine the outputs further, ensuring a balance between realism\nand prompt adherence.\nImage to Image\nAs we progressed with the project, user feedback revealed a clear trend: most users wanted a\nstraightforward method to transform their images without manually drawing over the images.\nWhile the inpainting feature offered some precise control, it often felt cumbersome to those who\nsimply wanted an overarching transformation of their entire image based on a textual prompt.\nWith Getimg.ai integrated into our workflow, we had access to a wide range of stable diffusion\nmodels optimized for different styles and applications. After experimenting with multiple\nmodels, we settled on realvis-xl-v4 from the stable-diffusion-xl family. Its robustness in handling\ndiverse prompts made it the ideal choice for the img2img feature.\nThe img2img process begins with the user-uploaded image which is resized to 512x512 pixels.\nThis resizing is model specific. The resized image is then encoded into a base64 format and is\nsent to the API along with a detailed textual prompt describing the desired transformation.\nThe payload sent to the Getimg.ai API includes the base64-encoded image, the user’s prompt,\nand the configured parameters. Once complete, the API returns a URL pointing to the\ntransformed image. The URL returned, was simply used as the source for the <img> tag in the\nfrontend.\nThe returned URL is preloaded to ensure the image is fully ready for display without delays.\nCharacter Impact Analysis\nThe core idea was to leverage LLM agents to simulate human personas and provide actionable\nfeedback tailored to specific user groups. By modeling diverse human needs, the system ensures\n\n--- Page 40 ---\nthat users receive insights during each iteration of their design process. This feedback helps users\nunderstand how their designs align with or deviate from the expectations of different\ndemographic groups and guides them toward creating more inclusive and impactful solutions.\nCharacter Focus\nMrs. Eleanor Tan Accessibility, healthcare, safety\nKumar and Priya Family amenities, education, safety\nAlex Wong Inclusive design, accessibility\nMaya Chen Sustainability, green spaces\nEthan Lim Study areas, affordability\nTable 2 Characters and Respective Focuses\nThe core mechanism involves crafting structured prompts that guide OpenAI's GPT-4o API,\nwhich supports vision inputs, to generate concise and actionable feedback in JSON format.\nThese prompts clearly describe the persona's traits and needs, enabling the API to focus on\ngenerating relevant insights.\nThe inputs sent to the API include the image URL, which serves as the visual context for\nanalysis, and a detailed prompt specifying the personas and their requirements. Initially, we\nconsidered using models like (Bootstrapped Language-Image Pretraining) BLIP or (Contrastive\nLanguage–Image Pretraining) CLIP to caption the image generations and use those captions to\ninform the analysis. However, since the image generation step already precedes this section,\nintroducing additional latency further disrupts the user experience, causing delays that could\nmake the app feel unresponsive and deter users from engaging with it effectively. Hence, we\nproceeded with using GPT-4o.\nPrompt Upscaling\nDuring testing, we noticed that many users, especially elderly individuals, had trouble writing\ngood prompts for editing or generating images. They were unfamiliar with how to structure AI\nprompts and found it hard to describe their ideas clearly. To help, we added a feature that\nautomatically improves their prompts. This makes the app easier to use and reduces the effort\nrequired to create detailed instructions.\nWhen a user submits a prompt, they also specify the mode—either img2img for broad\ntransformations or inpainting for targeted edits. The backend processes the prompt using a\nfunction called improve_caption, which sends the user’s input, along with the selected mode, to\n\n--- Page 41 ---\nGPT-4o-mini. We have crafted unique prompts for each mode, tailored to the specific\nrequirements of img2img and inpainting tasks.\nThe refined prompt, designed to be clearer, more detailed, and actionable, is then sent to the\nimage generation process along with the input image and/or a mask image, ensuring precise and\nhigh-quality outputs that align with the user’s intent.\nWord Generations\nFor the section on topic modelling on free response questions and as well as word cloud\ngeneration, we explored multiple methodologies. Referencing the provided tables, we compared\nOpenAI GPT, BERTopic, TopicGPT, and Custom PromptTopic. To evaluate these\nmethodologies, we established runtime performance and coherence value as our primary metrics.\nAfter conducting a detailed comparison, we found that the performance between OpenAI GPT\nand BERTopic was close. However, BERTopic demonstrated greater consistency when working\nwith larger text corpora, making it the final choice for our implementation.\nCriteria OpenAI GPT BERTopic TopicGPT\nC_V Coherence 0.6924 0.7746 0.6382\nScore\nOutput Quality 7 Distinct, Distinct Topic Repetitive Topics\nWell-Categorized Clusters with Centered Around\nTopics GPT-Enhanced 'Outdoor Recreation'\nLabels\nHigh Coherence\nHighest Coherence\nArchitecture Utilizes OpenAI BERT-based Embedding\nGPT4 with Prompt Embeddings Generation\nEngineering UMAP\nUMAP for HDBSCAN\nDimensionality\nReduction GPT for Labelling\nHDBSCAN for\nClustering\nc-TF-IDF for Topic\nRepresentation\n\n--- Page 42 ---\nCriteria OpenAI GPT BERTopic TopicGPT\nRuntime 1.28s 13.35s 185.77s\nPerformance\nTable 3 Comparison of Models for Word Generation\nThe `initialize_basic_topic_model` function and the `generate_words` endpoint work together\nto enable topic modeling and word extraction using BERTopic. The\n`initialize_basic_topic_model` function sets up the BERTopic model to capture clusters while\nfiltering out noise. A minimum topic size of 3 ensures that only clusters with at least three\nsamples are considered, avoiding smaller, insignificant groups. The model's configuration\nincludes an n-gram range of (1, 2), which captures both single words and short phrases for better\ncontextual insights.\nThe input prompts are processed using the `fit_transform` method, which identifies clusters or\ntopics within the data. For each topic, the top words and their weights are extracted, and a\ndictionary aggregates the cumulative frequencies of these words across all topics. The words are\nthen sorted in descending order of importance based on their weights, creating a ranked list of\nterms most relevant to the identified topics. This list is returned as a JSON response back to\nAdmin Dashboard to display the word cloud.\nTopic Generation\nThe generate-topics endpoint builds upon the same methodology used in word extraction but\nleverages an additional feature of BERTopic: the representation model. This feature allows for\nrefining the raw word clusters into meaningful, high-level topics. While the clustering process\nremains similar—identifying clusters within the input data using embeddings, dimensionality\nreduction, and density-based clustering—the representation model steps in to label these clusters\nwith coherent, human-readable topics rather than just raw words. The endpoint ultimately returns\nup to 20 of these topics in a JSON response, ensuring that they are both meaningful and easy to\ninterpret.\nCommunity Image\nThe Community Image endpoint is the culmination of all the methodologies we have developed\nso far, combining the BERTopic for meaningful representation and GPT-4o for prompt\ngeneration, ultimately feeding into the img2img (Getimg.ai) API for visual output. The goal was\nto create a community image that combines all contributions and showcase it back to the\nparticipants, helping them see how their ideas come together into a shared vision.\n\n--- Page 43 ---\nThe way it is done is by accepting a set of user-provided prompts, which are processed to extract\nthe most significant words using the generate_words method. Using BERTopic, this step\nidentifies clusters and aggregates the most important terms, effectively distilling the key\nelements from the input data. These words are then concatenated into a coherent string to form\nthe basis for further processing.\nNext, the extracted words are passed to GPT-4o to generate a concise and context-aware Stable\nDiffusion prompt.\nOnce the prompt is ready, the endpoint prepares the user-provided image (if any) by resizing it to\nfit the required dimensions (maintaining an aspect ratio with a maximum size of 1024x1024).\nThe image, along with the refined prompt, is sent to the img2img API for processing. This final\nstep uses the realvis-xl-v4 model to transform the image based on the refined prompt, producing\na visual output that embodies the themes and ideas extracted earlier.\nWith that, the application has been fully built and it was time to test it with an actual engagement\nsession.\n\n--- Page 44 ---\nEngagement Session and Results\n4.1 Choosing the Site for Engagement\nThe site was recommended by the URA as part of the ongoing Drafted Master Plan 2025 public\nengagement efforts. URA had previously engaged residents and stakeholders to explore ideas for\ntransforming the sheltered viaduct space below the Queensway Flyover into a new community\nnode.\nFollowing confirmation of the site, a site visit was conducted to analyze the area and its\nsurroundings. This included observing existing features, assessing accessibility, and\nunderstanding the current use of space to inform the application testing process.\nThe location is part of the Alexandra-Queensway Park Connector, which serves as a vital link\nconnecting nearby HDB estates to the Rail Corridor. The space is situated beneath a highway,\nfunctions as an underpass that facilitates accessibility and encourages active mobility, offering a\nseamless connection in the direction towards Tanjong Pagar and Woodlands.\n4.2 Choosing of Participants for the Engagement\nTo ensure a diverse and representative group, participants were selected across various\ndemographics, including age, gender, and background. The group ranged from university\nstudents to elderly residents, providing valuable insights into the differing needs, perspectives,\nand expectations of users across generations. This approach ensured that the feedback collected\nwas inclusive and reflective of the broader community's interests.\n4.3 Hosting of the Engagement\nThe application was tested during an in-person session with eight people, held at the URA\nbuilding. Due to timetable clashes within the participants, the team was not able to hold this\nevent in the actual engagement site.\nIn-Person Session\nThe application testing was conducted through an in-person session involving a total of eight\nparticipants. This number was chosen to ensure a manageable and focused group size, allowing\nfor thorough observation and data collection.\nSession Structure\nThe session was structured to compare the traditional method of using the application with our\nnewly developed method. To achieve this, the participants were randomly divided into two\ngroups of equal sizes. This division enabled a controlled comparison between the two methods,\n\n--- Page 45 ---\nensuring that any differences observed could be attributed to the methods themselves rather than\nother variables.\nMethodology\nEach group was initially exposed to one of the two methods. One group started with the\ntraditional method, while the other group began with our new method. This initial exposure\nallowed participants to familiarize themselves with the respective methods and complete a set of\ntasks or interactions as required by the application.\nAfter the initial exposure, the groups were swapped. This meant that the group that initially used\nthe traditional method then switched to our new method, and vice versa. This swap was crucial\nas it ensured that each participant had the opportunity to experience both methods, thereby\nreducing any bias that might be associated with individual preferences or the order of exposure.\n4.4 Data Collection\nTo gather comprehensive feedback and assess the user experience, exit surveys were conducted\nat the end of each method's trial. To ensure consistency in data collection, the same set of survey\nquestions was used across both methods.\nThe survey was designed to collect both quantitative and qualitative data using a mix of\nmultiple-choice questions (MCQ), rating scales, and open-ended responses. This approach\nenabled a thorough evaluation of user satisfaction, workflow usability, and areas for\nimprovement, providing a well-rounded understanding of participants’ experiences.\nThe exit survey began by recording participants' names and the method they were allocated to.\nThis ensured that feedback could be analyzed at both group and individual levels, facilitating\ntargeted observations and comparisons.\nThe key survey questions were grouped into the following categories, each serving a specific\npurpose:\nOverall Experience and Usability\n● How satisfied were you with the overall experience of using the workflow? (Rating)\nReason: To measure participants’ overall satisfaction and the effectiveness of the\nworkflow in providing a positive user experience.\n● Was the workflow intuitive and easy to follow? (Rating)\nReason: To assess the usability and clarity of the workflow, ensuring it is easy to\nnavigate.\nStrengths and Areas of Flexibility\n\n--- Page 46 ---\n● What did you like the most about the workflow?\nReason: To identify the key strengths and positive aspects that stood out to participants.\n● Did the workflow give you enough flexibility to edit or adjust the image? (MCQ)\nReason: To evaluate the degree of control users had in modifying the generated output.\nQuality and Alignment of Output\n● Did the generated image align with your expectations and prompt input? (MCQ)\nReason: To determine how accurately the workflow delivered results based on user input.\n● Rate the quality of the final image you generated. (Rating)\nReason: To measure the perceived quality of the generated images, which is critical for\nassessing workflow success.\nEngagement and Motivation\n● Did you feel engaged and motivated throughout the process? (Rating)\nReason: To assess the level of user engagement, which is vital for a positive and\nproductive experience.\nChallenges and Usability Issues\n● Did you encounter any difficulties throughout the process? (MCQ)\nReason: To identify any usability issues or obstacles faced during the workflow.\n● If yes, how many times did you need to retry or adjust the steps? (MCQ)\nReason: To quantify the frequency of challenges, providing insights into workflow\nstability and ease of use.\n● If yes, please describe the challenges you faced and how they affected your experience.\nReason: To gather qualitative data on the nature of the challenges and their impact on the\noverall experience.\nFuture Adoption and Improvement\n● How likely are you to use this workflow again for similar tasks? (Rating)\nReason: To gauge the potential for continued use and measure the workflow’s value for\nfuture tasks or sessions.\n● What improvements or additional features would you like to see in the workflow?\nReason: To gather suggestions for enhancements and identify areas for further\ndevelopment.\nAdditional Feedback\n● Do you have any other feedback about the workflows or your experience?\nReason: To provide participants with an open-ended opportunity to share additional\nthoughts or observations not covered in previous questions.\n\n--- Page 47 ---\nThis structured survey design ensures that all critical aspects of the workflow—including\nusability, user satisfaction, challenges, and future adoption—are addressed. The collected data\nprovides a clear, actionable foundation for analyzing user experiences and identifying areas for\nimprovement. Results from the survey will be analyzed in the following section.\n4.5 Results Analysis\nThe survey results from both trials were compared to identify any significant differences in user\nexperience, satisfaction, and performance between the two methods. This comparison was\nessential in evaluating the efficacy and user acceptance of our new method relative to the\ntraditional method.\nBy analysing the feedback and responses from both surveys, we were able to determine whether\nour new method offered any improvements over the existing one. This analysis included\nstatistical comparisons, thematic analysis of qualitative feedback, and an overall assessment of\nuser preferences and experiences.\nResults\nConventional Workflow (the AI Application\nMetric higher the better, out of a (the higher the better, out of a\nmaximum of 5) maximum of 5)\nSatisfaction 4.0 5.0\nIntuitiveness 4.6 4.8\nExpectation 2.8 4.7\nFlexibility 3.9 5.0\nEngagement 4.3 5.0\nOutput quality 3.3 4.6\n\n--- Page 48 ---\nRetention 3.3 4.6\nTable 4 Summary of Results from the Exit Survey Form\nComparative Analysis of AI Application vs. Conventional Workflow Performance Metrics\n\n--- Page 49 ---\nQuantitative Findings\nThe AI application demonstrated superior performance across all measured domains compared to\nthe conventional workflow, with some metrics receiving a perfect score. Most notably, user\nsatisfaction metrics showed a marked improvement, with the AI application scoring 5.0\ncompared to 4.0 for the conventional method. The intuitiveness ratings remained relatively\nconsistent between both approaches, with scores of 4.8 and 4.6 respectively, indicating that users\nfound both methods similarly approachable.\nA particularly significant finding emerged in the Expectation fulfillment category, where the AI\napplication achieved a score of 4.7 compared to 2.8 for the conventional method, representing a\nsubstantial 68% increase. This dramatic improvement suggests that the new methodology better\naligns with user requirements and anticipated outcomes.\nEngagement and Retention Metrics\nThe AI solution dramatically exceeded expectations, scoring 4.7 compared to 2.8 for the\nconventional method, showing just how much better it met users' needs and delivered on their\nexpectations.\nUsers found themselves far more drawn in and committed to the AI-powered approach. Perfect\nscores in flexibility and engagement (5.0) showed how much more intuitive and engaging it was\ncompared to traditional methods, which scored 3.9 and 4.3 respectively. The user retention\nmetric also saw an impressive jump to 4.6 from 3.3, highlighting how the AI solution not only\nkept users far more invested in the process and willing to return.\nIn particular, one aspect that surprised us was the significant jump in output quality, from 3.3 for\nthe conventional method to a 4.6/5 for our AI application. This surprised us as we used a more\ncapable AI image generation model in the conventional workflow. Yet participants ranked the AI\napplication’s outputs as a higher quality. We think this can be attributed to several process\ninnovations involved in the AI application, such as the prompt upscaling and user persona\nfeedback mechanisms, that helped our users better conceptualise and execute on their individual\nvisions.\nOperational Benefits\nThe implementation of our AI application has also yielded several operational advantages. The\nability to administer engagements remotely through user-specific links has streamlined the\nprocess significantly. Furthermore, the transition from shared iPads to individual mobile devices\nhas enhanced the ideation and feedback collection processes, resulting in:\n- Broader range of collected ideas\n- Increased diversity in feedback\n- Accelerated engagement completion times\n\n--- Page 50 ---\n- Enhanced parallel processing capabilities\nFuture Implications\nThe data also suggests a strong likelihood of continued user adoption, with participants showing\n39% higher willingness to engage in future activities using the new system. This increased\nretention rate, combined with the demonstrated improvements in user experience and operational\nefficiency, indicates a successful implementation that warrants continued deployment and\npotential expansion of the system.\n4.6 Additional Findings\nIn addition to the results from the survey analysis, further insights were gathered through\nobservations of participant behavior during the session and informal interviews conducted\nafterward.\nObservations\nParticipants using the AI application workflow appeared highly engaged and interested, despite\nencountering questions and challenges along the way. They actively explored the process and\nwere eager to understand its features. In contrast, participants using the conventional workflow\ndisplayed less enthusiasm, appearing more reserved and passive throughout the session.\nDiving deeper into the various age groups, the team has observed that the older folks tend to be\nvery focused on the application and have lesser in-person verbal engagement. As most of them\nhave not been exposed to AI, they tended to feel a bit of stress and confusion, which distracts\nthem from the aim of discussing their generations with their peers.\nOn the other hand, the younger participants who have already had experience prompting and\ngenerating images, tend to be more social with their peers. They enjoy sharing what they have\ngenerated, and are curious as to how varying the prompts can produce very different results.\nInterviews\nAs each method’s session concluded earlier than expected, participants were introduced to the\nalternative workflow they had not tried. This allowed us to gather comparative feedback directly.\nOne participant noted that the conventional method fosters group interaction, as participants sit\ntogether, share ideas, and broaden their perspectives. The final result—such as a generated\nimage—felt like a collective effort, reflecting input from multiple participants.\nIn contrast, the AI application workflow was described as more direct and site-specific. Its core\nfunctionality, particularly in imprinting, ensured that the generated images were well-suited to\nthe intended site. Participants appreciated this as the primary differentiator. Additionally, they\n\n--- Page 51 ---\nfelt a sense of personal ownership over the AI-generated output, as it was based entirely on their\nindividual input. They enjoyed the element of surprise in seeing the final image at the end of the\nprocess and expressed satisfaction with the result.\n4.7 Future Work from Findings\nFrom the post-event survey results and additional findings, our future work will focus on\nenhancing the application to address the key insights gathered during the engagement session.\nUser Interface Improvements:\nSeveral participants provided valuable feedback indicating that the current user interface requires\nfurther refinement to improve usability and intuitiveness. Key areas of improvement include:\n● Rendering Feedback: Some participants mistook the rendering process for lagging, as\nthe subtle blurring effect during image processing was not noticeable enough. Introducing\na more prominent visual indicator (e.g., a loading bar or animation) will provide clearer\nfeedback during processing.\n● Button Labeling: The “Submit” button was misinterpreted as “Enhance Prompt,”\nsuggesting the need to revisit button labeling and ensure that actions are self-explanatory.\nMoving forward, we will prioritize improving these elements to create a more intuitive and\nuser-friendly interface. This will help reduce confusion, improve workflow efficiency, and\nenhance the overall user experience.\n\n--- Page 52 ---\nConclusion\nThis project demonstrates how integrating AI into urban planning engagement sessions can\nsignificantly improve the quality and efficiency of public participation. By leveraging\ncutting-edge AI technologies, the application facilitates seamless translation of public feedback\ninto actionable urban design inputs, fostering greater transparency and inclusivity.\nResults from the engagement trials show that the AI-driven workflow offers superior\nperformance compared to conventional methods, particularly in satisfaction, intuitiveness, and\nengagement. The participants’ ability to visualize their ideas as part of tangible urban design\noutcomes strengthens their connection to the planning process.\nWhile promising, future work should address user interface refinements and expand accessibility\nfeatures to ensure the platform remains intuitive for diverse user demographics. By continuously\niterating on this foundation, the application has the potential to become a benchmark for\nparticipatory urban planning nationwide, exemplifying how technology can enhance\ncommunity-driven development and sustainable growth.\n",
    "cleaned_content": "[PAGE BREAK]\n60.006: Spatial Design Studio\nFinal Technical Report\nUrban Redevelopment Authority x SUTD\nAditya Kumar 1006300\nCaitlin Chiang 1006537\nChin Wei Ming 1006264\nTimothy Wee 1005545\nWang Zixuan 1006391\n[PAGE BREAK]\nTable of Contents\nExecutive Summary 3\nProject Overview 4\n1.1 Company Introduction 4\n1.2 Public Engagement Session 4\n1.3 Proposed Plan 5\nResearch 6\n2.1 Interview with URA 6\n2.2 Conventional Approach 7\n2.3 URA AI Engagement Framework 8\n2.4 Problem Scoping 10\n2.5 Problem Statement 10\n2.6 Proposed Approach 11\nDesign and Development of Application 13\n3.1 System Architecture 13\n3.2 Admin Interface (Create) 15\n3.3 User Interface 20\n3.4 Admin Interface (View & Analyze Data) 28\n3.5 Artificial Intelligence Models 37\nEngagement Session and Results 44\n4.1 Choosing the Site for Engagement 44\n4.2 Choosing of Participants for the Engagement 44\n4.3 Hosting of the Engagement 44\n4.4 Data Collection 45\n4.5 Results Analysis 47\n4.6 Additional Findings 50\nObservations 50\nInterviews 50\n4.7 Future Work from Findings 51\nConclusion 52\n[PAGE BREAK]\nExecutive Summary\nThe Urban Redevelopment Authority (URA) in Singapore aims to enhance urban planning\nthrough inclusive public engagement. This project introduces an innovative AI-driven\napplication to streamline and enrich engagement sessions. The application provides tools for\nvisualizing public feedback as actionable design inputs, bridging the gap between traditional\nmanual processes and modern digital frameworks.\nThe system integrates generative AI technologies for tasks like inpainting, image generation,\nprompt upscaling, and topic categorization. It supports both users and administrators by offering\ninteractive interfaces for inputting ideas, analyzing responses, and visualizing outcomes. Testing\ninvolved real-world participant trials comparing conventional and AI-based workflows,\ndemonstrating improved satisfaction, engagement, and output quality with the AI approach.\nKey results include:\n● Enhanced participant satisfaction and engagement with the AI workflow.\n● Simplified and accelerated data processing through integrated AI pipelines.\n● Increased transparency in linking public feedback to urban design solutions.\nThe application is a scalable, user-centric solution that addresses challenges in traditional\nengagement methods, paving the way for more inclusive and impactful urban planning.\n[PAGE BREAK]\nProject Overview\n1.1 Company Introduction\nThe Urban Redevelopment Authority is the national land-use planning and conservation\nauthority in Singapore. Established to guide Singapore's physical development, URA is tasked\nwith formulating strategies that address the country's land constraints while ensuring sustainable\ngrowth and liveability. Over the years, URA has played a pivotal role in shaping Singapore’s\ntransformation into a modern, vibrant, and highly liveable city-state. Its efforts are guided by a\nlong-term vision to balance economic development, environmental sustainability, and quality of\nlife. URA’s work spans various domains, including urban planning, heritage conservation,\ninfrastructure development, and placemaking initiatives.\nURA’s approach to urban planning is underpinned by a data-driven and community-centered\nphilosophy. Through careful consideration of current and future trends, URA crafts development\nstrategies that address pressing challenges such as housing needs, mobility, and climate\nresilience. By integrating innovative solutions into its planning processes, URA ensures that\nSingapore remains adaptable to global changes while continuing to thrive as a competitive and\ninclusive city.\n1.2 Public Engagement Session\nURA leverages the Master Plan as a statutory land-use blueprint to guide Singapore’s\ndevelopment over the medium to long term, typically spanning 10 to 15 years. The Master Plan’s\nprimary objective is to optimize limited land resources, supporting sustainable growth while\nenhancing the living environment for residents. Reviewed every five years, this dynamic plan\ntranslates long-term strategies into actionable, detailed development plans.\nPublic engagement is an integral part of URA’s planning process. Through these sessions, URA\ngathers insights, feedback, and aspirations from the community to ensure that urban development\naligns with the needs and desires of residents, businesses, and other stakeholders. These sessions\nact as a bridge between planners and the public, fostering dialogue and mutual understanding.\nPublic engagement sessions not only provide a platform for participants to voice concerns and\nshare ideas but also encourage collaboration in the design of urban spaces. By involving diverse\ncommunity groups, URA ensures inclusivity, amplifying under-represented voices in the\nplanning process. This participatory approach cultivates a sense of ownership and belonging\namong residents, strengthening their connection to their living spaces.\nThe insights and feedback collected during these sessions play a crucial role in shaping URA’s\npolicies and projects. They help ensure that the Master Plan remains responsive to the evolving\n[PAGE BREAK]\nneeds of Singapore’s population, maintaining its relevance and effectiveness in supporting\nsustainable urban development.\n1.3 Proposed Plan\nTo further URA’s mission of creating livable spaces, this project aims to enhance public\nengagement through innovative approaches. By introducing interactive platforms and leveraging\nadvanced technologies, the project seeks to facilitate more meaningful and participatory\nfeedback processes. These insights will be systematically translated into actionable outcomes to\ninform tangible spatial interventions within urban spaces. Serving as a bridge between URA’s\nvision and the voices of the public, this project reinforces a collaborative and inclusive approach\nto urban development.\n[PAGE BREAK]\nResearch\n2.1 Interview with URA\nAn interview session was conducted with representatives from the URA to gain deeper insights\ninto their current engagement methods and the challenges they face. Several critical insights\ngathered from the discussion, offering a comprehensive understanding of their approaches and\nareas for improvement.\nTypes of Engagement Sessions\nURA adopts both conventional and AI-enhanced approaches to gather public feedback.\nConventional engagement methods include workshops and in-person discussions, while the more\nrecent integration of generative AI tools has allowed for the visualization of ideas through image\ngeneration. This dual approach reflects URA’s commitment to embracing innovative\ntechnologies alongside traditional practices to achieve more effective public participation.\nPreparation of Engagement Sessions\nThe preparation process for engagement sessions is tailored to the specific site or scenario being\naddressed. Project teams design public engagement frameworks and craft questions that align\nwith the objectives of each session. This targeted approach ensures that the feedback collected is\nrelevant and actionable, providing meaningful insights for urban planning decisions.\nTranslating Public Opinions into Actionable Outcomes\nKey takeaways from public feedback are summarized to address participants' concerns.\nHowever, no immediate commitments are made during the sessions. Instead, the feedback\nundergoes further study and evaluation to ensure that decisions are inclusive, well-informed and\nreflective of the broader community’s needs.\nPublic Perception of AI Implementations\nThe perception of AI implementations among the public varies across demographic groups.\nYounger participants adapt quickly to AI-based tools, demonstrating a high level of comfort and\nengagement. Meanwhile, older generations feel empowered by being included in the process,\nthough some require additional assistance to effectively participate. This highlights the need for\naccessible and inclusive tools to cater to all age groups.\nUser Profiles in Decision-Making\nUser profiles also play a significant role in decision-making processes. Individuals who live in\nthe area of the engagement site or are part of groups directly related to the topic are given greater\npriority in the feedback evaluation. These stakeholders are considered key contributors due to\ntheir vested interest in the project outcomes, ensuring that their input carries substantial weight in\nshaping decisions.\n[PAGE BREAK]\n2.2 Conventional Approach\nFigure 1: Conventional Engagement Process Approach\nThe Urban Redevelopment Authority (URA) traditionally adapts a manual, pen-and-paper-based\nprocess. Figure 1 illustrates the workflow, starting from the use of sticky notes for ideation and\nsharing, to curating a public exhibition of the outcome of the workshop. At the start of the\nworkshop, participants are strategically grouped, taking into account factors such as\ndemographics, backgrounds, and familiarity with the site. This approach ensures the formation of\nwell-balanced groups for effective results.\nSticky Note + Share\nWithin their groups, participants are encouraged to brainstorm and propose ideas for improving\nthe given site. They are free to explore any topics and write down as many ideas as possible on\nsticky notes. Following this, each participant takes turns sharing their ideas with the group.\nGroup + Select\nSubsequently, the group organises similar ideas into clusters and collaboratively selects the best\nideas to represent their collective vision.\nSketch Proposals\nParticipants are then tasked with visualising their group’s ideas through sketches. Pen and paper\nare provided, and time is allocated for them to create their drawings.\nCross Share\nAfterward, participants engage in a cross-sharing session, where they present their sketches and\nideas to members of other groups, fostering collaborative exchange and diverse perspectives.\nSummarise\nThe workshop facilitators will collect all the sketches and sticky notes from the workshop and\nsummarise the results, consolidating the insights and ideas generated.\n[PAGE BREAK]\nDevelop + Exhibit\nLastly, they will utilise the summarised results to develop an exhibition that showcases the\nworkshop’s outcome. This involves curating the collected sketches and ideas into a cohesive\nnarrative, creating visual displays that highlight key themes. The exhibition serves to engage\nstakeholders, gather further feedback, and demonstrate the potential for future site improvements\nbased on the participants contribution.\n2.3 URA AI Engagement Framework\nFigure 2: AI Hybrid Engagement Process Approach\nAlongside the conventional approach, URA has also embraced a more digitalised AI engagement\nframework in response to the rise of AI tools. They leverage existing technologies, including\nChat GPT and Mid Journey, to enhance analytical insights and improve the visualization.\nQuestion and Answer (Mentimeter)\nURA designs site-specific questions to encourage meaningful engagement, combining\nmultiple-choice questions (MCQs) and open-ended queries. Predefined answers simplify the\nresponse process, while open-ended questions allow participants to elaborate on their thoughts.\nMentimeter, an interactive tool, is used to collect real-time textual responses efficiently and\neffectively.\nBrainstorming Ideas (Mentimeter)\n[PAGE BREAK]\nParticipants use the questions as a starting point to brainstorm and share their ideas about what\nthey envision or prioritize. This collaborative process enables them to expand on their initial\nthoughts and contribute creative and diverse perspectives via Mentimeter.\nUpvote individual Ideas (Mentimeter)\nResponses to open-ended questions are shared with the community, allowing participants to read\nand evaluate others’ ideas. Through Mentimeter, they upvote the ideas they find most compelling\nor relevant. This voting process ensures that the most popular and meaningful suggestions are\nhighlighted for deeper analysis, making the engagement more inclusive and community-driven.\nExport Response Data (Excel)\nThe collected responses, along with their upvote counts, are exported to Excel format for further\nprocessing.\nPrompt Engineering (Chat GPT-4)\nThe exported data undergoes a two-stage prompt engineering process using Chat GPT-4, designed\nto amplify community input and identify key themes for analysis:\nFirst level: Replication Based on Upvote Weight\nIn the first stage, Chat GPT-4 processes the Excel data, which includes textual feedback and the\ncorresponding upvotes counts. Each response is duplicated in proportion to its upvote count,\nensuring that ideas with higher community support are given greater emphasis in the subsequent\nanalysis. This weighting mechanism ensures that the most popular and relevant ideas are not\ndiluted or overlooked during theme extraction and prompt generation.\nSecond level: Topic Modeling with LDA\nFollowing replication, the enhanced dataset undergoes a topic modeling process using Latent\nDIrichlet Allocation (LDA). This step identifies key themes or clusters from the feedback,\ncapturing the broad areas of concern or interest expressed by the participants. LDA ensures that\nthe extracted themes are distinct, non-overlapping, and reflective of diverse aspects of the\ncommunity’s input.\nGenerate Image Prompts (Chat GPT-4)\nBased on the identified themes, Chat GPT-4 formulates distinct and detailed prompts for image\ngeneration. These prompts incorporate critical keywords and elements derived from the\nfeedback, ensuring that the generated images align closely with the participants’ input and reflect\ntheir original intentions.\nGenerate Image (Mid Journey)\n[PAGE BREAK]\nThe finalized prompts are processed using Mid Journey, an AI-based tool that creates visual\nrepresentations of the ideas. These images provide a tangible and immersive way to visualize\npublic feedback, making the engagement process more interactive and impactful.\n2.4 Problem Scoping\nEffective public engagement is a critical component of urban planning, as it ensures that\ncommunity needs and aspirations are integrated into the design and development process.\nHowever, both conventional and AI-driven engagement approaches face challenges that need to\nbe addressed for more impactful outcomes. The three identified issues in the engagement process\nare as follows:\nManual Toggling Between Applications\nThe current AI approach is innovative, but it suffers from inefficiencies caused by the manual\ntoggling between multiple applications at different stages of the workflow. For instance, public\nfeedback collected through Mentimeter is exported to Excel for data organization, then processed\nthrough Chat GPT-4 for prompt engineering, and finally input into Mid Journey for visualization.\nThis fragmented process not only consumes time but also increases the likelihood of errors,\ncreating a barrier to seamless integration of AI technologies into the engagement framework.\nDisconnection Between Public Feedback and Urban Design\nIn both conventional and AI approaches, there exists a gap between the insights gathered from\npublic feedback and their translation into actionable urban design solutions. While feedback is\ncollected and categorized effectively, the connection between these insights and the resulting\nurban development plans often remains unclear. This disconnection can lead to public\ndisengagement, as stakeholders may feel that their contributions are not directly influencing\nplanning outcomes. Addressing this issue requires mechanisms to bridge the gap between\nparticipatory input and urban design implementation.\nDifficulty in Translating Public Input into Actionable Outcomes\nBoth conventional and AI engagement methods face challenges in transforming diverse and\noften unstructured public input into clear, actionable outcomes. The complexity of integrating\nqualitative data, such as community preferences and concerns, into measurable urban planning\nobjectives makes the process inherently difficult. This challenge is exacerbated when public\ninput is abstract, subjective, or fragmented, necessitating advanced tools and methodologies to\nsynthesize these ideas into actionable designs and strategies.\n2.5 Problem Statement\nThe identified challenges - manual toggling between applications, the disconnection between\npublic feedback and urban design, and the difficulty in translating public input into actionable\n[PAGE BREAK]\noutcomes - highlight inefficiencies and gaps in the current engagement processes. These issues\nhinder the ability to fully integrate community input into urban planning strategies.\nBuilding on these challenges, the problem can be framed as follows:\n“How might we integrate Artificial Intelligence (AI) to enhance user engagement, enabling\nusers to express desired space changes and distill actionable outcomes from their feedback?”\n2.6 Proposed Approach\nBuilding on the problem statement and URA's existing engagement framework, the proposed\napproach seeks to revolutionize public engagement in urban planning by leveraging AI\ntechnologies and an inclusive, user-centric solution. The goal is to empower users to contribute\nmeaningfully while streamlining the workflow for administrators, fostering a deeper connection\nbetween public feedback and actionable outcomes.\nGuiding Principles\n1. Empowering Public Engagement\nThe solution will enable users to visually express desired space changes, helping them\nsee how their input directly influences urban design. This fosters transparency and builds\na stronger connection between public feedback and decision-making processes.\n2. Leveraging AI for Actionable Outcomes\nBy using AI, the system will transform public feedback into real-time visualizations of\nurban space changes. This approach will help translate user input into actionable design\nproposals, while also tracking user preferences and identifying emerging trends to inform\ninclusive and sustainable decisions.\nKey Features of the Proposed Solution\nFor Users\n● A user-friendly web application designed to cater to all age groups and demographics.\n● Interactive features guide users through a seamless journey of translating their ideas into\nAI-generated images, enabling them to express preferences for space changes visually\nand intuitively.\n● Ensures inclusivity by accommodating diverse user needs and levels of digital literacy.\nFor Admins (URA)\n● An administrative extension that streamlines engagement sessions through customizable\ncontrols for displays, questions, and interactions.\n[PAGE BREAK]\n● Analytical tools to aggregate and summarize public feedback, offering insights into\nemerging trends and priorities.\n● Enhanced decision-making through data-driven summaries of user input and\nAI-generated design proposals.\nThis proposed approach establishes a direct and efficient connection between public input and\nurban planning, addressing the problem statement's core challenges and creating a framework for\nsustainable and inclusive urban development.\n[PAGE BREAK]\nDesign and Development of Application\n3.1 System Architecture\nTechnologies Used\nAll moving parts of the application are within one codebase that contains both the frontend and\nbackend of the application.\nApplication Portion Technologies Used Reason\nFrontend Type Script, React.js, CSS, Allows full flexibility in\nand Material-UI creation of user interfaces\nBackend Python Python handles all API calls\nin a high speed without\nintroducing complexity\nDatabase Firebase Effective for prototyping, and\npresents a flexible low-cost\ndatabase structure\nImage Hosting Cloudinary Free of charge, and effective\nfor prototyping due to the fast\nspeed and high quality of\nimages.\nApplication Hosting Frontend: Git Hub Pages Flexibility to setup a CI/CD\npipeline\nBackend: 72 Core Dual Xeon,\n64 GB RAM, Ubuntu + ngrok\nTunnel to Web\nTable 1 Technologies Used in System Building\n[PAGE BREAK]\nDatabase UML Diagram\nFigure 3: Database UML Diagram\nThe application was built mainly on four types of resources: engagements, questionnaires,\ngenerations, and users. This structure was chosen to ensure that the system is as customizable\nand dynamic as it can be. Here is the breakdown for each resource:\n1. Engagements: Represents the engagement / event that URA holds.\n○ image Url: Saved url of the engagement’s main image (the image users will be\nable to edit with)\n○ image Caption: Context given to the main image such that context can be passed\nto the backend APIs for better generation results.\n○ location: Google maps link of the location of engagement\n○ title: Title or name of the engagement\n2. Questionnaires: Represents the questions that are linked to each engagement. These\nquestions can come in various forms, and having it as a separate entity allows for admins\nto dynamically add or remove questions from an engagement. Furthermore, the\nquestionnaire links up to the specific engagement in the database by being nested in this\nformat: questionnaires / engagement Id / questions\n○ questions: All the questions within that questionnaire; where each question entity\nincludes the question string, as well as the type and the choices that it can come\nwith\n[PAGE BREAK]\n3. Generations: Represents the image generations that users submit.\n○ engagement Id: ID link to the engagement event that the generation belongs to\n○ category: The category that the image falls under, created by a backend API\n○ coordinates: An array of coordinates that represent the highlighted portions of the\nimage if inpainting is used\n○ original Prompt: The original prompt that the user inputted\n○ upscaled Prompt: The upscaled prompt that the backend API generated, further\nimproving the original prompt that the user submitted.\n○ user Id: ID link to the user who created the generation\n○ voters: An array of IDs that represent the users who voted for this particular\nimage generation\n4. Users: The users that sign up to the application; they are not connected to a specific\nengagement since one user can attend multiple engagements.\n○ age Group, email, gender, name, postal Code: Particulars of user\n○ preferences: Represents the answers of the user to particular questionnaires\n3.2 Admin Interface (Create)\nFigure 4: User Workflow of Admin Interface (Create)\nThe admin dashboard has two workflows: the first being the process of creating the engagement\nthat users will interact with.\nThe process starts with the admin inputting the title of the engagement as well as the Google\nMaps link of the location. Then they proceed to upload the image that will be shown to all users\nduring the image generation section. This is accompanied by a context description which they\nhave to input as well. Lastly, the admin has to create the questionnaire in regards to that specific\nengagement that will be shown after the user signs up to the application. The questionnaire can\ninclude questions that are free-response type, or multi-select.\nInterface\n1. Admin Landing Page\n[PAGE BREAK]\nThe admin begins on a landing page displaying an overview of all ongoing engagement\nsessions. To plan a new session, the admin can simply click the \"Create New Engagement\nSession\" button to start the process. The team wanted a page that the admins can see all\ndifferent engagement sessions at a glance, to avoid repetition of engagement sessions that\nthey have already created in the past. This is to also ensure quick access to any of the\nengagement sessions that they want to continue working on.\n2. Create Title Page\nUpon clicking the “Create New Engagement Session” button, the admin is prompted to\nadd a title for the session and has the option to input a Google Maps link. This link allows\nusers of the app to view the site directly in Google Maps Street View. The team designed\nthis such that users will all be evaluating the exact same coordinates for a fair\nengagement, otherwise different users may evaluate different parts of the assigned\nlocation.\n[PAGE BREAK]\n3. Upload Site Image with Contextual Information\nThe admin can then upload images of the actual site, which will serve as the base for\nparticipants to generate their ideas. Additionally, the admin is prompted to provide\ncontextual information for each uploaded image. The contextual information added will\nbe helpful, especially to participants who haven’t visited the site before. This is to also\nhelp inform the AI model in the backend about more context on the image its processing.\n[PAGE BREAK]\n4. Create Questionnaire\nNext, the admin can add questions to gather responses from users. They have the option\nto include either multiple-choice questions or open-ended questions.\na. Multiple Choice Question: Participants are allowed to quickly select predefined\nanswers, requiring less time to brainstorm and craft their answers. With a Multiple\nChoice Question, we are able to gather the public's opinion in a more structural\nway as all the answers are uniform. We are also able to prompt the users to think\nabout certain themes or issues predefined by the urban planners.\nb. Open Ended Question: With Open Ended Questions, the goal is to try to collect\nas many responses as possible for the participants, these answers will then be\nuseful for the admin’s analysis. Participants are able to share their personalised\nresponses, and urban planners are able to gain a more comprehensive\n[PAGE BREAK]\nunderstanding of public sentiment.\n5. End Create New Engagement Journey\nOnce the admin completes all the steps, the newly created engagement session will\nappear in the admin's home view and will be ready for participants to engage with.\n[PAGE BREAK]\n3.3 User Interface\nFigure 5: User Workflow of User Interface\nThe user interface has a straight-forward workflow. The team wanted to ensure that the user\ndidn’t have to toggle between a lot of screens, and just allow for a smooth linear progression\nfrom the start of the application to the end.\nThe user starts with being welcomed and introduced to the application, then proceeds to fill in\ntheir particulars, the questionnaire, and finally meet the characters that will provide feedback on\ntheir generation. The bulk of the workflow happens during the image workshop.\nThe image workshop has two tabs: one that allows the user to in-paint, and the other to input the\nprompt. If the user decides to click on the tools and in-paint, they are still required to input a\nprompt. The system will detect that the in-painting option has been picked, and it will send the\nrequest to our backend with models realistic-vision-v5-1-inpainting, and GPT 4 o-mini.\nImplementation and information on all the models mentioned in the workflows will be detailed\nlater in the report at Section 3.5. Essentially, the second model will upscale the prompt that the\n[PAGE BREAK]\nuser inputs, to make it more descriptive. Then this upscaled prompt will be passed in the first\nmodel, alongside the in-painted image. The model returns back the generated image which will\nthen be critiqued by the same GPT 4 o-mini in which we have prompted with the different\ncharacters.\nIf the system has not detected any tools being used, that means that no in-painting has been done.\nIf this is the case, the model we are passing the image to is realvis-xl-v4. This serves as an\nimage-to-image generation. The prompt is still upscaled before that by GPT 4 o-mini, and the\nsame critique workflow applies after.\nOnce the image generation has been completed, the user can repeat the entire workflow again to\ngenerate another image, and so on. When they are satisfied with the final generated image, they\ncan then end their journey.\nAfter the end of the image workshop session, users will be directed to an image feed that they\ncan scroll through to view all other generations by other participants, as well as view each post\nindividually.\nThe next section will detail each of the processes described in this overview, as well as showcase\nthe user interfaces that are displayed in each step.\nInterface\nFor the interface, the goal here is to create a platform that is effective and intuitive to all user\nprofiles -- ranging from young students to older folks that may not be as exposed to technology.\nThe following are the various screens that the user will be exposed to:\n1. User Welcome\nThe user will be first welcomed with a series of screens that introduces them to what the\napplication is about. On the last of the screens, the user will be able to click a ‘View Location in\nMaps’ button that will lead them to a direct link of the Google Map coordinates of the site. This\nis such that the user can explore the area in a 3 D street view, and understand the context of the\nlocation they’re engaging about.\n[PAGE BREAK]\n2. User Particulars\nThe user will be led to a screen where they can input their essential information. The reason why\nthe team decided to collect postal codes is for the possibility that URA may want to start\nweighing the opinions of the users based on who actually lives in the area or not.\n[PAGE BREAK]\n3. Questionnaire\nThe questionnaire is a series of questions curated by the admin, that collects the user preference\ndata. This is to allow the admin to find out more about the user’s thoughts on the location and\nwhy they may generate certain topics. The choices presented are displayed in the form of large\nbuttons, as the target users may have shaky hands (especially older folks), which will give them\ndifficulty pressing smaller radio-buttons.\n[PAGE BREAK]\n4. Meet the Characters\nThis screen introduces the various characters that will be critiquing the user’s image generation\nlater on. The purpose of this is to inform the user on the various considerations that they may\nmiss out on whilst generating their image; considerations include topics such as accessibility,\nsafety, play, sustainability, and more. The team has decided to attach generated cartoon images\nfor each character, to give personality and allow the users to feel more connected to them.\n5. Image Workshop and Character Feedback\nThis is the main section where all user image generations happen in the application. Here, the\nuser is greeted with a prompt input that they can fill in. Clicking the sparkle icon at the right\ncorner of the input section will then generate the image based on the prompt. If no inpainting has\nbeen done, then the system will call the image to image function to generate the image.\nOtherwise, if the user decides to use the tools and start inpainting on the image, the system will\ncall the inpainting function, in order to apply the prompt input only in that specific area. For the\n[PAGE BREAK]\ninpainting option, brush size can be adjusted to better suit the user’s needs. Users are able to\ndownload the image as well in order to save all their iterations.\nAll prompts in the input section will be upscaled in the backend, as users may input vague\ndescriptions that will cause the image generation to not produce accurate results. The team aimed\nto relieve the user from using too much cognitive load in curating the perfect prompt, and allow\nthem to instead prompt anything that they want and let their creativity flow.\nOnce the image is generated, a scrollable feedback modal will pop up. The user is required to\nscroll and read through everything before clicking the ‘Acknowledge’ button that will close the\nmodal. This is to encourage users to be more aware of the various issues that they need to\nconsider when generating such images.\nWhen the modal closes, the user can then scroll through all the iterations that they have created\nvia the image carousel buttons.\nThe session will be completed once the button ‘End Journey’ is pressed. This button is separated\nfrom the generation button, as users are able to keep on generating new images or end their\nsession. This design choice was not the best, as the users during the actual engagement had\ndifficulty distinguishing between the two buttons, and would accidentally click end journey --\nthinking they’re able to generate more images.\n[PAGE BREAK]\n6. Generation Feed\nUsers are able to scroll through and view the various generations viewed by the other\nparticipants in the engagement session. This is to foster community interaction and provide\ninsight on what others in the community are thinking about.\nUsers can also like the posts that they enjoy via clicking the heart icon on the top right. The\nability to like generations are informative for the URA admins to know which designs do the\nmajority of the participants agree / resonate with.\n[PAGE BREAK]\n7. Individual Generation Post\nThe last screen that users can see is the individual post details. This page can be reached by\nclicking a post from the feed. The information showcased includes the image generated, the user\nhandle, the upscaled prompt, as well as a heart icon that the user can click to like the post.\n[PAGE BREAK]\n3.4 Admin Interface (View & Analyze Data)\nFigure 6: User Workflow of Admin Interface (View & Analyze Data)\nAs mentioned earlier in Section 3.2 in the document, the admin dashboard has two workflows.\nThe second one is meant for the admin to view all the analytics and responses of the engagement.\nAfter the engagement session has been conducted, the admin can then view three types of data.\nFirst, the home analytics section showcases a range of user data in a summarized manner. It\ndisplays the participants list, regions of where the users come from, user profiles, and more\ncondensed information that can come in useful for them. For instance, the heatmap provides the\nparts of the image where the users have in-painted the most, indicating the areas the public most\n[PAGE BREAK]\nwanted changing. There is also a community image that can be generated here, which is\nshowcased to the participants to allow them to see their creations as part of the bigger picture.\nThe model gpt-4 o-mini processes all the saved upscaled prompts of the users during the\ngenerations, and combines them to form one descriptive prompt inclusive of all the ideas. This\nprompt is passed in realvis-xl-v4, which generates the image. A word cloud of the most popular\nwords are displayed in this section as well.\nSecond, the questionnaire section showcases a summary of all the question responses of the\nparticipants. Bar graphs display the multi-select answers, but the free responses have their own\nsection. Free responses are further categorized, as vague questions will elicit responses that span\nacross multiple topics. The team uses the models BERTopic and gpt-4 o-mini to break down\nresponses to its respective domains.\nLastly, the image generations section displays all generations created by the participants of that\nengagement. Admins can further analyze a generation by viewing its individual details alongside\nthe profile of the user who generated that image.\nInterface\n1. Admin Landing Page\nUpon accessing the Admin Dashboard, the admin can view an overview of all\nengagement sessions. They can click on any session to view its results and analytics.\n[PAGE BREAK]\n2. Engagement Session - Home View\nUpon selecting a specific session, the admin is directed to the engagement session’s\ndashboard home view. This dashboard offers comprehensive insights, including the\nparticipant list, regional distribution based on postal codes, demographics, highlighted\nareas, community image generation, and the most popular words. These analytics are\ncrucial for the admin to extract meaningful insights from public engagement sessions.\na. Participants List: The admin can view a complete list of participants who\ntook part in the engagement session and has the option to export all\nparticipant information as a CSV file.\n[PAGE BREAK]\nb. Location and Demographics: The admin can view the percentage of\nparticipants residing in different regions of Singapore, determined by the\npostal codes provided. This information is valuable as opinions from\nparticipants closer to the site carry more weight. Additionally, the\ndashboard includes age and gender data, helping the admin identify the\nprimary user group of the engagement session.\nc. Image In-Painting: The admin can visualize the areas highlighted most\nby participants on the base image. This helps identify the most popular\nareas for change and insights into participants' preferences and\nsuggestions.\nd. Community Image Generation: The admin can generate a community\nimage that encapsulates the collective ideas and sentiments of participants,\ncreating a single visual representation of the group's insights and\nperspectives. This community image is generated by collecting an array of\nuser’s prompts, and using GPT-4 o-mini to create a community prompt that\nencapsulates the overall participant’s ideas. The Community Image is then\nbeing generated with a Stable Diffusion model from img2 img, by\n[PAGE BREAK]\ninputting the community prompt generated.\ne. Most Popular Words: The admin can view the most frequently\nmentioned words from participants' image prompts, providing insights into\nthe textual elements of users' ideas and feedback.\n3. Question and Answer View\nThe admin can view all participant responses from the engagement session, enabling a\ndeeper understanding of public opinions and insights.\na. Multiple Choice Questions: The responses will be reflected in a bar chart.\nAdmin is able to analyse the most popular answer as well as the number of votes\n[PAGE BREAK]\nfor each answer with ease.\nb. Open Ended Questions: Admin is able to see all answers from the participants\nfrom the Open Ended Question. These answers are categorised via the model\ngpt-04-mini, to a category for each of the admin’s further analysis. Admin is able\nto get a sense of the topics which the public is mainly interested in.\n[PAGE BREAK]\n4. AI Image Generations View\nIn this tab, the admin can review all ideation outcomes submitted by participants, with\neach output automatically categorized into relevant topics and organized by themes for\nefficient navigation and analysis. The dashboard leverages categorization techniques to\ngroup participant’s outputs into categories. This structured view allows the admin to\nanalyze participant ideas more effectively and draw actionable conclusions from the\n[PAGE BREAK]\nengagement session data.\na. Assigning Categories: Each generation by the participants will then be\ncategorized with our topic modelling model, the admin is then able to see\ngenerations sorted by topics, this eliminates the need for the admin to go\nthrough each image one by one to look for specific themes or topics.\n[PAGE BREAK]\nb. Participant Output Information: By clicking on a specific image, the\nadmin is able to view the participant’s image generation, the upscaled\nversion of the prompt, the number of upvotes, participant demographics,\nand their responses to any Q&A prompts. Admin is then able to analyse in\n[PAGE BREAK]\ndetail each user’s train of thought that led to the generated image.\n3.5 Artificial Intelligence Models\nAs mentioned in the various parts of the interface walkthroughs, a crucial part of allowing the\nteam’s vision to come to life was with the use of AI models. This section will be broken down\ninto the various models used throughout the project.\nAll these models are running via a backend Flask application in the main codebase. These\nendpoints are being called in the frontend to connect it with the interface. The structure of which\nthese responses are returned, are all determined based on what is the most convenient way to\nmap out the data in conjunction with React Java Script.\nInpainting (Editing of Image)\nOne of the crucial sections of our application was the inpainting. To achieve this, we tried\ndifferent pipelines and methodologies.\n1. Hugging Face Diffusers: Auto Pipeline for Inpainting\nWe initially were ambitious and used Hugging Face’s Auto Pipeline For Inpainting, a part of the\nDiffusers library. This pipeline is designed to perform inpainting tasks by taking an input image,\na mask image, and a textual prompt describing the modifications.\n[PAGE BREAK]\nWe tested several models with this pipeline, including Stability AI Stable Diffusion 2\nInpainting (stabilityai/stable-diffusion-2-inpainting), Runway ML Stable Diffusion v1-5\n(runwayml/stable-diffusion-v1-5), and Diffusers Stable Diffusion XL Inpainting 1.0\n(diffusers/stable-diffusion-xl-1.0-inpainting-0.1).\nThis pipeline processes these inputs and generates a modified image that blends the masked\nregions with the rest of the content. However, this method required a powerful GPU for efficient\nprocessing. Even with an NVIDIA RTX 3090, the process took over 35 seconds for a single\nimage. This was a crucial metric when choosing the right inpainting pipeline as it really takes the\nusers out of the engagement process.\nWhile the outputs of the stable-diffusion-2 and stable-diffusion-xl-1.0-inpainting were highly\nrealistic, the computational cost made this approach unsuitable for large-scale or user-facing\napplications.\n2. Open AI DALL·E 2\nAfter encountering the limitations with the Hugging Face Diffusers, we decided to test Open AI’s\nDALL.E 2. As of recently DALL.E 2 allows for inpainting or outpainting an image by specifying\na mask and a text prompt. This approach promised a more accessible API and simplified the\ncomputational requirements, making it a suitable candidate for scalable and user-friendly\napplications.\nTo implement inpainting using DALL·E 2, we leveraged Open AI's Python API. The process\ninvolved uploading a square Portable Network Graphics (PNG) image as the base, and an alpha\nchannel mask to indicate the areas for modification (transparent regions signifying the editable\nparts). Along with these inputs, a descriptive text prompt detailing the desired edits was\nprovided. The API processed the inputs and returned a URL to the edited image.\nHowever, the quality of DALL.E 2’s outputs was not as expected. For simple scenes, it produced\nacceptable results, but for complex scenes, it often failed to deliver with most generations turning\nblanks or with random artifacts.\n3. Realistic Vision v5.1 (Getimg.ai)\nGetimg.ai stood out for its competitive pricing, with inpainting generations costing as little as\n$0.00075 per image, making it highly cost-effective for large-scale applications. Its lightning-fast\nperformance ensured near-instant results, even for high-resolution images. Furthermore, the\nplatform provided access to a wide array of models tailored to different tasks, such as inpainting,\n[PAGE BREAK]\ntext-to-image, and image-to-image transformations. This versatility allowed us to experiment\nwith multiple models and choose the one that best suited our requirements.\nTo integrate Getimg.ai into our workflow, we used its inpainting API via Python. The Realistic\nVision v5.1 (from the stable-diffusion/inpaint family) Inpainting model was selected due to its\nability to maintain consistency in complex scenes and its efficient processing capabilities,\naligning with our requirements for scalability and quality.\nThe process involved resizing the input image and mask to a uniform size (512 x512), encoding\nthem into base64 format, and sending them to the API along with a descriptive text prompt. The\nAPI processed these inputs and returned a URL to the edited image, which was simply used as\nthe source for the <img> tag in the frontend. The API’s adjustable parameters, such as strength,\nguidance, and steps, allowed us to refine the outputs further, ensuring a balance between realism\nand prompt adherence.\nImage to Image\nAs we progressed with the project, user feedback revealed a clear trend: most users wanted a\nstraightforward method to transform their images without manually drawing over the images.\nWhile the inpainting feature offered some precise control, it often felt cumbersome to those who\nsimply wanted an overarching transformation of their entire image based on a textual prompt.\nWith Getimg.ai integrated into our workflow, we had access to a wide range of stable diffusion\nmodels optimized for different styles and applications. After experimenting with multiple\nmodels, we settled on realvis-xl-v4 from the stable-diffusion-xl family. Its robustness in handling\ndiverse prompts made it the ideal choice for the img2 img feature.\nThe img2 img process begins with the user-uploaded image which is resized to 512 x512 pixels.\nThis resizing is model specific. The resized image is then encoded into a base64 format and is\nsent to the API along with a detailed textual prompt describing the desired transformation.\nThe payload sent to the Getimg.ai API includes the base64-encoded image, the user’s prompt,\nand the configured parameters. Once complete, the API returns a URL pointing to the\ntransformed image. The URL returned, was simply used as the source for the <img> tag in the\nfrontend.\nThe returned URL is preloaded to ensure the image is fully ready for display without delays.\nCharacter Impact Analysis\nThe core idea was to leverage LLM agents to simulate human personas and provide actionable\nfeedback tailored to specific user groups. By modeling diverse human needs, the system ensures\n[PAGE BREAK]\nthat users receive insights during each iteration of their design process. This feedback helps users\nunderstand how their designs align with or deviate from the expectations of different\ndemographic groups and guides them toward creating more inclusive and impactful solutions.\nCharacter Focus\nMrs. Eleanor Tan Accessibility, healthcare, safety\nKumar and Priya Family amenities, education, safety\nAlex Wong Inclusive design, accessibility\nMaya Chen Sustainability, green spaces\nEthan Lim Study areas, affordability\nTable 2 Characters and Respective Focuses\nThe core mechanism involves crafting structured prompts that guide Open AI's GPT-4 o API,\nwhich supports vision inputs, to generate concise and actionable feedback in JSON format.\nThese prompts clearly describe the persona's traits and needs, enabling the API to focus on\ngenerating relevant insights.\nThe inputs sent to the API include the image URL, which serves as the visual context for\nanalysis, and a detailed prompt specifying the personas and their requirements. Initially, we\nconsidered using models like (Bootstrapped Language-Image Pretraining) BLIP or (Contrastive\nLanguage–Image Pretraining) CLIP to caption the image generations and use those captions to\ninform the analysis. However, since the image generation step already precedes this section,\nintroducing additional latency further disrupts the user experience, causing delays that could\nmake the app feel unresponsive and deter users from engaging with it effectively. Hence, we\nproceeded with using GPT-4 o.\nPrompt Upscaling\nDuring testing, we noticed that many users, especially elderly individuals, had trouble writing\ngood prompts for editing or generating images. They were unfamiliar with how to structure AI\nprompts and found it hard to describe their ideas clearly. To help, we added a feature that\nautomatically improves their prompts. This makes the app easier to use and reduces the effort\nrequired to create detailed instructions.\nWhen a user submits a prompt, they also specify the mode—either img2 img for broad\ntransformations or inpainting for targeted edits. The backend processes the prompt using a\nfunction called improve_caption, which sends the user’s input, along with the selected mode, to\n[PAGE BREAK]\nGPT-4 o-mini. We have crafted unique prompts for each mode, tailored to the specific\nrequirements of img2 img and inpainting tasks.\nThe refined prompt, designed to be clearer, more detailed, and actionable, is then sent to the\nimage generation process along with the input image and/or a mask image, ensuring precise and\nhigh-quality outputs that align with the user’s intent.\nWord Generations\nFor the section on topic modelling on free response questions and as well as word cloud\ngeneration, we explored multiple methodologies. Referencing the provided tables, we compared\nOpen AI GPT, BERTopic, Topic GPT, and Custom Prompt Topic. To evaluate these\nmethodologies, we established runtime performance and coherence value as our primary metrics.\nAfter conducting a detailed comparison, we found that the performance between Open AI GPT\nand BERTopic was close. However, BERTopic demonstrated greater consistency when working\nwith larger text corpora, making it the final choice for our implementation.\nCriteria Open AI GPT BERTopic Topic GPT\nC_V Coherence 0.6924 0.7746 0.6382\nScore\nOutput Quality 7 Distinct, Distinct Topic Repetitive Topics\nWell-Categorized Clusters with Centered Around\nTopics GPT-Enhanced 'Outdoor Recreation'\nLabels\nHigh Coherence\nHighest Coherence\nArchitecture Utilizes Open AI BERT-based Embedding\nGPT4 with Prompt Embeddings Generation\nEngineering UMAP\nUMAP for HDBSCAN\nDimensionality\nReduction GPT for Labelling\nHDBSCAN for\nClustering\nc-TF-IDF for Topic\nRepresentation\n[PAGE BREAK]\nCriteria Open AI GPT BERTopic Topic GPT\nRuntime 1.28 s 13.35 s 185.77 s\nPerformance\nTable 3 Comparison of Models for Word Generation\nThe `initialize_basic_topic_model` function and the `generate_words` endpoint work together\nto enable topic modeling and word extraction using BERTopic. The\n`initialize_basic_topic_model` function sets up the BERTopic model to capture clusters while\nfiltering out noise. A minimum topic size of 3 ensures that only clusters with at least three\nsamples are considered, avoiding smaller, insignificant groups. The model's configuration\nincludes an n-gram range of (1, 2), which captures both single words and short phrases for better\ncontextual insights.\nThe input prompts are processed using the `fit_transform` method, which identifies clusters or\ntopics within the data. For each topic, the top words and their weights are extracted, and a\ndictionary aggregates the cumulative frequencies of these words across all topics. The words are\nthen sorted in descending order of importance based on their weights, creating a ranked list of\nterms most relevant to the identified topics. This list is returned as a JSON response back to\nAdmin Dashboard to display the word cloud.\nTopic Generation\nThe generate-topics endpoint builds upon the same methodology used in word extraction but\nleverages an additional feature of BERTopic: the representation model. This feature allows for\nrefining the raw word clusters into meaningful, high-level topics. While the clustering process\nremains similar—identifying clusters within the input data using embeddings, dimensionality\nreduction, and density-based clustering—the representation model steps in to label these clusters\nwith coherent, human-readable topics rather than just raw words. The endpoint ultimately returns\nup to 20 of these topics in a JSON response, ensuring that they are both meaningful and easy to\ninterpret.\nCommunity Image\nThe Community Image endpoint is the culmination of all the methodologies we have developed\nso far, combining the BERTopic for meaningful representation and GPT-4 o for prompt\ngeneration, ultimately feeding into the img2 img (Getimg.ai) API for visual output. The goal was\nto create a community image that combines all contributions and showcase it back to the\nparticipants, helping them see how their ideas come together into a shared vision.\n[PAGE BREAK]\nThe way it is done is by accepting a set of user-provided prompts, which are processed to extract\nthe most significant words using the generate_words method. Using BERTopic, this step\nidentifies clusters and aggregates the most important terms, effectively distilling the key\nelements from the input data. These words are then concatenated into a coherent string to form\nthe basis for further processing.\nNext, the extracted words are passed to GPT-4 o to generate a concise and context-aware Stable\nDiffusion prompt.\nOnce the prompt is ready, the endpoint prepares the user-provided image (if any) by resizing it to\nfit the required dimensions (maintaining an aspect ratio with a maximum size of 1024 x1024).\nThe image, along with the refined prompt, is sent to the img2 img API for processing. This final\nstep uses the realvis-xl-v4 model to transform the image based on the refined prompt, producing\na visual output that embodies the themes and ideas extracted earlier.\nWith that, the application has been fully built and it was time to test it with an actual engagement\nsession.\n[PAGE BREAK]\nEngagement Session and Results\n4.1 Choosing the Site for Engagement\nThe site was recommended by the URA as part of the ongoing Drafted Master Plan 2025 public\nengagement efforts. URA had previously engaged residents and stakeholders to explore ideas for\ntransforming the sheltered viaduct space below the Queensway Flyover into a new community\nnode.\nFollowing confirmation of the site, a site visit was conducted to analyze the area and its\nsurroundings. This included observing existing features, assessing accessibility, and\nunderstanding the current use of space to inform the application testing process.\nThe location is part of the Alexandra-Queensway Park Connector, which serves as a vital link\nconnecting nearby HDB estates to the Rail Corridor. The space is situated beneath a highway,\nfunctions as an underpass that facilitates accessibility and encourages active mobility, offering a\nseamless connection in the direction towards Tanjong Pagar and Woodlands.\n4.2 Choosing of Participants for the Engagement\nTo ensure a diverse and representative group, participants were selected across various\ndemographics, including age, gender, and background. The group ranged from university\nstudents to elderly residents, providing valuable insights into the differing needs, perspectives,\nand expectations of users across generations. This approach ensured that the feedback collected\nwas inclusive and reflective of the broader community's interests.\n4.3 Hosting of the Engagement\nThe application was tested during an in-person session with eight people, held at the URA\nbuilding. Due to timetable clashes within the participants, the team was not able to hold this\nevent in the actual engagement site.\nIn-Person Session\nThe application testing was conducted through an in-person session involving a total of eight\nparticipants. This number was chosen to ensure a manageable and focused group size, allowing\nfor thorough observation and data collection.\nSession Structure\nThe session was structured to compare the traditional method of using the application with our\nnewly developed method. To achieve this, the participants were randomly divided into two\ngroups of equal sizes. This division enabled a controlled comparison between the two methods,\n[PAGE BREAK]\nensuring that any differences observed could be attributed to the methods themselves rather than\nother variables.\nMethodology\nEach group was initially exposed to one of the two methods. One group started with the\ntraditional method, while the other group began with our new method. This initial exposure\nallowed participants to familiarize themselves with the respective methods and complete a set of\ntasks or interactions as required by the application.\nAfter the initial exposure, the groups were swapped. This meant that the group that initially used\nthe traditional method then switched to our new method, and vice versa. This swap was crucial\nas it ensured that each participant had the opportunity to experience both methods, thereby\nreducing any bias that might be associated with individual preferences or the order of exposure.\n4.4 Data Collection\nTo gather comprehensive feedback and assess the user experience, exit surveys were conducted\nat the end of each method's trial. To ensure consistency in data collection, the same set of survey\nquestions was used across both methods.\nThe survey was designed to collect both quantitative and qualitative data using a mix of\nmultiple-choice questions (MCQ), rating scales, and open-ended responses. This approach\nenabled a thorough evaluation of user satisfaction, workflow usability, and areas for\nimprovement, providing a well-rounded understanding of participants’ experiences.\nThe exit survey began by recording participants' names and the method they were allocated to.\nThis ensured that feedback could be analyzed at both group and individual levels, facilitating\ntargeted observations and comparisons.\nThe key survey questions were grouped into the following categories, each serving a specific\npurpose:\nOverall Experience and Usability\n● How satisfied were you with the overall experience of using the workflow? (Rating)\nReason: To measure participants’ overall satisfaction and the effectiveness of the\nworkflow in providing a positive user experience.\n● Was the workflow intuitive and easy to follow? (Rating)\nReason: To assess the usability and clarity of the workflow, ensuring it is easy to\nnavigate.\nStrengths and Areas of Flexibility\n[PAGE BREAK]\n● What did you like the most about the workflow?\nReason: To identify the key strengths and positive aspects that stood out to participants.\n● Did the workflow give you enough flexibility to edit or adjust the image? (MCQ)\nReason: To evaluate the degree of control users had in modifying the generated output.\nQuality and Alignment of Output\n● Did the generated image align with your expectations and prompt input? (MCQ)\nReason: To determine how accurately the workflow delivered results based on user input.\n● Rate the quality of the final image you generated. (Rating)\nReason: To measure the perceived quality of the generated images, which is critical for\nassessing workflow success.\nEngagement and Motivation\n● Did you feel engaged and motivated throughout the process? (Rating)\nReason: To assess the level of user engagement, which is vital for a positive and\nproductive experience.\nChallenges and Usability Issues\n● Did you encounter any difficulties throughout the process? (MCQ)\nReason: To identify any usability issues or obstacles faced during the workflow.\n● If yes, how many times did you need to retry or adjust the steps? (MCQ)\nReason: To quantify the frequency of challenges, providing insights into workflow\nstability and ease of use.\n● If yes, please describe the challenges you faced and how they affected your experience.\nReason: To gather qualitative data on the nature of the challenges and their impact on the\noverall experience.\nFuture Adoption and Improvement\n● How likely are you to use this workflow again for similar tasks? (Rating)\nReason: To gauge the potential for continued use and measure the workflow’s value for\nfuture tasks or sessions.\n● What improvements or additional features would you like to see in the workflow?\nReason: To gather suggestions for enhancements and identify areas for further\ndevelopment.\nAdditional Feedback\n● Do you have any other feedback about the workflows or your experience?\nReason: To provide participants with an open-ended opportunity to share additional\nthoughts or observations not covered in previous questions.\n[PAGE BREAK]\nThis structured survey design ensures that all critical aspects of the workflow—including\nusability, user satisfaction, challenges, and future adoption—are addressed. The collected data\nprovides a clear, actionable foundation for analyzing user experiences and identifying areas for\nimprovement. Results from the survey will be analyzed in the following section.\n4.5 Results Analysis\nThe survey results from both trials were compared to identify any significant differences in user\nexperience, satisfaction, and performance between the two methods. This comparison was\nessential in evaluating the efficacy and user acceptance of our new method relative to the\ntraditional method.\nBy analysing the feedback and responses from both surveys, we were able to determine whether\nour new method offered any improvements over the existing one. This analysis included\nstatistical comparisons, thematic analysis of qualitative feedback, and an overall assessment of\nuser preferences and experiences.\nResults\nConventional Workflow (the AI Application\nMetric higher the better, out of a (the higher the better, out of a\nmaximum of 5) maximum of 5)\nSatisfaction 4.0 5.0\nIntuitiveness 4.6 4.8\nExpectation 2.8 4.7\nFlexibility 3.9 5.0\nEngagement 4.3 5.0\nOutput quality 3.3 4.6\n[PAGE BREAK]\nRetention 3.3 4.6\nTable 4 Summary of Results from the Exit Survey Form\nComparative Analysis of AI Application vs. Conventional Workflow Performance Metrics\n[PAGE BREAK]\nQuantitative Findings\nThe AI application demonstrated superior performance across all measured domains compared to\nthe conventional workflow, with some metrics receiving a perfect score. Most notably, user\nsatisfaction metrics showed a marked improvement, with the AI application scoring 5.0\ncompared to 4.0 for the conventional method. The intuitiveness ratings remained relatively\nconsistent between both approaches, with scores of 4.8 and 4.6 respectively, indicating that users\nfound both methods similarly approachable.\nA particularly significant finding emerged in the Expectation fulfillment category, where the AI\napplication achieved a score of 4.7 compared to 2.8 for the conventional method, representing a\nsubstantial 68% increase. This dramatic improvement suggests that the new methodology better\naligns with user requirements and anticipated outcomes.\nEngagement and Retention Metrics\nThe AI solution dramatically exceeded expectations, scoring 4.7 compared to 2.8 for the\nconventional method, showing just how much better it met users' needs and delivered on their\nexpectations.\nUsers found themselves far more drawn in and committed to the AI-powered approach. Perfect\nscores in flexibility and engagement (5.0) showed how much more intuitive and engaging it was\ncompared to traditional methods, which scored 3.9 and 4.3 respectively. The user retention\nmetric also saw an impressive jump to 4.6 from 3.3, highlighting how the AI solution not only\nkept users far more invested in the process and willing to return.\nIn particular, one aspect that surprised us was the significant jump in output quality, from 3.3 for\nthe conventional method to a 4.6/5 for our AI application. This surprised us as we used a more\ncapable AI image generation model in the conventional workflow. Yet participants ranked the AI\napplication’s outputs as a higher quality. We think this can be attributed to several process\ninnovations involved in the AI application, such as the prompt upscaling and user persona\nfeedback mechanisms, that helped our users better conceptualise and execute on their individual\nvisions.\nOperational Benefits\nThe implementation of our AI application has also yielded several operational advantages. The\nability to administer engagements remotely through user-specific links has streamlined the\nprocess significantly. Furthermore, the transition from shared i Pads to individual mobile devices\nhas enhanced the ideation and feedback collection processes, resulting in:\n- Broader range of collected ideas\n- Increased diversity in feedback\n- Accelerated engagement completion times\n[PAGE BREAK]\n- Enhanced parallel processing capabilities\nFuture Implications\nThe data also suggests a strong likelihood of continued user adoption, with participants showing\n39% higher willingness to engage in future activities using the new system. This increased\nretention rate, combined with the demonstrated improvements in user experience and operational\nefficiency, indicates a successful implementation that warrants continued deployment and\npotential expansion of the system.\n4.6 Additional Findings\nIn addition to the results from the survey analysis, further insights were gathered through\nobservations of participant behavior during the session and informal interviews conducted\nafterward.\nObservations\nParticipants using the AI application workflow appeared highly engaged and interested, despite\nencountering questions and challenges along the way. They actively explored the process and\nwere eager to understand its features. In contrast, participants using the conventional workflow\ndisplayed less enthusiasm, appearing more reserved and passive throughout the session.\nDiving deeper into the various age groups, the team has observed that the older folks tend to be\nvery focused on the application and have lesser in-person verbal engagement. As most of them\nhave not been exposed to AI, they tended to feel a bit of stress and confusion, which distracts\nthem from the aim of discussing their generations with their peers.\nOn the other hand, the younger participants who have already had experience prompting and\ngenerating images, tend to be more social with their peers. They enjoy sharing what they have\ngenerated, and are curious as to how varying the prompts can produce very different results.\nInterviews\nAs each method’s session concluded earlier than expected, participants were introduced to the\nalternative workflow they had not tried. This allowed us to gather comparative feedback directly.\nOne participant noted that the conventional method fosters group interaction, as participants sit\ntogether, share ideas, and broaden their perspectives. The final result—such as a generated\nimage—felt like a collective effort, reflecting input from multiple participants.\nIn contrast, the AI application workflow was described as more direct and site-specific. Its core\nfunctionality, particularly in imprinting, ensured that the generated images were well-suited to\nthe intended site. Participants appreciated this as the primary differentiator. Additionally, they\n[PAGE BREAK]\nfelt a sense of personal ownership over the AI-generated output, as it was based entirely on their\nindividual input. They enjoyed the element of surprise in seeing the final image at the end of the\nprocess and expressed satisfaction with the result.\n4.7 Future Work from Findings\nFrom the post-event survey results and additional findings, our future work will focus on\nenhancing the application to address the key insights gathered during the engagement session.\nUser Interface Improvements:\nSeveral participants provided valuable feedback indicating that the current user interface requires\nfurther refinement to improve usability and intuitiveness. Key areas of improvement include:\n● Rendering Feedback: Some participants mistook the rendering process for lagging, as\nthe subtle blurring effect during image processing was not noticeable enough. Introducing\na more prominent visual indicator (e.g., a loading bar or animation) will provide clearer\nfeedback during processing.\n● Button Labeling: The “Submit” button was misinterpreted as “Enhance Prompt,”\nsuggesting the need to revisit button labeling and ensure that actions are self-explanatory.\nMoving forward, we will prioritize improving these elements to create a more intuitive and\nuser-friendly interface. This will help reduce confusion, improve workflow efficiency, and\nenhance the overall user experience.\n[PAGE BREAK]\nConclusion\nThis project demonstrates how integrating AI into urban planning engagement sessions can\nsignificantly improve the quality and efficiency of public participation. By leveraging\ncutting-edge AI technologies, the application facilitates seamless translation of public feedback\ninto actionable urban design inputs, fostering greater transparency and inclusivity.\nResults from the engagement trials show that the AI-driven workflow offers superior\nperformance compared to conventional methods, particularly in satisfaction, intuitiveness, and\nengagement. The participants’ ability to visualize their ideas as part of tangible urban design\noutcomes strengthens their connection to the planning process.\nWhile promising, future work should address user interface refinements and expand accessibility\nfeatures to ensure the platform remains intuitive for diverse user demographics. By continuously\niterating on this foundation, the application has the potential to become a benchmark for\nparticipatory urban planning nationwide, exemplifying how technology can enhance\ncommunity-driven development and sustainable growth.",
    "sections": [
      {
        "title": "Content",
        "content": "60.006: Spatial Design Studio\n"
      },
      {
        "title": "Content",
        "content": "Table of Contents\n"
      },
      {
        "title": "Project Overview 4",
        "content": "1.1 Company Introduction 4\n1.2 Public Engagement Session 4\n1.3 Proposed Plan 5\nResearch 6\n2.1 Interview with URA 6\n2.2 Conventional Approach 7\n2.3 URA AI Engagement Framework 8\n2.4 Problem Scoping 10\n2.5 Problem Statement 10\n2.6 Proposed Approach 11\nDesign and Development of Application 13\n3.1 System Architecture 13\n3.2 Admin Interface (Create) 15\n3.3 User Interface 20\n3.4 Admin Interface (View & Analyze Data) 28\n3.5 Artificial Intelligence Models 37\n"
      },
      {
        "title": "Engagement Session and Results 44",
        "content": "4.1 Choosing the Site for Engagement 44\n4.2 Choosing of Participants for the Engagement 44\n4.3 Hosting of the Engagement 44\n4.4 Data Collection 45\n4.5 Results Analysis 47\n4.6 Additional Findings 50\nObservations 50\nInterviews 50\n4.7 Future Work from Findings 51\n"
      },
      {
        "title": "The Urban Redevelopment Authority (URA) in Singapore aims to enhance urban planning",
        "content": "through inclusive public engagement. This project introduces an innovative AI-driven\napplication to streamline and enrich engagement sessions. The application provides tools for\nvisualizing public feedback as actionable design inputs, bridging the gap between traditional\nmanual processes and modern digital frameworks.\nThe system integrates generative AI technologies for tasks like inpainting, image generation,\nprompt upscaling, and topic categorization. It supports both users and administrators by offering\ninteractive interfaces for inputting ideas, analyzing responses, and visualizing outcomes. Testing\ninvolved real-world participant trials comparing conventional and AI-based workflows,\ndemonstrating improved satisfaction, engagement, and output quality with the AI approach.\n"
      },
      {
        "title": "Key results include:",
        "content": "● Enhanced participant satisfaction and engagement with the AI workflow.\n● Simplified and accelerated data processing through integrated AI pipelines.\n● Increased transparency in linking public feedback to urban design solutions.\nThe application is a scalable, user-centric solution that addresses challenges in traditional\nengagement methods, paving the way for more inclusive and impactful urban planning.\n"
      },
      {
        "title": "Project Overview",
        "content": "1.1 Company Introduction\n"
      },
      {
        "title": "The Urban Redevelopment Authority is the national land-use planning and conservation",
        "content": "authority in Singapore. Established to guide Singapore's physical development, URA is tasked\nwith formulating strategies that address the country's land constraints while ensuring sustainable\ngrowth and liveability. Over the years, URA has played a pivotal role in shaping Singapore’s\ntransformation into a modern, vibrant, and highly liveable city-state. Its efforts are guided by a\nlong-term vision to balance economic development, environmental sustainability, and quality of\nlife. URA’s work spans various domains, including urban planning, heritage conservation,\ninfrastructure development, and placemaking initiatives.\nURA’s approach to urban planning is underpinned by a data-driven and community-centered\nphilosophy. Through careful consideration of current and future trends, URA crafts development\nstrategies that address pressing challenges such as housing needs, mobility, and climate\nresilience. By integrating innovative solutions into its planning processes, URA ensures that\nSingapore remains adaptable to global changes while continuing to thrive as a competitive and\ninclusive city.\n1.2 Public Engagement Session\nURA leverages the Master Plan as a statutory land-use blueprint to guide Singapore’s\ndevelopment over the medium to long term, typically spanning 10 to 15 years. The Master Plan’s\nprimary objective is to optimize limited land resources, supporting sustainable growth while\nenhancing the living environment for residents. Reviewed every five years, this dynamic plan\ntranslates long-term strategies into actionable, detailed development plans.\nPublic engagement is an integral part of URA’s planning process. Through these sessions, URA\ngathers insights, feedback, and aspirations from the community to ensure that urban development\naligns with the needs and desires of residents, businesses, and other stakeholders. These sessions\nact as a bridge between planners and the public, fostering dialogue and mutual understanding.\nPublic engagement sessions not only provide a platform for participants to voice concerns and\nshare ideas but also encourage collaboration in the design of urban spaces. By involving diverse\ncommunity groups, URA ensures inclusivity, amplifying under-represented voices in the\nplanning process. This participatory approach cultivates a sense of ownership and belonging\namong residents, strengthening their connection to their living spaces.\nThe insights and feedback collected during these sessions play a crucial role in shaping URA’s\npolicies and projects. They help ensure that the Master Plan remains responsive to the evolving\n"
      },
      {
        "title": "Content",
        "content": "needs of Singapore’s population, maintaining its relevance and effectiveness in supporting\nsustainable urban development.\n1.3 Proposed Plan\nTo further URA’s mission of creating livable spaces, this project aims to enhance public\nengagement through innovative approaches. By introducing interactive platforms and leveraging\nadvanced technologies, the project seeks to facilitate more meaningful and participatory\nfeedback processes. These insights will be systematically translated into actionable outcomes to\ninform tangible spatial interventions within urban spaces. Serving as a bridge between URA’s\nvision and the voices of the public, this project reinforces a collaborative and inclusive approach\nto urban development.\n"
      },
      {
        "title": "Content",
        "content": "Research\n2.1 Interview with URA\nAn interview session was conducted with representatives from the URA to gain deeper insights\ninto their current engagement methods and the challenges they face. Several critical insights\ngathered from the discussion, offering a comprehensive understanding of their approaches and\nareas for improvement.\nTypes of Engagement Sessions\nURA adopts both conventional and AI-enhanced approaches to gather public feedback.\nConventional engagement methods include workshops and in-person discussions, while the more\nrecent integration of generative AI tools has allowed for the visualization of ideas through image\ngeneration. This dual approach reflects URA’s commitment to embracing innovative\ntechnologies alongside traditional practices to achieve more effective public participation.\nPreparation of Engagement Sessions\nThe preparation process for engagement sessions is tailored to the specific site or scenario being\naddressed. Project teams design public engagement frameworks and craft questions that align\nwith the objectives of each session. This targeted approach ensures that the feedback collected is\nrelevant and actionable, providing meaningful insights for urban planning decisions.\n"
      },
      {
        "title": "Translating Public Opinions into Actionable Outcomes",
        "content": "Key takeaways from public feedback are summarized to address participants' concerns.\nHowever, no immediate commitments are made during the sessions. Instead, the feedback\nundergoes further study and evaluation to ensure that decisions are inclusive, well-informed and\nreflective of the broader community’s needs.\n"
      },
      {
        "title": "Public Perception of AI Implementations",
        "content": "The perception of AI implementations among the public varies across demographic groups.\nYounger participants adapt quickly to AI-based tools, demonstrating a high level of comfort and\nengagement. Meanwhile, older generations feel empowered by being included in the process,\nthough some require additional assistance to effectively participate. This highlights the need for\naccessible and inclusive tools to cater to all age groups.\n"
      },
      {
        "title": "User Profiles in Decision-Making",
        "content": "User profiles also play a significant role in decision-making processes. Individuals who live in\nthe area of the engagement site or are part of groups directly related to the topic are given greater\npriority in the feedback evaluation. These stakeholders are considered key contributors due to\ntheir vested interest in the project outcomes, ensuring that their input carries substantial weight in\nshaping decisions.\n"
      },
      {
        "title": "Content",
        "content": "2.2 Conventional Approach\nFigure 1: Conventional Engagement Process Approach\n"
      },
      {
        "title": "The Urban Redevelopment Authority (URA) traditionally adapts a manual, pen-and-paper-based",
        "content": "process. Figure 1 illustrates the workflow, starting from the use of sticky notes for ideation and\nsharing, to curating a public exhibition of the outcome of the workshop. At the start of the\nworkshop, participants are strategically grouped, taking into account factors such as\ndemographics, backgrounds, and familiarity with the site. This approach ensures the formation of\nwell-balanced groups for effective results.\n"
      },
      {
        "title": "Sticky Note + Share",
        "content": "Within their groups, participants are encouraged to brainstorm and propose ideas for improving\nthe given site. They are free to explore any topics and write down as many ideas as possible on\nsticky notes. Following this, each participant takes turns sharing their ideas with the group.\nGroup + Select\nSubsequently, the group organises similar ideas into clusters and collaboratively selects the best\nideas to represent their collective vision.\n"
      },
      {
        "title": "Sketch Proposals",
        "content": "Participants are then tasked with visualising their group’s ideas through sketches. Pen and paper\nare provided, and time is allocated for them to create their drawings.\n"
      },
      {
        "title": "Cross Share",
        "content": "Afterward, participants engage in a cross-sharing session, where they present their sketches and\nideas to members of other groups, fostering collaborative exchange and diverse perspectives.\nSummarise\nThe workshop facilitators will collect all the sketches and sticky notes from the workshop and\nsummarise the results, consolidating the insights and ideas generated.\n"
      },
      {
        "title": "Content",
        "content": "Develop + Exhibit\nLastly, they will utilise the summarised results to develop an exhibition that showcases the\nworkshop’s outcome. This involves curating the collected sketches and ideas into a cohesive\nnarrative, creating visual displays that highlight key themes. The exhibition serves to engage\nstakeholders, gather further feedback, and demonstrate the potential for future site improvements\nbased on the participants contribution.\n2.3 URA AI Engagement Framework\nFigure 2: AI Hybrid Engagement Process Approach\nAlongside the conventional approach, URA has also embraced a more digitalised AI engagement\nframework in response to the rise of AI tools. They leverage existing technologies, including\nChat GPT and Mid Journey, to enhance analytical insights and improve the visualization.\nQuestion and Answer (Mentimeter)\nURA designs site-specific questions to encourage meaningful engagement, combining\nmultiple-choice questions (MCQs) and open-ended queries. Predefined answers simplify the\nresponse process, while open-ended questions allow participants to elaborate on their thoughts.\nMentimeter, an interactive tool, is used to collect real-time textual responses efficiently and\neffectively.\n"
      },
      {
        "title": "Content",
        "content": "Participants use the questions as a starting point to brainstorm and share their ideas about what\nthey envision or prioritize. This collaborative process enables them to expand on their initial\nthoughts and contribute creative and diverse perspectives via Mentimeter.\nUpvote individual Ideas (Mentimeter)\nResponses to open-ended questions are shared with the community, allowing participants to read\nand evaluate others’ ideas. Through Mentimeter, they upvote the ideas they find most compelling\nor relevant. This voting process ensures that the most popular and meaningful suggestions are\nhighlighted for deeper analysis, making the engagement more inclusive and community-driven.\n"
      },
      {
        "title": "Export Response Data (Excel)",
        "content": "The collected responses, along with their upvote counts, are exported to Excel format for further\nprocessing.\n"
      },
      {
        "title": "Prompt Engineering (Chat GPT-4)",
        "content": "The exported data undergoes a two-stage prompt engineering process using Chat GPT-4, designed\nto amplify community input and identify key themes for analysis:\nFirst level: Replication Based on Upvote Weight\nIn the first stage, Chat GPT-4 processes the Excel data, which includes textual feedback and the\ncorresponding upvotes counts. Each response is duplicated in proportion to its upvote count,\nensuring that ideas with higher community support are given greater emphasis in the subsequent\nanalysis. This weighting mechanism ensures that the most popular and relevant ideas are not\ndiluted or overlooked during theme extraction and prompt generation.\nSecond level: Topic Modeling with LDA\nFollowing replication, the enhanced dataset undergoes a topic modeling process using Latent\nDIrichlet Allocation (LDA). This step identifies key themes or clusters from the feedback,\ncapturing the broad areas of concern or interest expressed by the participants. LDA ensures that\nthe extracted themes are distinct, non-overlapping, and reflective of diverse aspects of the\ncommunity’s input.\n"
      },
      {
        "title": "Generate Image Prompts (Chat GPT-4)",
        "content": "Based on the identified themes, Chat GPT-4 formulates distinct and detailed prompts for image\ngeneration. These prompts incorporate critical keywords and elements derived from the\nfeedback, ensuring that the generated images align closely with the participants’ input and reflect\ntheir original intentions.\n"
      },
      {
        "title": "Content",
        "content": "The finalized prompts are processed using Mid Journey, an AI-based tool that creates visual\nrepresentations of the ideas. These images provide a tangible and immersive way to visualize\npublic feedback, making the engagement process more interactive and impactful.\n2.4 Problem Scoping\nEffective public engagement is a critical component of urban planning, as it ensures that\ncommunity needs and aspirations are integrated into the design and development process.\nHowever, both conventional and AI-driven engagement approaches face challenges that need to\nbe addressed for more impactful outcomes. The three identified issues in the engagement process\nare as follows:\n"
      },
      {
        "title": "Manual Toggling Between Applications",
        "content": "The current AI approach is innovative, but it suffers from inefficiencies caused by the manual\ntoggling between multiple applications at different stages of the workflow. For instance, public\nfeedback collected through Mentimeter is exported to Excel for data organization, then processed\nthrough Chat GPT-4 for prompt engineering, and finally input into Mid Journey for visualization.\nThis fragmented process not only consumes time but also increases the likelihood of errors,\ncreating a barrier to seamless integration of AI technologies into the engagement framework.\n"
      },
      {
        "title": "Disconnection Between Public Feedback and Urban Design",
        "content": "In both conventional and AI approaches, there exists a gap between the insights gathered from\npublic feedback and their translation into actionable urban design solutions. While feedback is\ncollected and categorized effectively, the connection between these insights and the resulting\nurban development plans often remains unclear. This disconnection can lead to public\ndisengagement, as stakeholders may feel that their contributions are not directly influencing\nplanning outcomes. Addressing this issue requires mechanisms to bridge the gap between\nparticipatory input and urban design implementation.\nDifficulty in Translating Public Input into Actionable Outcomes\nBoth conventional and AI engagement methods face challenges in transforming diverse and\noften unstructured public input into clear, actionable outcomes. The complexity of integrating\nqualitative data, such as community preferences and concerns, into measurable urban planning\nobjectives makes the process inherently difficult. This challenge is exacerbated when public\ninput is abstract, subjective, or fragmented, necessitating advanced tools and methodologies to\nsynthesize these ideas into actionable designs and strategies.\n2.5 Problem Statement\nThe identified challenges - manual toggling between applications, the disconnection between\npublic feedback and urban design, and the difficulty in translating public input into actionable\n"
      },
      {
        "title": "Content",
        "content": "outcomes - highlight inefficiencies and gaps in the current engagement processes. These issues\nhinder the ability to fully integrate community input into urban planning strategies.\nBuilding on these challenges, the problem can be framed as follows:\n“How might we integrate Artificial Intelligence (AI) to enhance user engagement, enabling\nusers to express desired space changes and distill actionable outcomes from their feedback?”\n2.6 Proposed Approach\nBuilding on the problem statement and URA's existing engagement framework, the proposed\napproach seeks to revolutionize public engagement in urban planning by leveraging AI\ntechnologies and an inclusive, user-centric solution. The goal is to empower users to contribute\nmeaningfully while streamlining the workflow for administrators, fostering a deeper connection\nbetween public feedback and actionable outcomes.\n"
      },
      {
        "title": "1. Empowering Public Engagement",
        "content": "The solution will enable users to visually express desired space changes, helping them\nsee how their input directly influences urban design. This fosters transparency and builds\na stronger connection between public feedback and decision-making processes.\n"
      },
      {
        "title": "2. Leveraging AI for Actionable Outcomes",
        "content": "By using AI, the system will transform public feedback into real-time visualizations of\nurban space changes. This approach will help translate user input into actionable design\nproposals, while also tracking user preferences and identifying emerging trends to inform\ninclusive and sustainable decisions.\n"
      },
      {
        "title": "For Users",
        "content": "● A user-friendly web application designed to cater to all age groups and demographics.\n● Interactive features guide users through a seamless journey of translating their ideas into\nAI-generated images, enabling them to express preferences for space changes visually\nand intuitively.\n● Ensures inclusivity by accommodating diverse user needs and levels of digital literacy.\n"
      },
      {
        "title": "For Admins (URA)",
        "content": "● An administrative extension that streamlines engagement sessions through customizable\ncontrols for displays, questions, and interactions.\n"
      },
      {
        "title": "Content",
        "content": "● Analytical tools to aggregate and summarize public feedback, offering insights into\nemerging trends and priorities.\n● Enhanced decision-making through data-driven summaries of user input and\nAI-generated design proposals.\nThis proposed approach establishes a direct and efficient connection between public input and\nurban planning, addressing the problem statement's core challenges and creating a framework for\nsustainable and inclusive urban development.\n"
      },
      {
        "title": "Content",
        "content": "Design and Development of Application\n3.1 System Architecture\n"
      },
      {
        "title": "Technologies Used",
        "content": "All moving parts of the application are within one codebase that contains both the frontend and\nbackend of the application.\n"
      },
      {
        "title": "Frontend Type Script, React.js, CSS, Allows full flexibility in",
        "content": "and Material-UI creation of user interfaces\n"
      },
      {
        "title": "Backend Python Python handles all API calls",
        "content": "in a high speed without\nintroducing complexity\n"
      },
      {
        "title": "Database Firebase Effective for prototyping, and",
        "content": "presents a flexible low-cost\ndatabase structure\n"
      },
      {
        "title": "Image Hosting Cloudinary Free of charge, and effective",
        "content": "for prototyping due to the fast\nspeed and high quality of\nimages.\n"
      },
      {
        "title": "Application Hosting Frontend: Git Hub Pages Flexibility to setup a CI/CD",
        "content": "pipeline\nBackend: 72 Core Dual Xeon,\n64 GB RAM, Ubuntu + ngrok\nTunnel to Web\nTable 1 Technologies Used in System Building\n"
      },
      {
        "title": "Content",
        "content": "Database UML Diagram\nFigure 3: Database UML Diagram\nThe application was built mainly on four types of resources: engagements, questionnaires,\ngenerations, and users. This structure was chosen to ensure that the system is as customizable\nand dynamic as it can be. Here is the breakdown for each resource:\n"
      },
      {
        "title": "1. Engagements: Represents the engagement / event that URA holds.",
        "content": "○ image Url: Saved url of the engagement’s main image (the image users will be\nable to edit with)\n○ image Caption: Context given to the main image such that context can be passed\nto the backend APIs for better generation results.\n○ location: Google maps link of the location of engagement\n○ title: Title or name of the engagement\n"
      },
      {
        "title": "2. Questionnaires: Represents the questions that are linked to each engagement. These",
        "content": "questions can come in various forms, and having it as a separate entity allows for admins\nto dynamically add or remove questions from an engagement. Furthermore, the\nquestionnaire links up to the specific engagement in the database by being nested in this\nformat: questionnaires / engagement Id / questions\n○ questions: All the questions within that questionnaire; where each question entity\nincludes the question string, as well as the type and the choices that it can come\nwith\n"
      },
      {
        "title": "3. Generations: Represents the image generations that users submit.",
        "content": "○ engagement Id: ID link to the engagement event that the generation belongs to\n○ category: The category that the image falls under, created by a backend API\n○ coordinates: An array of coordinates that represent the highlighted portions of the\nimage if inpainting is used\n○ original Prompt: The original prompt that the user inputted\n○ upscaled Prompt: The upscaled prompt that the backend API generated, further\nimproving the original prompt that the user submitted.\n○ user Id: ID link to the user who created the generation\n○ voters: An array of IDs that represent the users who voted for this particular\nimage generation\n"
      },
      {
        "title": "4. Users: The users that sign up to the application; they are not connected to a specific",
        "content": "engagement since one user can attend multiple engagements.\n○ age Group, email, gender, name, postal Code: Particulars of user\n○ preferences: Represents the answers of the user to particular questionnaires\n3.2 Admin Interface (Create)\nFigure 4: User Workflow of Admin Interface (Create)\nThe admin dashboard has two workflows: the first being the process of creating the engagement\nthat users will interact with.\nThe process starts with the admin inputting the title of the engagement as well as the Google\nMaps link of the location. Then they proceed to upload the image that will be shown to all users\nduring the image generation section. This is accompanied by a context description which they\nhave to input as well. Lastly, the admin has to create the questionnaire in regards to that specific\nengagement that will be shown after the user signs up to the application. The questionnaire can\ninclude questions that are free-response type, or multi-select.\nInterface\n"
      },
      {
        "title": "Content",
        "content": "The admin begins on a landing page displaying an overview of all ongoing engagement\nsessions. To plan a new session, the admin can simply click the \"Create New Engagement\nSession\" button to start the process. The team wanted a page that the admins can see all\ndifferent engagement sessions at a glance, to avoid repetition of engagement sessions that\nthey have already created in the past. This is to also ensure quick access to any of the\nengagement sessions that they want to continue working on.\n"
      },
      {
        "title": "2. Create Title Page",
        "content": "Upon clicking the “Create New Engagement Session” button, the admin is prompted to\nadd a title for the session and has the option to input a Google Maps link. This link allows\nusers of the app to view the site directly in Google Maps Street View. The team designed\nthis such that users will all be evaluating the exact same coordinates for a fair\nengagement, otherwise different users may evaluate different parts of the assigned\nlocation.\n"
      },
      {
        "title": "3. Upload Site Image with Contextual Information",
        "content": "The admin can then upload images of the actual site, which will serve as the base for\nparticipants to generate their ideas. Additionally, the admin is prompted to provide\ncontextual information for each uploaded image. The contextual information added will\nbe helpful, especially to participants who haven’t visited the site before. This is to also\nhelp inform the AI model in the backend about more context on the image its processing.\n"
      },
      {
        "title": "4. Create Questionnaire",
        "content": "Next, the admin can add questions to gather responses from users. They have the option\nto include either multiple-choice questions or open-ended questions.\na. Multiple Choice Question: Participants are allowed to quickly select predefined\nanswers, requiring less time to brainstorm and craft their answers. With a Multiple\n"
      },
      {
        "title": "Choice Question, we are able to gather the public's opinion in a more structural",
        "content": "way as all the answers are uniform. We are also able to prompt the users to think\nabout certain themes or issues predefined by the urban planners.\nb. Open Ended Question: With Open Ended Questions, the goal is to try to collect\nas many responses as possible for the participants, these answers will then be\nuseful for the admin’s analysis. Participants are able to share their personalised\nresponses, and urban planners are able to gain a more comprehensive\n"
      },
      {
        "title": "Content",
        "content": "understanding of public sentiment.\n"
      },
      {
        "title": "5. End Create New Engagement Journey",
        "content": "Once the admin completes all the steps, the newly created engagement session will\nappear in the admin's home view and will be ready for participants to engage with.\n"
      },
      {
        "title": "Content",
        "content": "3.3 User Interface\nFigure 5: User Workflow of User Interface\nThe user interface has a straight-forward workflow. The team wanted to ensure that the user\ndidn’t have to toggle between a lot of screens, and just allow for a smooth linear progression\nfrom the start of the application to the end.\nThe user starts with being welcomed and introduced to the application, then proceeds to fill in\ntheir particulars, the questionnaire, and finally meet the characters that will provide feedback on\ntheir generation. The bulk of the workflow happens during the image workshop.\nThe image workshop has two tabs: one that allows the user to in-paint, and the other to input the\nprompt. If the user decides to click on the tools and in-paint, they are still required to input a\nprompt. The system will detect that the in-painting option has been picked, and it will send the\nrequest to our backend with models realistic-vision-v5-1-inpainting, and GPT 4 o-mini.\nImplementation and information on all the models mentioned in the workflows will be detailed\nlater in the report at Section 3.5. Essentially, the second model will upscale the prompt that the\n"
      },
      {
        "title": "Content",
        "content": "user inputs, to make it more descriptive. Then this upscaled prompt will be passed in the first\nmodel, alongside the in-painted image. The model returns back the generated image which will\nthen be critiqued by the same GPT 4 o-mini in which we have prompted with the different\ncharacters.\nIf the system has not detected any tools being used, that means that no in-painting has been done.\nIf this is the case, the model we are passing the image to is realvis-xl-v4. This serves as an\nimage-to-image generation. The prompt is still upscaled before that by GPT 4 o-mini, and the\nsame critique workflow applies after.\nOnce the image generation has been completed, the user can repeat the entire workflow again to\ngenerate another image, and so on. When they are satisfied with the final generated image, they\ncan then end their journey.\nAfter the end of the image workshop session, users will be directed to an image feed that they\ncan scroll through to view all other generations by other participants, as well as view each post\nindividually.\nThe next section will detail each of the processes described in this overview, as well as showcase\nthe user interfaces that are displayed in each step.\nInterface\nFor the interface, the goal here is to create a platform that is effective and intuitive to all user\nprofiles -- ranging from young students to older folks that may not be as exposed to technology.\nThe following are the various screens that the user will be exposed to:\n"
      },
      {
        "title": "1. User Welcome",
        "content": "The user will be first welcomed with a series of screens that introduces them to what the\napplication is about. On the last of the screens, the user will be able to click a ‘View Location in\nMaps’ button that will lead them to a direct link of the Google Map coordinates of the site. This\nis such that the user can explore the area in a 3 D street view, and understand the context of the\nlocation they’re engaging about.\n"
      },
      {
        "title": "2. User Particulars",
        "content": "The user will be led to a screen where they can input their essential information. The reason why\nthe team decided to collect postal codes is for the possibility that URA may want to start\nweighing the opinions of the users based on who actually lives in the area or not.\n"
      },
      {
        "title": "3. Questionnaire",
        "content": "The questionnaire is a series of questions curated by the admin, that collects the user preference\ndata. This is to allow the admin to find out more about the user’s thoughts on the location and\nwhy they may generate certain topics. The choices presented are displayed in the form of large\nbuttons, as the target users may have shaky hands (especially older folks), which will give them\ndifficulty pressing smaller radio-buttons.\n"
      },
      {
        "title": "4. Meet the Characters",
        "content": "This screen introduces the various characters that will be critiquing the user’s image generation\nlater on. The purpose of this is to inform the user on the various considerations that they may\nmiss out on whilst generating their image; considerations include topics such as accessibility,\nsafety, play, sustainability, and more. The team has decided to attach generated cartoon images\nfor each character, to give personality and allow the users to feel more connected to them.\n"
      },
      {
        "title": "5. Image Workshop and Character Feedback",
        "content": "This is the main section where all user image generations happen in the application. Here, the\nuser is greeted with a prompt input that they can fill in. Clicking the sparkle icon at the right\ncorner of the input section will then generate the image based on the prompt. If no inpainting has\nbeen done, then the system will call the image to image function to generate the image.\nOtherwise, if the user decides to use the tools and start inpainting on the image, the system will\ncall the inpainting function, in order to apply the prompt input only in that specific area. For the\n"
      },
      {
        "title": "Content",
        "content": "inpainting option, brush size can be adjusted to better suit the user’s needs. Users are able to\ndownload the image as well in order to save all their iterations.\nAll prompts in the input section will be upscaled in the backend, as users may input vague\ndescriptions that will cause the image generation to not produce accurate results. The team aimed\nto relieve the user from using too much cognitive load in curating the perfect prompt, and allow\nthem to instead prompt anything that they want and let their creativity flow.\nOnce the image is generated, a scrollable feedback modal will pop up. The user is required to\nscroll and read through everything before clicking the ‘Acknowledge’ button that will close the\nmodal. This is to encourage users to be more aware of the various issues that they need to\nconsider when generating such images.\nWhen the modal closes, the user can then scroll through all the iterations that they have created\nvia the image carousel buttons.\nThe session will be completed once the button ‘End Journey’ is pressed. This button is separated\nfrom the generation button, as users are able to keep on generating new images or end their\nsession. This design choice was not the best, as the users during the actual engagement had\ndifficulty distinguishing between the two buttons, and would accidentally click end journey --\nthinking they’re able to generate more images.\n"
      },
      {
        "title": "6. Generation Feed",
        "content": "Users are able to scroll through and view the various generations viewed by the other\nparticipants in the engagement session. This is to foster community interaction and provide\ninsight on what others in the community are thinking about.\nUsers can also like the posts that they enjoy via clicking the heart icon on the top right. The\nability to like generations are informative for the URA admins to know which designs do the\nmajority of the participants agree / resonate with.\n"
      },
      {
        "title": "7. Individual Generation Post",
        "content": "The last screen that users can see is the individual post details. This page can be reached by\nclicking a post from the feed. The information showcased includes the image generated, the user\nhandle, the upscaled prompt, as well as a heart icon that the user can click to like the post.\n"
      },
      {
        "title": "Content",
        "content": "3.4 Admin Interface (View & Analyze Data)\nFigure 6: User Workflow of Admin Interface (View & Analyze Data)\nAs mentioned earlier in Section 3.2 in the document, the admin dashboard has two workflows.\nThe second one is meant for the admin to view all the analytics and responses of the engagement.\nAfter the engagement session has been conducted, the admin can then view three types of data.\nFirst, the home analytics section showcases a range of user data in a summarized manner. It\ndisplays the participants list, regions of where the users come from, user profiles, and more\ncondensed information that can come in useful for them. For instance, the heatmap provides the\nparts of the image where the users have in-painted the most, indicating the areas the public most\n"
      },
      {
        "title": "Content",
        "content": "wanted changing. There is also a community image that can be generated here, which is\nshowcased to the participants to allow them to see their creations as part of the bigger picture.\nThe model gpt-4 o-mini processes all the saved upscaled prompts of the users during the\ngenerations, and combines them to form one descriptive prompt inclusive of all the ideas. This\nprompt is passed in realvis-xl-v4, which generates the image. A word cloud of the most popular\nwords are displayed in this section as well.\nSecond, the questionnaire section showcases a summary of all the question responses of the\nparticipants. Bar graphs display the multi-select answers, but the free responses have their own\nsection. Free responses are further categorized, as vague questions will elicit responses that span\nacross multiple topics. The team uses the models BERTopic and gpt-4 o-mini to break down\nresponses to its respective domains.\nLastly, the image generations section displays all generations created by the participants of that\nengagement. Admins can further analyze a generation by viewing its individual details alongside\nthe profile of the user who generated that image.\nInterface\n"
      },
      {
        "title": "1. Admin Landing Page",
        "content": "Upon accessing the Admin Dashboard, the admin can view an overview of all\nengagement sessions. They can click on any session to view its results and analytics.\n"
      },
      {
        "title": "2. Engagement Session - Home View",
        "content": "Upon selecting a specific session, the admin is directed to the engagement session’s\ndashboard home view. This dashboard offers comprehensive insights, including the\nparticipant list, regional distribution based on postal codes, demographics, highlighted\nareas, community image generation, and the most popular words. These analytics are\ncrucial for the admin to extract meaningful insights from public engagement sessions.\na. Participants List: The admin can view a complete list of participants who\ntook part in the engagement session and has the option to export all\nparticipant information as a CSV file.\n"
      },
      {
        "title": "Content",
        "content": "b. Location and Demographics: The admin can view the percentage of\nparticipants residing in different regions of Singapore, determined by the\npostal codes provided. This information is valuable as opinions from\nparticipants closer to the site carry more weight. Additionally, the\ndashboard includes age and gender data, helping the admin identify the\nprimary user group of the engagement session.\nc. Image In-Painting: The admin can visualize the areas highlighted most\nby participants on the base image. This helps identify the most popular\nareas for change and insights into participants' preferences and\nsuggestions.\nd. Community Image Generation: The admin can generate a community\nimage that encapsulates the collective ideas and sentiments of participants,\ncreating a single visual representation of the group's insights and\nperspectives. This community image is generated by collecting an array of\nuser’s prompts, and using GPT-4 o-mini to create a community prompt that\nencapsulates the overall participant’s ideas. The Community Image is then\nbeing generated with a Stable Diffusion model from img2 img, by\n"
      },
      {
        "title": "Content",
        "content": "inputting the community prompt generated.\ne. Most Popular Words: The admin can view the most frequently\nmentioned words from participants' image prompts, providing insights into\nthe textual elements of users' ideas and feedback.\n"
      },
      {
        "title": "3. Question and Answer View",
        "content": "The admin can view all participant responses from the engagement session, enabling a\ndeeper understanding of public opinions and insights.\na. Multiple Choice Questions: The responses will be reflected in a bar chart.\nAdmin is able to analyse the most popular answer as well as the number of votes\n"
      },
      {
        "title": "Content",
        "content": "for each answer with ease.\nb. Open Ended Questions: Admin is able to see all answers from the participants\nfrom the Open Ended Question. These answers are categorised via the model\ngpt-04-mini, to a category for each of the admin’s further analysis. Admin is able\nto get a sense of the topics which the public is mainly interested in.\n"
      },
      {
        "title": "4. AI Image Generations View",
        "content": "In this tab, the admin can review all ideation outcomes submitted by participants, with\neach output automatically categorized into relevant topics and organized by themes for\nefficient navigation and analysis. The dashboard leverages categorization techniques to\ngroup participant’s outputs into categories. This structured view allows the admin to\nanalyze participant ideas more effectively and draw actionable conclusions from the\n"
      },
      {
        "title": "Content",
        "content": "engagement session data.\na. Assigning Categories: Each generation by the participants will then be\ncategorized with our topic modelling model, the admin is then able to see\ngenerations sorted by topics, this eliminates the need for the admin to go\nthrough each image one by one to look for specific themes or topics.\n"
      },
      {
        "title": "Content",
        "content": "b. Participant Output Information: By clicking on a specific image, the\nadmin is able to view the participant’s image generation, the upscaled\nversion of the prompt, the number of upvotes, participant demographics,\nand their responses to any Q&A prompts. Admin is then able to analyse in\n"
      },
      {
        "title": "Content",
        "content": "detail each user’s train of thought that led to the generated image.\n3.5 Artificial Intelligence Models\nAs mentioned in the various parts of the interface walkthroughs, a crucial part of allowing the\nteam’s vision to come to life was with the use of AI models. This section will be broken down\ninto the various models used throughout the project.\nAll these models are running via a backend Flask application in the main codebase. These\nendpoints are being called in the frontend to connect it with the interface. The structure of which\nthese responses are returned, are all determined based on what is the most convenient way to\nmap out the data in conjunction with React Java Script.\nInpainting (Editing of Image)\nOne of the crucial sections of our application was the inpainting. To achieve this, we tried\ndifferent pipelines and methodologies.\n"
      },
      {
        "title": "1. Hugging Face Diffusers: Auto Pipeline for Inpainting",
        "content": "We initially were ambitious and used Hugging Face’s Auto Pipeline For Inpainting, a part of the\nDiffusers library. This pipeline is designed to perform inpainting tasks by taking an input image,\na mask image, and a textual prompt describing the modifications.\n"
      },
      {
        "title": "Content",
        "content": "We tested several models with this pipeline, including Stability AI Stable Diffusion 2\nInpainting (stabilityai/stable-diffusion-2-inpainting), Runway ML Stable Diffusion v1-5\n(runwayml/stable-diffusion-v1-5), and Diffusers Stable Diffusion XL Inpainting 1.0\n(diffusers/stable-diffusion-xl-1.0-inpainting-0.1).\nThis pipeline processes these inputs and generates a modified image that blends the masked\nregions with the rest of the content. However, this method required a powerful GPU for efficient\nprocessing. Even with an NVIDIA RTX 3090, the process took over 35 seconds for a single\nimage. This was a crucial metric when choosing the right inpainting pipeline as it really takes the\nusers out of the engagement process.\nWhile the outputs of the stable-diffusion-2 and stable-diffusion-xl-1.0-inpainting were highly\nrealistic, the computational cost made this approach unsuitable for large-scale or user-facing\napplications.\n"
      },
      {
        "title": "2. Open AI DALL·E 2",
        "content": "After encountering the limitations with the Hugging Face Diffusers, we decided to test Open AI’s\nDALL.E 2. As of recently DALL.E 2 allows for inpainting or outpainting an image by specifying\na mask and a text prompt. This approach promised a more accessible API and simplified the\ncomputational requirements, making it a suitable candidate for scalable and user-friendly\napplications.\nTo implement inpainting using DALL·E 2, we leveraged Open AI's Python API. The process\ninvolved uploading a square Portable Network Graphics (PNG) image as the base, and an alpha\nchannel mask to indicate the areas for modification (transparent regions signifying the editable\nparts). Along with these inputs, a descriptive text prompt detailing the desired edits was\nprovided. The API processed the inputs and returned a URL to the edited image.\nHowever, the quality of DALL.E 2’s outputs was not as expected. For simple scenes, it produced\nacceptable results, but for complex scenes, it often failed to deliver with most generations turning\nblanks or with random artifacts.\n"
      },
      {
        "title": "3. Realistic Vision v5.1 (Getimg.ai)",
        "content": "Getimg.ai stood out for its competitive pricing, with inpainting generations costing as little as\n$0.00075 per image, making it highly cost-effective for large-scale applications. Its lightning-fast\nperformance ensured near-instant results, even for high-resolution images. Furthermore, the\nplatform provided access to a wide array of models tailored to different tasks, such as inpainting,\n"
      },
      {
        "title": "Content",
        "content": "text-to-image, and image-to-image transformations. This versatility allowed us to experiment\nwith multiple models and choose the one that best suited our requirements.\nTo integrate Getimg.ai into our workflow, we used its inpainting API via Python. The Realistic\nVision v5.1 (from the stable-diffusion/inpaint family) Inpainting model was selected due to its\nability to maintain consistency in complex scenes and its efficient processing capabilities,\naligning with our requirements for scalability and quality.\nThe process involved resizing the input image and mask to a uniform size (512 x512), encoding\nthem into base64 format, and sending them to the API along with a descriptive text prompt. The\nAPI processed these inputs and returned a URL to the edited image, which was simply used as\nthe source for the <img> tag in the frontend. The API’s adjustable parameters, such as strength,\nguidance, and steps, allowed us to refine the outputs further, ensuring a balance between realism\nand prompt adherence.\nImage to Image\nAs we progressed with the project, user feedback revealed a clear trend: most users wanted a\nstraightforward method to transform their images without manually drawing over the images.\nWhile the inpainting feature offered some precise control, it often felt cumbersome to those who\nsimply wanted an overarching transformation of their entire image based on a textual prompt.\n"
      },
      {
        "title": "With Getimg.ai integrated into our workflow, we had access to a wide range of stable diffusion",
        "content": "models optimized for different styles and applications. After experimenting with multiple\nmodels, we settled on realvis-xl-v4 from the stable-diffusion-xl family. Its robustness in handling\ndiverse prompts made it the ideal choice for the img2 img feature.\nThe img2 img process begins with the user-uploaded image which is resized to 512 x512 pixels.\nThis resizing is model specific. The resized image is then encoded into a base64 format and is\nsent to the API along with a detailed textual prompt describing the desired transformation.\nThe payload sent to the Getimg.ai API includes the base64-encoded image, the user’s prompt,\nand the configured parameters. Once complete, the API returns a URL pointing to the\ntransformed image. The URL returned, was simply used as the source for the <img> tag in the\nfrontend.\nThe returned URL is preloaded to ensure the image is fully ready for display without delays.\n"
      },
      {
        "title": "Character Impact Analysis",
        "content": "The core idea was to leverage LLM agents to simulate human personas and provide actionable\nfeedback tailored to specific user groups. By modeling diverse human needs, the system ensures\n"
      },
      {
        "title": "Content",
        "content": "that users receive insights during each iteration of their design process. This feedback helps users\nunderstand how their designs align with or deviate from the expectations of different\ndemographic groups and guides them toward creating more inclusive and impactful solutions.\n"
      },
      {
        "title": "Character Focus",
        "content": "Mrs. Eleanor Tan Accessibility, healthcare, safety\nKumar and Priya Family amenities, education, safety\n"
      },
      {
        "title": "Ethan Lim Study areas, affordability",
        "content": "Table 2 Characters and Respective Focuses\nThe core mechanism involves crafting structured prompts that guide Open AI's GPT-4 o API,\nwhich supports vision inputs, to generate concise and actionable feedback in JSON format.\nThese prompts clearly describe the persona's traits and needs, enabling the API to focus on\ngenerating relevant insights.\nThe inputs sent to the API include the image URL, which serves as the visual context for\nanalysis, and a detailed prompt specifying the personas and their requirements. Initially, we\nconsidered using models like (Bootstrapped Language-Image Pretraining) BLIP or (Contrastive\nLanguage–Image Pretraining) CLIP to caption the image generations and use those captions to\ninform the analysis. However, since the image generation step already precedes this section,\nintroducing additional latency further disrupts the user experience, causing delays that could\nmake the app feel unresponsive and deter users from engaging with it effectively. Hence, we\nproceeded with using GPT-4 o.\n"
      },
      {
        "title": "Prompt Upscaling",
        "content": "During testing, we noticed that many users, especially elderly individuals, had trouble writing\ngood prompts for editing or generating images. They were unfamiliar with how to structure AI\nprompts and found it hard to describe their ideas clearly. To help, we added a feature that\nautomatically improves their prompts. This makes the app easier to use and reduces the effort\nrequired to create detailed instructions.\nWhen a user submits a prompt, they also specify the mode—either img2 img for broad\ntransformations or inpainting for targeted edits. The backend processes the prompt using a\nfunction called improve_caption, which sends the user’s input, along with the selected mode, to\n"
      },
      {
        "title": "Content",
        "content": "GPT-4 o-mini. We have crafted unique prompts for each mode, tailored to the specific\nrequirements of img2 img and inpainting tasks.\nThe refined prompt, designed to be clearer, more detailed, and actionable, is then sent to the\nimage generation process along with the input image and/or a mask image, ensuring precise and\nhigh-quality outputs that align with the user’s intent.\n"
      },
      {
        "title": "Word Generations",
        "content": "For the section on topic modelling on free response questions and as well as word cloud\ngeneration, we explored multiple methodologies. Referencing the provided tables, we compared\nOpen AI GPT, BERTopic, Topic GPT, and Custom Prompt Topic. To evaluate these\nmethodologies, we established runtime performance and coherence value as our primary metrics.\nAfter conducting a detailed comparison, we found that the performance between Open AI GPT\nand BERTopic was close. However, BERTopic demonstrated greater consistency when working\nwith larger text corpora, making it the final choice for our implementation.\n"
      },
      {
        "title": "Criteria Open AI GPT BERTopic Topic GPT",
        "content": "C_V Coherence 0.6924 0.7746 0.6382\nScore\n"
      },
      {
        "title": "Output Quality 7 Distinct, Distinct Topic Repetitive Topics",
        "content": "Well-Categorized Clusters with Centered Around\nTopics GPT-Enhanced 'Outdoor Recreation'\nLabels\n"
      },
      {
        "title": "Architecture Utilizes Open AI BERT-based Embedding",
        "content": "GPT4 with Prompt Embeddings Generation\nEngineering UMAP\nUMAP for HDBSCAN\nDimensionality\nReduction GPT for Labelling\nHDBSCAN for\nClustering\nc-TF-IDF for Topic\nRepresentation\n"
      },
      {
        "title": "Criteria Open AI GPT BERTopic Topic GPT",
        "content": "Runtime 1.28 s 13.35 s 185.77 s\nPerformance\nTable 3 Comparison of Models for Word Generation\nThe `initialize_basic_topic_model` function and the `generate_words` endpoint work together\nto enable topic modeling and word extraction using BERTopic. The\n`initialize_basic_topic_model` function sets up the BERTopic model to capture clusters while\nfiltering out noise. A minimum topic size of 3 ensures that only clusters with at least three\nsamples are considered, avoiding smaller, insignificant groups. The model's configuration\nincludes an n-gram range of (1, 2), which captures both single words and short phrases for better\ncontextual insights.\nThe input prompts are processed using the `fit_transform` method, which identifies clusters or\ntopics within the data. For each topic, the top words and their weights are extracted, and a\ndictionary aggregates the cumulative frequencies of these words across all topics. The words are\nthen sorted in descending order of importance based on their weights, creating a ranked list of\nterms most relevant to the identified topics. This list is returned as a JSON response back to\n"
      },
      {
        "title": "Topic Generation",
        "content": "The generate-topics endpoint builds upon the same methodology used in word extraction but\nleverages an additional feature of BERTopic: the representation model. This feature allows for\nrefining the raw word clusters into meaningful, high-level topics. While the clustering process\nremains similar—identifying clusters within the input data using embeddings, dimensionality\nreduction, and density-based clustering—the representation model steps in to label these clusters\nwith coherent, human-readable topics rather than just raw words. The endpoint ultimately returns\nup to 20 of these topics in a JSON response, ensuring that they are both meaningful and easy to\ninterpret.\n"
      },
      {
        "title": "The Community Image endpoint is the culmination of all the methodologies we have developed",
        "content": "so far, combining the BERTopic for meaningful representation and GPT-4 o for prompt\ngeneration, ultimately feeding into the img2 img (Getimg.ai) API for visual output. The goal was\nto create a community image that combines all contributions and showcase it back to the\nparticipants, helping them see how their ideas come together into a shared vision.\n"
      },
      {
        "title": "Content",
        "content": "The way it is done is by accepting a set of user-provided prompts, which are processed to extract\nthe most significant words using the generate_words method. Using BERTopic, this step\nidentifies clusters and aggregates the most important terms, effectively distilling the key\nelements from the input data. These words are then concatenated into a coherent string to form\nthe basis for further processing.\nNext, the extracted words are passed to GPT-4 o to generate a concise and context-aware Stable\nDiffusion prompt.\nOnce the prompt is ready, the endpoint prepares the user-provided image (if any) by resizing it to\nfit the required dimensions (maintaining an aspect ratio with a maximum size of 1024 x1024).\nThe image, along with the refined prompt, is sent to the img2 img API for processing. This final\nstep uses the realvis-xl-v4 model to transform the image based on the refined prompt, producing\na visual output that embodies the themes and ideas extracted earlier.\nWith that, the application has been fully built and it was time to test it with an actual engagement\nsession.\n"
      },
      {
        "title": "Engagement Session and Results",
        "content": "4.1 Choosing the Site for Engagement\nThe site was recommended by the URA as part of the ongoing Drafted Master Plan 2025 public\nengagement efforts. URA had previously engaged residents and stakeholders to explore ideas for\ntransforming the sheltered viaduct space below the Queensway Flyover into a new community\nnode.\nFollowing confirmation of the site, a site visit was conducted to analyze the area and its\nsurroundings. This included observing existing features, assessing accessibility, and\nunderstanding the current use of space to inform the application testing process.\nThe location is part of the Alexandra-Queensway Park Connector, which serves as a vital link\nconnecting nearby HDB estates to the Rail Corridor. The space is situated beneath a highway,\nfunctions as an underpass that facilitates accessibility and encourages active mobility, offering a\nseamless connection in the direction towards Tanjong Pagar and Woodlands.\n4.2 Choosing of Participants for the Engagement\nTo ensure a diverse and representative group, participants were selected across various\ndemographics, including age, gender, and background. The group ranged from university\nstudents to elderly residents, providing valuable insights into the differing needs, perspectives,\nand expectations of users across generations. This approach ensured that the feedback collected\nwas inclusive and reflective of the broader community's interests.\n4.3 Hosting of the Engagement\nThe application was tested during an in-person session with eight people, held at the URA\nbuilding. Due to timetable clashes within the participants, the team was not able to hold this\nevent in the actual engagement site.\nIn-Person Session\nThe application testing was conducted through an in-person session involving a total of eight\nparticipants. This number was chosen to ensure a manageable and focused group size, allowing\nfor thorough observation and data collection.\n"
      },
      {
        "title": "Session Structure",
        "content": "The session was structured to compare the traditional method of using the application with our\nnewly developed method. To achieve this, the participants were randomly divided into two\ngroups of equal sizes. This division enabled a controlled comparison between the two methods,\n"
      },
      {
        "title": "Content",
        "content": "ensuring that any differences observed could be attributed to the methods themselves rather than\nother variables.\n"
      },
      {
        "title": "Methodology",
        "content": "Each group was initially exposed to one of the two methods. One group started with the\ntraditional method, while the other group began with our new method. This initial exposure\nallowed participants to familiarize themselves with the respective methods and complete a set of\ntasks or interactions as required by the application.\nAfter the initial exposure, the groups were swapped. This meant that the group that initially used\nthe traditional method then switched to our new method, and vice versa. This swap was crucial\nas it ensured that each participant had the opportunity to experience both methods, thereby\nreducing any bias that might be associated with individual preferences or the order of exposure.\n4.4 Data Collection\nTo gather comprehensive feedback and assess the user experience, exit surveys were conducted\nat the end of each method's trial. To ensure consistency in data collection, the same set of survey\nquestions was used across both methods.\nThe survey was designed to collect both quantitative and qualitative data using a mix of\nmultiple-choice questions (MCQ), rating scales, and open-ended responses. This approach\nenabled a thorough evaluation of user satisfaction, workflow usability, and areas for\nimprovement, providing a well-rounded understanding of participants’ experiences.\nThe exit survey began by recording participants' names and the method they were allocated to.\nThis ensured that feedback could be analyzed at both group and individual levels, facilitating\ntargeted observations and comparisons.\nThe key survey questions were grouped into the following categories, each serving a specific\npurpose:\n"
      },
      {
        "title": "Overall Experience and Usability",
        "content": "● How satisfied were you with the overall experience of using the workflow? (Rating)\nReason: To measure participants’ overall satisfaction and the effectiveness of the\nworkflow in providing a positive user experience.\n● Was the workflow intuitive and easy to follow? (Rating)\nReason: To assess the usability and clarity of the workflow, ensuring it is easy to\nnavigate.\nStrengths and Areas of Flexibility\n"
      },
      {
        "title": "Content",
        "content": "● What did you like the most about the workflow?\nReason: To identify the key strengths and positive aspects that stood out to participants.\n● Did the workflow give you enough flexibility to edit or adjust the image? (MCQ)\nReason: To evaluate the degree of control users had in modifying the generated output.\nQuality and Alignment of Output\n● Did the generated image align with your expectations and prompt input? (MCQ)\nReason: To determine how accurately the workflow delivered results based on user input.\n● Rate the quality of the final image you generated. (Rating)\nReason: To measure the perceived quality of the generated images, which is critical for\nassessing workflow success.\nEngagement and Motivation\n● Did you feel engaged and motivated throughout the process? (Rating)\nReason: To assess the level of user engagement, which is vital for a positive and\nproductive experience.\nChallenges and Usability Issues\n● Did you encounter any difficulties throughout the process? (MCQ)\nReason: To identify any usability issues or obstacles faced during the workflow.\n● If yes, how many times did you need to retry or adjust the steps? (MCQ)\nReason: To quantify the frequency of challenges, providing insights into workflow\nstability and ease of use.\n● If yes, please describe the challenges you faced and how they affected your experience.\nReason: To gather qualitative data on the nature of the challenges and their impact on the\noverall experience.\n"
      },
      {
        "title": "Future Adoption and Improvement",
        "content": "● How likely are you to use this workflow again for similar tasks? (Rating)\nReason: To gauge the potential for continued use and measure the workflow’s value for\nfuture tasks or sessions.\n● What improvements or additional features would you like to see in the workflow?\nReason: To gather suggestions for enhancements and identify areas for further\ndevelopment.\n"
      },
      {
        "title": "Additional Feedback",
        "content": "● Do you have any other feedback about the workflows or your experience?\nReason: To provide participants with an open-ended opportunity to share additional\nthoughts or observations not covered in previous questions.\n"
      },
      {
        "title": "Content",
        "content": "This structured survey design ensures that all critical aspects of the workflow—including\nusability, user satisfaction, challenges, and future adoption—are addressed. The collected data\nprovides a clear, actionable foundation for analyzing user experiences and identifying areas for\nimprovement. Results from the survey will be analyzed in the following section.\n4.5 Results Analysis\nThe survey results from both trials were compared to identify any significant differences in user\nexperience, satisfaction, and performance between the two methods. This comparison was\nessential in evaluating the efficacy and user acceptance of our new method relative to the\ntraditional method.\nBy analysing the feedback and responses from both surveys, we were able to determine whether\nour new method offered any improvements over the existing one. This analysis included\nstatistical comparisons, thematic analysis of qualitative feedback, and an overall assessment of\nuser preferences and experiences.\n"
      },
      {
        "title": "Conventional Workflow (the AI Application",
        "content": "Metric higher the better, out of a (the higher the better, out of a\nmaximum of 5) maximum of 5)\nSatisfaction 4.0 5.0\nIntuitiveness 4.6 4.8\nExpectation 2.8 4.7\nFlexibility 3.9 5.0\nEngagement 4.3 5.0\nOutput quality 3.3 4.6\n"
      },
      {
        "title": "Content",
        "content": "Retention 3.3 4.6\nTable 4 Summary of Results from the Exit Survey Form\n"
      },
      {
        "title": "Quantitative Findings",
        "content": "The AI application demonstrated superior performance across all measured domains compared to\nthe conventional workflow, with some metrics receiving a perfect score. Most notably, user\nsatisfaction metrics showed a marked improvement, with the AI application scoring 5.0\ncompared to 4.0 for the conventional method. The intuitiveness ratings remained relatively\nconsistent between both approaches, with scores of 4.8 and 4.6 respectively, indicating that users\nfound both methods similarly approachable.\nA particularly significant finding emerged in the Expectation fulfillment category, where the AI\napplication achieved a score of 4.7 compared to 2.8 for the conventional method, representing a\nsubstantial 68% increase. This dramatic improvement suggests that the new methodology better\naligns with user requirements and anticipated outcomes.\nEngagement and Retention Metrics\nThe AI solution dramatically exceeded expectations, scoring 4.7 compared to 2.8 for the\nconventional method, showing just how much better it met users' needs and delivered on their\nexpectations.\nUsers found themselves far more drawn in and committed to the AI-powered approach. Perfect\nscores in flexibility and engagement (5.0) showed how much more intuitive and engaging it was\ncompared to traditional methods, which scored 3.9 and 4.3 respectively. The user retention\nmetric also saw an impressive jump to 4.6 from 3.3, highlighting how the AI solution not only\nkept users far more invested in the process and willing to return.\nIn particular, one aspect that surprised us was the significant jump in output quality, from 3.3 for\nthe conventional method to a 4.6/5 for our AI application. This surprised us as we used a more\ncapable AI image generation model in the conventional workflow. Yet participants ranked the AI\napplication’s outputs as a higher quality. We think this can be attributed to several process\ninnovations involved in the AI application, such as the prompt upscaling and user persona\nfeedback mechanisms, that helped our users better conceptualise and execute on their individual\nvisions.\n"
      },
      {
        "title": "Operational Benefits",
        "content": "The implementation of our AI application has also yielded several operational advantages. The\nability to administer engagements remotely through user-specific links has streamlined the\nprocess significantly. Furthermore, the transition from shared i Pads to individual mobile devices\nhas enhanced the ideation and feedback collection processes, resulting in:\n- Broader range of collected ideas\n- Increased diversity in feedback\n- Accelerated engagement completion times\n"
      },
      {
        "title": "Content",
        "content": "- Enhanced parallel processing capabilities\n"
      },
      {
        "title": "Future Implications",
        "content": "The data also suggests a strong likelihood of continued user adoption, with participants showing\n39% higher willingness to engage in future activities using the new system. This increased\nretention rate, combined with the demonstrated improvements in user experience and operational\nefficiency, indicates a successful implementation that warrants continued deployment and\npotential expansion of the system.\n4.6 Additional Findings\nIn addition to the results from the survey analysis, further insights were gathered through\nobservations of participant behavior during the session and informal interviews conducted\nafterward.\nObservations\nParticipants using the AI application workflow appeared highly engaged and interested, despite\nencountering questions and challenges along the way. They actively explored the process and\nwere eager to understand its features. In contrast, participants using the conventional workflow\ndisplayed less enthusiasm, appearing more reserved and passive throughout the session.\nDiving deeper into the various age groups, the team has observed that the older folks tend to be\nvery focused on the application and have lesser in-person verbal engagement. As most of them\nhave not been exposed to AI, they tended to feel a bit of stress and confusion, which distracts\nthem from the aim of discussing their generations with their peers.\nOn the other hand, the younger participants who have already had experience prompting and\ngenerating images, tend to be more social with their peers. They enjoy sharing what they have\ngenerated, and are curious as to how varying the prompts can produce very different results.\nInterviews\nAs each method’s session concluded earlier than expected, participants were introduced to the\nalternative workflow they had not tried. This allowed us to gather comparative feedback directly.\nOne participant noted that the conventional method fosters group interaction, as participants sit\ntogether, share ideas, and broaden their perspectives. The final result—such as a generated\nimage—felt like a collective effort, reflecting input from multiple participants.\nIn contrast, the AI application workflow was described as more direct and site-specific. Its core\nfunctionality, particularly in imprinting, ensured that the generated images were well-suited to\nthe intended site. Participants appreciated this as the primary differentiator. Additionally, they\n"
      },
      {
        "title": "Content",
        "content": "felt a sense of personal ownership over the AI-generated output, as it was based entirely on their\nindividual input. They enjoyed the element of surprise in seeing the final image at the end of the\nprocess and expressed satisfaction with the result.\n4.7 Future Work from Findings\nFrom the post-event survey results and additional findings, our future work will focus on\nenhancing the application to address the key insights gathered during the engagement session.\n"
      },
      {
        "title": "User Interface Improvements:",
        "content": "Several participants provided valuable feedback indicating that the current user interface requires\nfurther refinement to improve usability and intuitiveness. Key areas of improvement include:\n● Rendering Feedback: Some participants mistook the rendering process for lagging, as\nthe subtle blurring effect during image processing was not noticeable enough. Introducing\na more prominent visual indicator (e.g., a loading bar or animation) will provide clearer\nfeedback during processing.\n● Button Labeling: The “Submit” button was misinterpreted as “Enhance Prompt,”\nsuggesting the need to revisit button labeling and ensure that actions are self-explanatory.\nMoving forward, we will prioritize improving these elements to create a more intuitive and\nuser-friendly interface. This will help reduce confusion, improve workflow efficiency, and\nenhance the overall user experience.\n"
      },
      {
        "title": "Conclusion",
        "content": "This project demonstrates how integrating AI into urban planning engagement sessions can\nsignificantly improve the quality and efficiency of public participation. By leveraging\ncutting-edge AI technologies, the application facilitates seamless translation of public feedback\ninto actionable urban design inputs, fostering greater transparency and inclusivity.\nResults from the engagement trials show that the AI-driven workflow offers superior\nperformance compared to conventional methods, particularly in satisfaction, intuitiveness, and\nengagement. The participants’ ability to visualize their ideas as part of tangible urban design\noutcomes strengthens their connection to the planning process.\nWhile promising, future work should address user interface refinements and expand accessibility\nfeatures to ensure the platform remains intuitive for diverse user demographics. By continuously\niterating on this foundation, the application has the potential to become a benchmark for\nparticipatory urban planning nationwide, exemplifying how technology can enhance\ncommunity-driven development and sustainable growth.\n"
      }
    ],
    "metadata": {
      "title": "Final Technical Report",
      "category": "project_report",
      "file_name": "Final_Report",
      "relative_path": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/Final_Report.pdf",
      "page_count": 53,
      "project_name": null,
      "file_size": 22216897,
      "last_modified": 1749024948.5172703
    },
    "word_count": 10160,
    "page_count": 53
  },
  {
    "id": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f_AID_Project_2",
    "source_file": "/Users/weimingchin/Desktop/weiming_chatbot/data/raw/notion_export/Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/AID_Project_2.pdf",
    "type": "pdf",
    "title": "Opportunitiesfor Design Improvementsfor Nothing Phone1 using Artificial Intelligence",
    "category": "academic_paper",
    "raw_content": "\n--- Page 1 ---\nOpportunitiesforDesignImprovementsforNothingPhone1usingArtificialIntelligence\nAUTHORS STUDENTID\nHeTianli 1006151\nLimYuJie 1005999\nAdityaKumar 1006300\nEthanChooE-Rhen 1006324\nCaitlinDaphneTanChiang 1006537\nChinWeiMing 1006264\nAllteammemberscontributedequallytothetask.\nSingaporeUniversityofTechnologyandDesign\n60.002:AIApplicationsinDesign\nDr.KwanWeiLek,DrEdwinKoh,MrMichaelAlexanderReeves\nApril16,2023\n\n--- Page 2 ---\n1.Introduction\nIn recent times, technology has been advancing at a rapid rate and in recent times,there has been a\nsudden boom in interesttowardsArtificialIntelligenceandMachineLearning.Thisisnotunexpected\ngiven how society is progressing towards digitization where more and more people are becoming\nglued to their smartphones,beittobrowsethroughsocialmedia,doingonlineshoppingetc.Withthis\nimmense surge of people going online, there is also a huge amount of data being uploaded every\nsingle second, and these data are the honeypots of feedback and consumer taste and preferences\nwheremanycompanieswouldbeinterestedinobtainingtodeveloptheirproduct.\nHowever, the issue lies with the immense amount of data online, how are we able todiscernwhatis\nrelevant and irrelevant to what the companies are focusing on for design improvements. In this\nproject we will be exploring and categorising largequantitiesofdataextractedfromonlineplatforms\nthroughtheuseofArtificialIntelligencetofindpossibledesignimprovementsofaproduct.\n1.1ReasonforchoosingtheNothingPhone(1)\nWe have chosen to implement AI machine learning on the Nothing Phone (1), a phone that was\nannounced in January in 2021 and released in July 2022. It was a highly anticipated phone that\ncirculated a lot of discussions among the technology geek community, and the general view for this\nphone is that it is very controversial in thesensewhereitisnothinglikewehaveseenbefore.People\nwere excited for the final reveal and release of the phone back in July 2022, but just to be\ndisappointedonhowitactuallycameout.\nThe company that came out with this product is called ‘Nothing’. This company was foundedon29\nOctober 2020, so it's still a relatively newcompanywhencomparedtootherphonecompanies.Asof\nMarch 2023, the company has only released 4 products, 3ofwhicharewirelessearbuds,andthelast\nproduct being a smartphone. Being a product which does not have a subsequent model, there are\nbound to be issues that the smartphone faces when used by consumers in the mass. Thus, our group\ndecidedtorideontheopportunityandfocusonNothingPhone(1).\nOne application we can implement AI and machine learning is what we will be exploring in this\nproject where we would be utilising machine learning models tofindroomsforimprovementsofthe\nproductNothingPhone(1).\nWe have chosen to find design opportunities and improvements on “Nothing Phone 1” asitgotalot\noftractionduringitsprereleaseduetoitsuniquedesign.However,afteritgotreleasedtherewasalot\nof bad feedback. Hence we think that this product has potential areas for design opportunities and\nimprovements. Toidentifydesignopportunitiesandimprovements,weuseddata-driventextanalytics\nand classificationmodels.Ultimatelywewishtoseekspecificdesignrequirementswhichcanbeused\nasaguidelinewhendesigningthenextiterationofnothingphones.\n1.2Methodology\nFirstly, we decided to get comments and reviews from varioussourcestogetthegeneralideaofhow\npeople view the Nothing Phone 1. We have narrowed downto3maindatasources:Youtube,Reddit\nand Websites (web scraping). There are 3 main steps to go through in order to do text analysis and\nclassificationonaproduct.\nDataextraction,datacleaning,dataanalysis.\n2\n\n--- Page 3 ---\n1.3OverviewandWorkflowofproject\nSettingup:\nThis project requires the use of multiple libraries, classification models as well as text models that\nneedtobeinstalledatthebeginningoftheproject.\nLibrary/Models Description/Use\ngooglesearch ‘Search’:searchesthetermongoogle\n“build”:Thegoogleapiclient.discoverymoduleisusedtobuildaserviceobject\nthatcaninteractwithaGoogleAPI.Thebuild()functionisusedtocreatethis\nserviceobject.\nrequests SendhttprequeststointeractwithwebAPIstoscrapewebpages\nBeautifulsoup4 ConvertsHTMLcodeintoanobjectthatcanbeusedtofindelementsinapage\npickle Toserialize/deserializepythonobjectsandsavestateofprogramsforsending\ndatabetweenprocesses.\nos ThislibraryprovidesawaytointeractwiththeoperatingsysteminPython.It\nincludesfunctionsforworkingwithfilesanddirectories,launching\nsubprocesses,andmanagingenvironmentvariables.\npandas Datastructuresforefficientlystoringandmanipulatinglargedatasets,and\nfunctionsforcleaning,transformingandanalysingdata.\n3\n\n--- Page 4 ---\ngoogle_key UseofGoogleAPIkeystoauthenticateandauthorizeaccesstogoogle\nservices,inthisproject,itwillbeusedwhenwebscrapinggooglesearch\nresults\nre Usedforpatternmatchingandstringmanipulation,toextractinformationfrom\ntext,validateuserinputsetc.\nNumpy Libraryforworkingwitharraysandnumericaloperations\npraw RedditAPItoobtaincommentsfromreddit\nlangdetect Libraryfordetectinglanguageofagiventextstring,usedforcontentfiltering\nandlanguage-specifictextanalysis\nnltk NaturalLanguageToolkitfortextanalysis\nopenai ToenableustoaccesstoChatGPTusingpython\nBefore gathering data to perform text analysis on, a variety of websites were considered for data\ncollection, we mainly chose websites that are popular and tend to have a lot of reviewers. However\nduring our selectionphase,werealisedthatcertainmainstreamwebsitesdonothaveanythingtooffer\nfor the Nothing Phone(1), such asthecaseofAmazon,ShopeeandLazada,wheresearchingtheterm\ndoesnotgivebacktheproduct.\nHence we decided to do data extraction on Youtube, Reddit and Websites that do have reviews and\ncommentsontheproduct.\nYouTubeDataExtraction:\nBefore we are able to extract data from youtube, we have to set up the YouTubeDataAPI.APIsare\nunique to every individual’s google account. Itisneededforustobeabletoaccessallyoutube’sdata\nforexamplevideos,comments,titlesandmanymoreofyoutube’sdata.\nThe first step of extracting youtube data is first to specify what is the search terms that you are\nsearching for, which in our case will be the ‘nothing phone (1)’.Nextistospecifyhowmanyvideos\ndo you want the code to run through under the variable “max_results”. Afterwhich the code will\nessentially iterate through each video’s comments, and even furthermore each comment’s replies.\nWhichallthesesentenceswillbeappendedonebyonetoaninitiallyemptylist.\nWe utilised the pandas Dataframe to store each of the comments with the list of comments formed\nwhen running the code. And saved it as a csv file in the end to avoid needing to run it again in the\nfutureasthiscodetakesawhiletofinishrunning.\nRedditDataExtraction:\nFirstly, create a reddit account.Then go to https://www.reddit.com/prefs/apps on your web browser\nandclickonapps>createapps\nChoosethe“script”optionandsetbothurlsas“https://localhost:8080”,thenclickcreateapp\n4\n\n--- Page 5 ---\nAftercreationwewillbegiventheclient_secretsandclient_IDwhichwewillbeusinglater.\nNext,pipinstallPRAWlibrary,thisisthelibrarycreatedspecificallytowebscrapereddit.\nInattemptstokeepdataextractiontobeasautomatedaspossible,r/allwhichisourhomepage\nThe function ‘search_term’ is the value we would need to change depending on the product you\nchoose.\nInputs: ‘reddit.subreddit(‘all’)‘, searches through the whole of reddit, if we want to search througha\nspecific subreddit we just replace ‘all’ with the subreddit you want to do the scrapping\non.’search(search_term, sort=’relevance’, limit=100)’, ‘sort’ allows you to choose which\nrecommendation of subreddits to look into, relevance is just to keep the results closelyrelatedtothe\nproduct we have search. Whereas ‘limit’ just sets the maximum number of subreddits we will be\nscrappingfrom.\nThefunctionforpostinposttakesinthetitleandtheselftextandcollatesitas1comment\nThenthecomments.replace_more(limit=25)functionloadsthatpostandtheircomments\n.append({‘Comment’:comment.body}) adds comments post and self text under a dataframe column\ncalledComments\ndf[‘comment’].str.replace(‘\\n’,‘‘)removesthespacesandlinebreaksfromthecomments\nWebsiteExtraction\nWe also decided that the websites obtained from google will be useful for our project. Thereforewe\ncreated a code that is essentially able to get the comments and text from these websites to aid our\ndesignprocess.\nFirstly, we specified our search_terms, which is ‘nothing phone (1)’ for this project. Then using the\ngoogle_search library, wewillgetalistofURLofthetop10websitesofour searchresultongoogle.\nAfterwhich, we defined a get_sentences(link) function which willbeabletogetallthesentencesina\nwebsite into a list when given a website link. And we iterate for each URL found using the\ngoogle_searchlibrary,werunitthroughtheget_sentences(link)functionandeventuallyendupwitha\nlistofallthesentencesofall10websites.\nFinally using pandas dataframe, we store the list of sentences into a dataframe and exportitasacsv\nfile.\n5\n\n--- Page 6 ---\nCombinationofall3datasources\nThe final step of our data extraction process is combining all comments retrieved from youtube,\nreddit and web scraping into a single dataframe and eventually saving it as a csv file as\n‘All_CombinedRaw_Data.csv’.\nHow we did it is that we read all the csv files that were created when extracting data from youtube,\nreddit and web scraping as a pandas dataframe. Andconcatenatingall3dataframesinto1dataframe\nandexportingittocsv.\n2.DataCleaning\nBefore running analysis and classification modelsonthedatasets,itisessentialtododatacleaningto\nremove anythingthatdoesnotprovidevaluetoourprojectgoalsoffindingareasofimprovementsfor\ntheNothingPhone(1).Ourdatacleaningprocessinvolvesthesestepsbelow:\n1. Tokenisation\n2. RemovalofStopwords\n3. Lemmatization\n4. Splitneutralterms\n5. Languagesplit\n6. Remove1wordcomments\nThe code for the data cleaning is all in the “Clean_data.py” code folder. Our inputs for this process\nwill be the extracted comments CSV file from our data collection process. In the code, all these\nfunctions would run on the same file but we will be going in depth to what each of these cleaning\nfunctionsdoesandthejustificationindoingso.\n2.1Tokenisation,Removalstopwords,Lemmatization\nTokenization is the first step to do when doing data cleaning, it splits the comments into individual\nwords or terms called tokens. It is an important first step in data preprocessing as it is required for\nlemmatization. Lemmatization is used to remove inflection by determining the part of speech and\nutilising a detailed database of the language, reducing a given word to its root meaning to identify\nsimilarities e.g. from ‘better’ to ‘good’, it derives the meaning of a word from a dictionary. The\nfunction ‘Lemmastise_sentence’ tokenise the sentences and lemmatises it and then joins the words\nbacktogether.\nStopwords are frequent words used in the english language that do not have specific semanticssuch\nas “the”,”is” etc, removing them prevents common words from showing up forcommonwordsused\nduring the data analysis step such as when we wish to detect the most used word. The function\n“remove_stop_words”identifiesnon-semanticwordsandremovesthemfromthesentences.Doingall\nofthiswillincreasesearchperformanceastherearelesserwordstoprocess.\nIt is important to note that we also remove the words “nothing”, “phone”, “1”, and “one” fromeach\nsentence in the data too. Reason being wepredictthatthesewordswillaffectourresultwhenfinding\nout the words that appear the most in thelaterstage.Asoursearchtermis“nothingphone(1)”,most\ncomments describing this product will have a high probability of having the name of the product in\ntheir sentences too. So in order to avoid the top words being the name of theproduct,wedecidedto\nremovethenameoftheproductinourdata.\n6\n\n--- Page 7 ---\n2.3Removalofemojis,links,duplicatedcommentsandlowercasingwords\nLinks, emojis and duplicated/repeated comments are then removed from our dataset through the\n“clean_comment()” function. These are removed as it will not be relevanttous;althoughemojiscan\ngive us an emotion towards something, it will not beabletoprovideusexactdetailsinwhattheyare\nreacting to, i.e. what specifications, hence we remove it as it will bedifficulttoobtainspecifications\nfromthat.\nAs we are focusing on text emojis, we cannot really discern much from emojis so we will be\nremoving it to reduce our data. Additionally, links incommentswouldnotberelevanttoourprojects\nasmostofthemwouldbeforadvertisement.whichisnotusefulforourproject.\nWe also used the pandas dataframe’s drop_duplicate() function to remove all the entries that were\nrepeated. And later on resetting the index of the pandas dataframe to make it easiertovisualisehow\nmanyentriesareleftinthedataframe.\nWe also converted all characters inthedatatobelowercase.Thisissothattherewontbeanyerrorof\nupper and lower case when we try to search for any specific words within a comment in the later\nstage. Therefore we standardise for every single character of every comment in the data to be\nlowercase.\n2.4Splitneutralterms\nWe also have split long balanced comments with neutral words like 'but' and ‘however’' to rule out\nthe comments that are neutral, essentially splitting up one long comment into 2 comments; positive\nandnegative.\nWe proceeded to ask Chatgpt what are the possible words or terms that can make a sentence sound\nneutral. We then check if any comments consistofanyoftheseneutralwords,andifthereis,wewill\nsplitthesentencewiththatword.Belowisapictureofchatgpt’sresponseoftheneutralterms.\n2.5Removalofnon-englishcomments\nAfter this, in the dataframe of comments, we added another column named “language”. In this\ncolumn, essentially we used the function detect() from the langdetect library in python, so that it is\n7\n\n--- Page 8 ---\nable to go through each sentence in the dataframe to determine whatlanguageitisin,andlabeleach\nsentence’slanguageinthecolumn“language”.\nAfter we have a column of language labels for each sentence, we proceed to onlytakethesentences\nwhichareonlyclassifiedasEnglishtomovefurtherintoourproject.\n2.6Remove1wordcomments\nThis step is the last step of our data cleaning. How wedecidedtoremovethe1wordedentriesinthe\ndata is because when we looked at the data, we realisedthattheseonewordingwillnotgiveusgood\ninformationregardinghowtoimprovetheproduct.\nTake for example in the data there is the entry“battery”.Withoutanymorewords,wewilldefinitely\nbe able to deduce whether this is a positive or negative comment. Which will be analysed in the\nfurther part of this project. So considering the time taken to run analysis and thesizeofthedata,we\ndecided that itisawasteoftimeandcomputationalefforttoletthemodeltrytodeduceaone-worded\nentry whether it is a positive or negative comment. We also triedthinkingofone-wordedentriesthat\nmay be beneficial data to our design requirements but we could not thinkofanexample.Whichled\nustoultimatelydecidetoremove1wordcommentsfromthedata.\nHow wediditiswecheckifthereisanywhitespaceineachcomment.Andifthereisnowhitespace\ninthatcomment,indicatingthatitisaone-wordedentryinthedata,andsoweremovethisentry.\n3.DataProcessing&Analysis\nAt this stage, we will have a huge data of all the cleaned and ready to be processed and analyse\ncommentsfromvarioussources.\nOur group is left with a total of 34425 comments from all 3 data sources. However, it is from here\nonwards we have decidedtoremovethedatafromscrapingwebsitesfromourdatasetasitcontainsa\nlot of irrelevant text due to it including words from advertisements, headers,directoryetcanditalso\nrequires huge amounts of time and computer resources to run the whole data set through Sentient\nAnalysis.\nRemoving it leaves us with 29155 comments. But the issue is thatbecausethemodelthatwewillbe\nputting these comments into is adeeplearningmodel.Meaningthatitwilltakeaconsiderablylonger\ntime to analyse one comment. Given that we have 29155 comments, we do not have the time and\nresourcestorunitforallofthesecommentsasweestimatedthetimetakenforall29155commentsto\nbeanalysedbythemodelisaround22hours.\nTherefore, we decided to randomise our comments from Youtube and Reddit and randomly picked\nout 15% of the data. Leaving us with a sampleof3340commentsfromthehuge29155commentsof\nthe original dataset. Webelievethatthisisasmartmoveasitwillhugelyreducethehoursrequiredto\ndo the analysis and classification of our data and we believe that it will not affect the final result of\nour project. But we noted that inanidealsituation,weshouldrunallofthedataintothemodeltoget\nthe maximal possible accurate final output of the project given that we have enoughtimeandstrong\nenoughcomputerstorunthecomputations.\nFor this part of the project, we did research and decided to try 4 models on our dataset.Namely,the\nSiebert Sentiment Analysis and Zero shot classification which we retrieved from a website called\nhuggingface,VaderAnalysis,aswellastopicmodelling.\n8\n\n--- Page 9 ---\n3.1SiebertSentimentAnalysis\nTo run the Siebert Sentiment Analysis onourdatasetusingthehuggingfacewebsite,ourpythoncode\nuses a pipeline to connect the huggingface model to our code. There also is a function called\nanalyze_sentiment(comment) so that we can iterate through all comments and pass it on to this\nfunction for analysis. The function will then analyse whether a comment is positive or negative and\nlabelitinanewdataframecolumnaseither“POSITIVE”or“NEGATIVE”.\n3.2VADERmodel\nAt the same time, we have decided to use the VADER model which is an alternative to Sentiment\nAnalysis that is more suitable in discerning social media comments . It is during this process we\nrealised that the VADER modeltooklessertimeinclassifyingthesentimentvalueshencewedecided\nto run our full Dataset through it (29155). The key difference in terms of output is that this model\nclassifiedtheneutralcommentsaswell.\n3.3ExtractingnegativesentimentcommentsfrombothVADERandSiebertSentimentAnalysis\nComparing the 2modelswhichcandothesamethingcanbedoneverydifferently.Wedecidedtouse\nSiebert'sSentimentAnalysisfromhuggingfaceoverVader.Herearethereasonswhy.\n1. As seen in the figure below, Siebert’s Sentiment Analysis is able to identify more negative\ncomments in the dataset and Vader actually classified the least negative comments on the\nother hand. Looking at this bar chart, we deduced that Siebert is able to analyse a comment\nbetterthanVader,whichiswhySiebert’smodelwilltakealongertimetorun.\n2. Siebert Sentiment Analysis is a deep learning model, which means it is able to analyse\ncommentsmoreaccurately.\n3. Too manycommentsthatwentthroughtheVadermodelareclassifiedasNeutral.Becausewe\nbelieve that we already successfully ruled out all the neutral comments in the cleaning data\npart of the process. Therefore the fact that so many comments are still classified as neutral\ntellsusthattheVadermodelmaynotbesuitabletouseinourcase.\nTherefore moving on further into our project, we utilised the data that ran through the Siebert\nSentiment Analysis model. The output after running this model is a column in our dataframe,\nindicating whether that comment is a positive comment “POSITIVE” or a negative comment\n“NEGATIVE”. We then took out all thecommentsthatwerelabelledas“NEGATIVE”becausethese\nwill be the comments that will be more valuable to us when deducing the finaldesignrequirements.\nWhereasthepositivecommentswillnotbeabletotellushowwemightimprovethephone.\nFigure:VADERvsSentimentAnalysispolarityresults\n9\n\n--- Page 10 ---\n3.4ComparisonsofresultsbetweenSAandVADERforextractednegativecomments\nThe VADER model returns us with only 494 negative comments out of the full 29155 comments\nwithin 5 seconds and sentiment analysis returns us with 1773 negative comments out of3340which\ntook68minutes.\nFigure:ComparingVADERandSAmodels\n3.5 TopicModelling\nThis model is actually suggested by Chatgpt. Upon finalising our work plan for this project, we\nwondered if there is any other way we could go about this project in a better way. So we turned to\nChatGPT.BelowisascreenshotofourconversationwithChatGPT.\n10\n\n--- Page 11 ---\nEssentially what this model does is it goes through all of our comments, then it will try to find out\nwhatarethekeywordsineachofthesesentencesandgivesalistofkeywordsforeachcomment.\n3.6 ZeroshotClassification\nFormingcategoriestoclassifycommentsintobeforeputtingintoZeroshot:\nWe managed to write a python code that is able to ask ChatGPT to provideuswithcategoriestoput\ninto whendoingourZeroshotClassification.Reasonwhywedidthisissothatevenwhentheproduct\nofinterestchanges,itwillstillbeabletogiveuscategoriesregardlessofwhateverproduct.\nTo perform Zeroshot Classification on our model, wefirsthavetohavealistofcategoriessothatthe\nmodel knows what are the options it can classify each sentence into and later on proceed to classify\nthem into the category which the model thinks the sentence is more related to. So for the output of\nthis model after we ran it with our data, it is an additionalcolumninthedataframe,labellingwhatis\nthecommenttalkingaboutgiventhatthecategoriesweregivenbyChatGPT.\n11\n\n--- Page 12 ---\n3.7 ZeroshotClassificationorTopicModellingandtheinterpretationoftheresult\nWe eventually decided thatZeroshotClassificationwillbebetterthanTopicModellinginourproject.\nAndhereisthereasonwhy.\nThekeywordsofTopicModellingaretoowide.BecauseforTopicModellingthemodelisidentifying\nthe keywords for each sentence by itself. Therefore for every sentence, there will be a very diverse\nrange of keywords, which we will not be able to really deduce what this sentence is talking about.\nWhereasforZeroshotclassification,thecategoriesaregivenbyus.Soitwillclassifyeachsentenceto\nwhat we need to know about. Therefore eachsentencewillbefinetunedtoourcategorieswhichwill\nbeusedtoidentifydesignopportunities.\nFrom the result of the Zeroshot classification, we didabarcharttofindoutwhichisthecategorythat\nhas the most comments. In other words, which are thecategoriesofthephonethatpeoplehavemore\nnegative things to talkabout.Fromthefigurebelow,wefoundoutthatthetop4categoriesofnothing\nphone (1) that most people have bad things tosayaboutisFeatures,Design,Price,andPerformance.\nWhich will be the 4 categories we choose to focus more on when deducing our final design\nrequirements.\n12\n\n--- Page 13 ---\nFigure:DistributionofnegativecommentsfromSentimentanalysis\nAs can beseenbytheabovefigure,mostofthenegativecommentsaredissatisfiedabouttheFeatures\nof Nothing Phone (1), due to the overwhelming negative comments on this specifications as\ncompared to other specifications, we decided to focus on designing improvements for the features,\ndesign, price and performance of the Nothing Phone(1) which are the top 4 classes thatreceivedthe\nmostnegativecomments.\n3.7WordCloudVisualization\nFigure:WordcloudonSentimentAnalysis\nWe ran all the negative comments we obtained into the nltk Wordcloud text analysis to get a\nvisualisation of the most common words that appearedforeverycategory,wherethelargertheword,\nthebiggerthelettersize.\nEssentially, for each class we counted how many times each word appears, when we take thetop25\nhighestfrequencywordsanddisplayitinaWordcloud.\n3.5Extractedcommentsforfinalevaluation\nAfter we successfully obtained the words that appeared the most for each class of comets asseenas\nthe Wordcloud figure above. We thought that these words will not give us the full understanding of\nwhatthesereviewsaretrulytalkingabout.\nSo we track back a little to get the comments data that has the top3wordsthatappearedthemostin\nthe comments. How itworksisthatwefoundoutwhatarethe3highestappearancefrequencywords,\n13\n\n--- Page 14 ---\nthenwewrotethecodeinsuchawaythatforallcommentsthathaveanyofthe3topwords,weputit\ninanewdataframeandexportitasacsvfile.\nLastly as final evaluation, we analyse each comment in the csv file and finally deduce our design\nrequirements.\nDesignOpportunitiesandtheirJustifications\n1. FindingNothingPhone’sownidentityinthesmartphonemarket\nThere are many comments on how the Nothing Phone (1) is highly similar in terms of looks to the\niPhone, with some people even calling it a cheap version of the iPhone. As seen in the Wordcloud\nimage for ‘Features’, “iphone” is one of the top 3 words that were mentioned in the comments. The\nfact that most comments referred to the iPhonejustgoestoshowthattheNothingPhone(1)doesnot\nhave a trademark of its own that peoplecaninstantlylinktotheNothingcompanyimmediatelyupon\nseeing one of its products. For example, the iPhones are recognised for their flat edged bezel, while\nthe Samsung Galaxy series are known for their curved edged screens. IftheNothingPhone(1)hasa\nfeature unique to the brand, peoplewillstopcomparingtheNothingPhonetootherphonebrandsjust\nbasedonhowitlooksatfirstglance.\n2. Differentsubmodelswithdifferentspecifications\nThere were people who commented on how they want higher processing power while others\nmentioned that having a smaller battery and charging asmallerpricewouldmaketheNothingPhone\n(1)’s price range be a good mid rangepriceforaphone.Sobyhavingdifferentsubmodels,therecan\nbe different priceschargedfordifferentcombinationsofspecificationsthatwilltailortodifferentuser\nneeds.\n3. Newbackingforthephone\nConcernswereraisedaboutthewaterproofingforthephone,andtherewerealsocomplaintsaboutthe\nbacklights either being excessive or useless. One commenter mentioned that instead of having a\nbacklight, there could be a stronger back casing that does not shatter. Hence, the Nothing Phone (1)\ncan have a completely new backing where it takes all these comments into account. In other words,\nbalancingthefunctionalityandpracticalityofthedesigntogetherwiththeaestheticsofit.\nReflections\n1. Time taken:it take very long time to run SA to dataset, requires desktop/ stronger computer\nthatcanrunforhours\n2. Irrelevancy: Despite multiple cleaning,therewouldstillbecommentsthatareleftbehindthat\nareirrelevanttoourprojectgoals\n3. Detection isirrelevant,here,asourproductiscalled“NothingPhone1”itleavesuswithalot\nof ‘phone’ and ‘nothing’ in thewordclouds,wherenothingcanbeusedtorefertothephone,\nitcanalsorefertocriticismoftheproduct.\nSummaryoflessonlearnt\nTo conclude our report, we will be sharing a summary of our lessons learnt when doingthisproject,\nandwouldwehavedonedifferentlyifweevercomeacrossasimilarprojectagain.\n14\n\n--- Page 15 ---\n1. Deeperresearchoneachmodel’salgorithm.\nIn our project, we rely heavily on existing models and what we did essentially isonlytoapplythem\nwithout cleaned data. While we did some experiments and hadourowninterpretationofwhycertain\nmodels are better and certain models are worse. A lesson learnt is to do more thorough research on\nthe models that may be useful to our project, and study its algorithm answer the question of, will it\ntake longer to run?, how is it classifying certain things?, how does it deal with bad and irrelevant\ndata?, is it really suitable for comments/reviews classification or is it actually more for essays or\narticles?\n2. Datacleaningisnotalinearprocess,butmoreofatrialanderrorprocess.\nWhen we were doing our data cleaning, it was an iterative process. Reason being it was a constant\nprocess of us completing one part of the cleaning, then checking manually on the data itself to\nidentify what could be done better to the data. Take for example when we realised that there were a\nlot of random 39s in our dataset which needed toberemoved,whenwerealisedonewordcomments\nwere invaluable for our study, and when we found out that some comments were repeated or non\nenglish comments. It was an iterative process of checking the data,findingoutwhatwaswrongwith\nit, and coming with the code to clean it more thoroughly. So do not expectthecleaningdataprocess\ntobeaoneshotfinishedprocess.\n3. Differentsourcesfordifferentproducts\nFor different products that you are interested to do more research on,therewillbedifferentwebsites\nthat have more data on certain products. For our project, we manually checked which were the best\nsources to get data about the nothing phone (1) from and landed with reddit, youtube, and top 10\ngoogle websites. So it is importanttonotethatfordifferentproducts,wecannotusethesamesources\nas some products may be more popular in some sources and some products may be less popular in\nsome sources. Some of the keyfactorsweidentifywitharebecausesomesocialsources,forexample\nyoutube/facebook/instagram, have very different users. For example facebook users are normally\nolder people, instagram users are normally more of a teenage person. Therefore the topics that the\nusers discuss in both these apps will be very different. So it is very important to do research on the\nproductofinteresttofindoutwhicharethesourcesyoucanachievethebestdataandresultoutofit.\n15\n",
    "cleaned_content": "[PAGE BREAK]\nOpportunitiesfor Design Improvementsfor Nothing Phone1 using Artificial Intelligence\nAUTHORS STUDENTID\nHe Tianli 1006151\nLim Yu Jie 1005999\nAditya Kumar 1006300\nEthan Choo E-Rhen 1006324\nCaitlin Daphne Tan Chiang 1006537\nChin Wei Ming 1006264\nAllteammemberscontributedequallytothetask.\nSingapore Universityof Technologyand Design\n60.002:AIApplicationsin Design\nDr.Kwan Wei Lek,Dr Edwin Koh,Mr Michael Alexander Reeves\nApril16,2023\n[PAGE BREAK]\n1.Introduction\nIn recent times, technology has been advancing at a rapid rate and in recent times,there has been a\nsudden boom in interesttowards Artificial Intelligenceand Machine Learning.Thisisnotunexpected\ngiven how society is progressing towards digitization where more and more people are becoming\nglued to their smartphones,beittobrowsethroughsocialmedia,doingonlineshoppingetc.Withthis\nimmense surge of people going online, there is also a huge amount of data being uploaded every\nsingle second, and these data are the honeypots of feedback and consumer taste and preferences\nwheremanycompanieswouldbeinterestedinobtainingtodeveloptheirproduct.\nHowever, the issue lies with the immense amount of data online, how are we able todiscernwhatis\nrelevant and irrelevant to what the companies are focusing on for design improvements. In this\nproject we will be exploring and categorising largequantitiesofdataextractedfromonlineplatforms\nthroughtheuseof Artificial Intelligencetofindpossibledesignimprovementsofaproduct.\n1.1 Reasonforchoosingthe Nothing Phone(1)\nWe have chosen to implement AI machine learning on the Nothing Phone (1), a phone that was\nannounced in January in 2021 and released in July 2022. It was a highly anticipated phone that\ncirculated a lot of discussions among the technology geek community, and the general view for this\nphone is that it is very controversial in thesensewhereitisnothinglikewehaveseenbefore.People\nwere excited for the final reveal and release of the phone back in July 2022, but just to be\ndisappointedonhowitactuallycameout.\nThe company that came out with this product is called ‘Nothing’. This company was foundedon29\nOctober 2020, so it's still a relatively newcompanywhencomparedtootherphonecompanies.Asof\nMarch 2023, the company has only released 4 products, 3 ofwhicharewirelessearbuds,andthelast\nproduct being a smartphone. Being a product which does not have a subsequent model, there are\nbound to be issues that the smartphone faces when used by consumers in the mass. Thus, our group\ndecidedtorideontheopportunityandfocuson Nothing Phone(1).\nOne application we can implement AI and machine learning is what we will be exploring in this\nproject where we would be utilising machine learning models tofindroomsforimprovementsofthe\nproduct Nothing Phone(1).\nWe have chosen to find design opportunities and improvements on “Nothing Phone 1” asitgotalot\noftractionduringitsprereleaseduetoitsuniquedesign.However,afteritgotreleasedtherewasalot\nof bad feedback. Hence we think that this product has potential areas for design opportunities and\nimprovements. Toidentifydesignopportunitiesandimprovements,weuseddata-driventextanalytics\nand classificationmodels.Ultimatelywewishtoseekspecificdesignrequirementswhichcanbeused\nasaguidelinewhendesigningthenextiterationofnothingphones.\n1.2 Methodology\nFirstly, we decided to get comments and reviews from varioussourcestogetthegeneralideaofhow\npeople view the Nothing Phone 1. We have narrowed downto3 maindatasources:Youtube,Reddit\nand Websites (web scraping). There are 3 main steps to go through in order to do text analysis and\nclassificationonaproduct.\nDataextraction,datacleaning,dataanalysis.\n[PAGE BREAK]\n1.3 Overviewand Workflowofproject\nSettingup:\nThis project requires the use of multiple libraries, classification models as well as text models that\nneedtobeinstalledatthebeginningoftheproject.\nLibrary/Models Description/Use\ngooglesearch ‘Search’:searchesthetermongoogle\n“build”:Thegoogleapiclient.discoverymoduleisusedtobuildaserviceobject\nthatcaninteractwitha Google API.Thebuild()functionisusedtocreatethis\nserviceobject.\nrequests Sendhttprequeststointeractwithweb APIstoscrapewebpages\nBeautifulsoup4 Converts HTMLcodeintoanobjectthatcanbeusedtofindelementsinapage\npickle Toserialize/deserializepythonobjectsandsavestateofprogramsforsending\ndatabetweenprocesses.\nos Thislibraryprovidesawaytointeractwiththeoperatingsystemin Python.It\nincludesfunctionsforworkingwithfilesanddirectories,launching\nsubprocesses,andmanagingenvironmentvariables.\npandas Datastructuresforefficientlystoringandmanipulatinglargedatasets,and\nfunctionsforcleaning,transformingandanalysingdata.\n[PAGE BREAK]\ngoogle_key Useof Google APIkeystoauthenticateandauthorizeaccesstogoogle\nservices,inthisproject,itwillbeusedwhenwebscrapinggooglesearch\nresults\nre Usedforpatternmatchingandstringmanipulation,toextractinformationfrom\ntext,validateuserinputsetc.\nNumpy Libraryforworkingwitharraysandnumericaloperations\npraw Reddit APItoobtaincommentsfromreddit\nlangdetect Libraryfordetectinglanguageofagiventextstring,usedforcontentfiltering\nandlanguage-specifictextanalysis\nnltk Natural Language Toolkitfortextanalysis\nopenai Toenableustoaccessto Chat GPTusingpython\nBefore gathering data to perform text analysis on, a variety of websites were considered for data\ncollection, we mainly chose websites that are popular and tend to have a lot of reviewers. However\nduring our selectionphase,werealisedthatcertainmainstreamwebsitesdonothaveanythingtooffer\nfor the Nothing Phone(1), such asthecaseof Amazon,Shopeeand Lazada,wheresearchingtheterm\ndoesnotgivebacktheproduct.\nHence we decided to do data extraction on Youtube, Reddit and Websites that do have reviews and\ncommentsontheproduct.\nYou Tube Data Extraction:\nBefore we are able to extract data from youtube, we have to set up the You Tube Data API.APIsare\nunique to every individual’s google account. Itisneededforustobeabletoaccessallyoutube’sdata\nforexamplevideos,comments,titlesandmanymoreofyoutube’sdata.\nThe first step of extracting youtube data is first to specify what is the search terms that you are\nsearching for, which in our case will be the ‘nothing phone (1)’.Nextistospecifyhowmanyvideos\ndo you want the code to run through under the variable “max_results”. Afterwhich the code will\nessentially iterate through each video’s comments, and even furthermore each comment’s replies.\nWhichallthesesentenceswillbeappendedonebyonetoaninitiallyemptylist.\nWe utilised the pandas Dataframe to store each of the comments with the list of comments formed\nwhen running the code. And saved it as a csv file in the end to avoid needing to run it again in the\nfutureasthiscodetakesawhiletofinishrunning.\nReddit Data Extraction:\nFirstly, create a reddit account.Then go to https://www.reddit.com/prefs/apps on your web browser\nandclickonapps>createapps\nChoosethe“script”optionandsetbothurlsas“https://localhost:8080”,thenclickcreateapp\n[PAGE BREAK]\nAftercreationwewillbegiventheclient_secretsandclient_IDwhichwewillbeusinglater.\nNext,pipinstall PRAWlibrary,thisisthelibrarycreatedspecificallytowebscrapereddit.\nInattemptstokeepdataextractiontobeasautomatedaspossible,r/allwhichisourhomepage\nThe function ‘search_term’ is the value we would need to change depending on the product you\nchoose.\nInputs: ‘reddit.subreddit(‘all’)‘, searches through the whole of reddit, if we want to search througha\nspecific subreddit we just replace ‘all’ with the subreddit you want to do the scrapping\non.’search(search_term, sort=’relevance’, limit=100)’, ‘sort’ allows you to choose which\nrecommendation of subreddits to look into, relevance is just to keep the results closelyrelatedtothe\nproduct we have search. Whereas ‘limit’ just sets the maximum number of subreddits we will be\nscrappingfrom.\nThefunctionforpostinposttakesinthetitleandtheselftextandcollatesitas1 comment\nThenthecomments.replace_more(limit=25)functionloadsthatpostandtheircomments\n.append({‘Comment’:comment.body}) adds comments post and self text under a dataframe column\ncalled Comments\ndf[‘comment’].str.replace(‘\\n’,‘‘)removesthespacesandlinebreaksfromthecomments\nWebsite Extraction\nWe also decided that the websites obtained from google will be useful for our project. Thereforewe\ncreated a code that is essentially able to get the comments and text from these websites to aid our\ndesignprocess.\nFirstly, we specified our search_terms, which is ‘nothing phone (1)’ for this project. Then using the\ngoogle_search library, wewillgetalistof URLofthetop10 websitesofour searchresultongoogle.\nAfterwhich, we defined a get_sentences(link) function which willbeabletogetallthesentencesina\nwebsite into a list when given a website link. And we iterate for each URL found using the\ngoogle_searchlibrary,werunitthroughtheget_sentences(link)functionandeventuallyendupwitha\nlistofallthesentencesofall10 websites.\nFinally using pandas dataframe, we store the list of sentences into a dataframe and exportitasacsv\nfile.\n[PAGE BREAK]\nCombinationofall3 datasources\nThe final step of our data extraction process is combining all comments retrieved from youtube,\nreddit and web scraping into a single dataframe and eventually saving it as a csv file as\n‘All_Combined Raw_Data.csv’.\nHow we did it is that we read all the csv files that were created when extracting data from youtube,\nreddit and web scraping as a pandas dataframe. Andconcatenatingall3 dataframesinto1 dataframe\nandexportingittocsv.\n2.Data Cleaning\nBefore running analysis and classification modelsonthedatasets,itisessentialtododatacleaningto\nremove anythingthatdoesnotprovidevaluetoourprojectgoalsoffindingareasofimprovementsfor\nthe Nothing Phone(1).Ourdatacleaningprocessinvolvesthesestepsbelow:\n1. Tokenisation\n2. Removalof Stopwords\n3. Lemmatization\n4. Splitneutralterms\n5. Languagesplit\n6. Remove1 wordcomments\nThe code for the data cleaning is all in the “Clean_data.py” code folder. Our inputs for this process\nwill be the extracted comments CSV file from our data collection process. In the code, all these\nfunctions would run on the same file but we will be going in depth to what each of these cleaning\nfunctionsdoesandthejustificationindoingso.\n2.1 Tokenisation,Removalstopwords,Lemmatization\nTokenization is the first step to do when doing data cleaning, it splits the comments into individual\nwords or terms called tokens. It is an important first step in data preprocessing as it is required for\nlemmatization. Lemmatization is used to remove inflection by determining the part of speech and\nutilising a detailed database of the language, reducing a given word to its root meaning to identify\nsimilarities e.g. from ‘better’ to ‘good’, it derives the meaning of a word from a dictionary. The\nfunction ‘Lemmastise_sentence’ tokenise the sentences and lemmatises it and then joins the words\nbacktogether.\nStopwords are frequent words used in the english language that do not have specific semanticssuch\nas “the”,”is” etc, removing them prevents common words from showing up forcommonwordsused\nduring the data analysis step such as when we wish to detect the most used word. The function\n“remove_stop_words”identifiesnon-semanticwordsandremovesthemfromthesentences.Doingall\nofthiswillincreasesearchperformanceastherearelesserwordstoprocess.\nIt is important to note that we also remove the words “nothing”, “phone”, “1”, and “one” fromeach\nsentence in the data too. Reason being wepredictthatthesewordswillaffectourresultwhenfinding\nout the words that appear the most in thelaterstage.Asoursearchtermis“nothingphone(1)”,most\ncomments describing this product will have a high probability of having the name of the product in\ntheir sentences too. So in order to avoid the top words being the name of theproduct,wedecidedto\nremovethenameoftheproductinourdata.\n[PAGE BREAK]\n2.3 Removalofemojis,links,duplicatedcommentsandlowercasingwords\nLinks, emojis and duplicated/repeated comments are then removed from our dataset through the\n“clean_comment()” function. These are removed as it will not be relevanttous;althoughemojiscan\ngive us an emotion towards something, it will not beabletoprovideusexactdetailsinwhattheyare\nreacting to, i.e. what specifications, hence we remove it as it will bedifficulttoobtainspecifications\nfromthat.\nAs we are focusing on text emojis, we cannot really discern much from emojis so we will be\nremoving it to reduce our data. Additionally, links incommentswouldnotberelevanttoourprojects\nasmostofthemwouldbeforadvertisement.whichisnotusefulforourproject.\nWe also used the pandas dataframe’s drop_duplicate() function to remove all the entries that were\nrepeated. And later on resetting the index of the pandas dataframe to make it easiertovisualisehow\nmanyentriesareleftinthedataframe.\nWe also converted all characters inthedatatobelowercase.Thisissothattherewontbeanyerrorof\nupper and lower case when we try to search for any specific words within a comment in the later\nstage. Therefore we standardise for every single character of every comment in the data to be\nlowercase.\n2.4 Splitneutralterms\nWe also have split long balanced comments with neutral words like 'but' and ‘however’' to rule out\nthe comments that are neutral, essentially splitting up one long comment into 2 comments; positive\nandnegative.\nWe proceeded to ask Chatgpt what are the possible words or terms that can make a sentence sound\nneutral. We then check if any comments consistofanyoftheseneutralwords,andifthereis,wewill\nsplitthesentencewiththatword.Belowisapictureofchatgpt’sresponseoftheneutralterms.\n2.5 Removalofnon-englishcomments\nAfter this, in the dataframe of comments, we added another column named “language”. In this\ncolumn, essentially we used the function detect() from the langdetect library in python, so that it is\n[PAGE BREAK]\nable to go through each sentence in the dataframe to determine whatlanguageitisin,andlabeleach\nsentence’slanguageinthecolumn“language”.\nAfter we have a column of language labels for each sentence, we proceed to onlytakethesentences\nwhichareonlyclassifiedas Englishtomovefurtherintoourproject.\n2.6 Remove1 wordcomments\nThis step is the last step of our data cleaning. How wedecidedtoremovethe1 wordedentriesinthe\ndata is because when we looked at the data, we realisedthattheseonewordingwillnotgiveusgood\ninformationregardinghowtoimprovetheproduct.\nTake for example in the data there is the entry“battery”.Withoutanymorewords,wewilldefinitely\nbe able to deduce whether this is a positive or negative comment. Which will be analysed in the\nfurther part of this project. So considering the time taken to run analysis and thesizeofthedata,we\ndecided that itisawasteoftimeandcomputationalefforttoletthemodeltrytodeduceaone-worded\nentry whether it is a positive or negative comment. We also triedthinkingofone-wordedentriesthat\nmay be beneficial data to our design requirements but we could not thinkofanexample.Whichled\nustoultimatelydecidetoremove1 wordcommentsfromthedata.\nHow wediditiswecheckifthereisanywhitespaceineachcomment.Andifthereisnowhitespace\ninthatcomment,indicatingthatitisaone-wordedentryinthedata,andsoweremovethisentry.\n3.Data Processing&Analysis\nAt this stage, we will have a huge data of all the cleaned and ready to be processed and analyse\ncommentsfromvarioussources.\nOur group is left with a total of 34425 comments from all 3 data sources. However, it is from here\nonwards we have decidedtoremovethedatafromscrapingwebsitesfromourdatasetasitcontainsa\nlot of irrelevant text due to it including words from advertisements, headers,directoryetcanditalso\nrequires huge amounts of time and computer resources to run the whole data set through Sentient\nAnalysis.\nRemoving it leaves us with 29155 comments. But the issue is thatbecausethemodelthatwewillbe\nputting these comments into is adeeplearningmodel.Meaningthatitwilltakeaconsiderablylonger\ntime to analyse one comment. Given that we have 29155 comments, we do not have the time and\nresourcestorunitforallofthesecommentsasweestimatedthetimetakenforall29155 commentsto\nbeanalysedbythemodelisaround22 hours.\nTherefore, we decided to randomise our comments from Youtube and Reddit and randomly picked\nout 15% of the data. Leaving us with a sampleof3340 commentsfromthehuge29155 commentsof\nthe original dataset. Webelievethatthisisasmartmoveasitwillhugelyreducethehoursrequiredto\ndo the analysis and classification of our data and we believe that it will not affect the final result of\nour project. But we noted that inanidealsituation,weshouldrunallofthedataintothemodeltoget\nthe maximal possible accurate final output of the project given that we have enoughtimeandstrong\nenoughcomputerstorunthecomputations.\nFor this part of the project, we did research and decided to try 4 models on our dataset.Namely,the\nSiebert Sentiment Analysis and Zero shot classification which we retrieved from a website called\nhuggingface,Vader Analysis,aswellastopicmodelling.\n[PAGE BREAK]\n3.1 Siebert Sentiment Analysis\nTo run the Siebert Sentiment Analysis onourdatasetusingthehuggingfacewebsite,ourpythoncode\nuses a pipeline to connect the huggingface model to our code. There also is a function called\nanalyze_sentiment(comment) so that we can iterate through all comments and pass it on to this\nfunction for analysis. The function will then analyse whether a comment is positive or negative and\nlabelitinanewdataframecolumnaseither“POSITIVE”or“NEGATIVE”.\n3.2 VADERmodel\nAt the same time, we have decided to use the VADER model which is an alternative to Sentiment\nAnalysis that is more suitable in discerning social media comments . It is during this process we\nrealised that the VADER modeltooklessertimeinclassifyingthesentimentvalueshencewedecided\nto run our full Dataset through it (29155). The key difference in terms of output is that this model\nclassifiedtheneutralcommentsaswell.\n3.3 Extractingnegativesentimentcommentsfromboth VADERand Siebert Sentiment Analysis\nComparing the 2 modelswhichcandothesamethingcanbedoneverydifferently.Wedecidedtouse\nSiebert's Sentiment Analysisfromhuggingfaceover Vader.Herearethereasonswhy.\n1. As seen in the figure below, Siebert’s Sentiment Analysis is able to identify more negative\ncomments in the dataset and Vader actually classified the least negative comments on the\nother hand. Looking at this bar chart, we deduced that Siebert is able to analyse a comment\nbetterthan Vader,whichiswhy Siebert’smodelwilltakealongertimetorun.\n2. Siebert Sentiment Analysis is a deep learning model, which means it is able to analyse\ncommentsmoreaccurately.\n3. Too manycommentsthatwentthroughthe Vadermodelareclassifiedas Neutral.Becausewe\nbelieve that we already successfully ruled out all the neutral comments in the cleaning data\npart of the process. Therefore the fact that so many comments are still classified as neutral\ntellsusthatthe Vadermodelmaynotbesuitabletouseinourcase.\nTherefore moving on further into our project, we utilised the data that ran through the Siebert\nSentiment Analysis model. The output after running this model is a column in our dataframe,\nindicating whether that comment is a positive comment “POSITIVE” or a negative comment\n“NEGATIVE”. We then took out all thecommentsthatwerelabelledas“NEGATIVE”becausethese\nwill be the comments that will be more valuable to us when deducing the finaldesignrequirements.\nWhereasthepositivecommentswillnotbeabletotellushowwemightimprovethephone.\nFigure:VADERvs Sentiment Analysispolarityresults\n[PAGE BREAK]\n3.4 Comparisonsofresultsbetween SAand VADERforextractednegativecomments\nThe VADER model returns us with only 494 negative comments out of the full 29155 comments\nwithin 5 seconds and sentiment analysis returns us with 1773 negative comments out of3340 which\ntook68 minutes.\nFigure:Comparing VADERand SAmodels\n3.5 Topic Modelling\nThis model is actually suggested by Chatgpt. Upon finalising our work plan for this project, we\nwondered if there is any other way we could go about this project in a better way. So we turned to\nChat GPT.Belowisascreenshotofourconversationwith Chat GPT.\n[PAGE BREAK]\nEssentially what this model does is it goes through all of our comments, then it will try to find out\nwhatarethekeywordsineachofthesesentencesandgivesalistofkeywordsforeachcomment.\n3.6 Zeroshot Classification\nFormingcategoriestoclassifycommentsintobeforeputtinginto Zeroshot:\nWe managed to write a python code that is able to ask Chat GPT to provideuswithcategoriestoput\ninto whendoingour Zeroshot Classification.Reasonwhywedidthisissothatevenwhentheproduct\nofinterestchanges,itwillstillbeabletogiveuscategoriesregardlessofwhateverproduct.\nTo perform Zeroshot Classification on our model, wefirsthavetohavealistofcategoriessothatthe\nmodel knows what are the options it can classify each sentence into and later on proceed to classify\nthem into the category which the model thinks the sentence is more related to. So for the output of\nthis model after we ran it with our data, it is an additionalcolumninthedataframe,labellingwhatis\nthecommenttalkingaboutgiventhatthecategoriesweregivenby Chat GPT.\n[PAGE BREAK]\n3.7 Zeroshot Classificationor Topic Modellingandtheinterpretationoftheresult\nWe eventually decided that Zeroshot Classificationwillbebetterthan Topic Modellinginourproject.\nAndhereisthereasonwhy.\nThekeywordsof Topic Modellingaretoowide.Becausefor Topic Modellingthemodelisidentifying\nthe keywords for each sentence by itself. Therefore for every sentence, there will be a very diverse\nrange of keywords, which we will not be able to really deduce what this sentence is talking about.\nWhereasfor Zeroshotclassification,thecategoriesaregivenbyus.Soitwillclassifyeachsentenceto\nwhat we need to know about. Therefore eachsentencewillbefinetunedtoourcategorieswhichwill\nbeusedtoidentifydesignopportunities.\nFrom the result of the Zeroshot classification, we didabarcharttofindoutwhichisthecategorythat\nhas the most comments. In other words, which are thecategoriesofthephonethatpeoplehavemore\nnegative things to talkabout.Fromthefigurebelow,wefoundoutthatthetop4 categoriesofnothing\nphone (1) that most people have bad things tosayaboutis Features,Design,Price,and Performance.\nWhich will be the 4 categories we choose to focus more on when deducing our final design\nrequirements.\n[PAGE BREAK]\nFigure:Distributionofnegativecommentsfrom Sentimentanalysis\nAs can beseenbytheabovefigure,mostofthenegativecommentsaredissatisfiedaboutthe Features\nof Nothing Phone (1), due to the overwhelming negative comments on this specifications as\ncompared to other specifications, we decided to focus on designing improvements for the features,\ndesign, price and performance of the Nothing Phone(1) which are the top 4 classes thatreceivedthe\nmostnegativecomments.\n3.7 Word Cloud Visualization\nFigure:Wordcloudon Sentiment Analysis\nWe ran all the negative comments we obtained into the nltk Wordcloud text analysis to get a\nvisualisation of the most common words that appearedforeverycategory,wherethelargertheword,\nthebiggerthelettersize.\nEssentially, for each class we counted how many times each word appears, when we take thetop25\nhighestfrequencywordsanddisplayitina Wordcloud.\n3.5 Extractedcommentsforfinalevaluation\nAfter we successfully obtained the words that appeared the most for each class of comets asseenas\nthe Wordcloud figure above. We thought that these words will not give us the full understanding of\nwhatthesereviewsaretrulytalkingabout.\nSo we track back a little to get the comments data that has the top3 wordsthatappearedthemostin\nthe comments. How itworksisthatwefoundoutwhatarethe3 highestappearancefrequencywords,\n[PAGE BREAK]\nthenwewrotethecodeinsuchawaythatforallcommentsthathaveanyofthe3 topwords,weputit\ninanewdataframeandexportitasacsvfile.\nLastly as final evaluation, we analyse each comment in the csv file and finally deduce our design\nrequirements.\nDesign Opportunitiesandtheir Justifications\n1. Finding Nothing Phone’sownidentityinthesmartphonemarket\nThere are many comments on how the Nothing Phone (1) is highly similar in terms of looks to the\ni Phone, with some people even calling it a cheap version of the i Phone. As seen in the Wordcloud\nimage for ‘Features’, “iphone” is one of the top 3 words that were mentioned in the comments. The\nfact that most comments referred to the i Phonejustgoestoshowthatthe Nothing Phone(1)doesnot\nhave a trademark of its own that peoplecaninstantlylinktothe Nothingcompanyimmediatelyupon\nseeing one of its products. For example, the i Phones are recognised for their flat edged bezel, while\nthe Samsung Galaxy series are known for their curved edged screens. Ifthe Nothing Phone(1)hasa\nfeature unique to the brand, peoplewillstopcomparingthe Nothing Phonetootherphonebrandsjust\nbasedonhowitlooksatfirstglance.\n2. Differentsubmodelswithdifferentspecifications\nThere were people who commented on how they want higher processing power while others\nmentioned that having a smaller battery and charging asmallerpricewouldmakethe Nothing Phone\n(1)’s price range be a good mid rangepriceforaphone.Sobyhavingdifferentsubmodels,therecan\nbe different priceschargedfordifferentcombinationsofspecificationsthatwilltailortodifferentuser\nneeds.\n3. Newbackingforthephone\nConcernswereraisedaboutthewaterproofingforthephone,andtherewerealsocomplaintsaboutthe\nbacklights either being excessive or useless. One commenter mentioned that instead of having a\nbacklight, there could be a stronger back casing that does not shatter. Hence, the Nothing Phone (1)\ncan have a completely new backing where it takes all these comments into account. In other words,\nbalancingthefunctionalityandpracticalityofthedesigntogetherwiththeaestheticsofit.\nReflections\n1. Time taken:it take very long time to run SA to dataset, requires desktop/ stronger computer\nthatcanrunforhours\n2. Irrelevancy: Despite multiple cleaning,therewouldstillbecommentsthatareleftbehindthat\nareirrelevanttoourprojectgoals\n3. Detection isirrelevant,here,asourproductiscalled“Nothing Phone1”itleavesuswithalot\nof ‘phone’ and ‘nothing’ in thewordclouds,wherenothingcanbeusedtorefertothephone,\nitcanalsorefertocriticismoftheproduct.\nSummaryoflessonlearnt\nTo conclude our report, we will be sharing a summary of our lessons learnt when doingthisproject,\nandwouldwehavedonedifferentlyifweevercomeacrossasimilarprojectagain.\n[PAGE BREAK]\n1. Deeperresearchoneachmodel’salgorithm.\nIn our project, we rely heavily on existing models and what we did essentially isonlytoapplythem\nwithout cleaned data. While we did some experiments and hadourowninterpretationofwhycertain\nmodels are better and certain models are worse. A lesson learnt is to do more thorough research on\nthe models that may be useful to our project, and study its algorithm answer the question of, will it\ntake longer to run?, how is it classifying certain things?, how does it deal with bad and irrelevant\ndata?, is it really suitable for comments/reviews classification or is it actually more for essays or\narticles?\n2. Datacleaningisnotalinearprocess,butmoreofatrialanderrorprocess.\nWhen we were doing our data cleaning, it was an iterative process. Reason being it was a constant\nprocess of us completing one part of the cleaning, then checking manually on the data itself to\nidentify what could be done better to the data. Take for example when we realised that there were a\nlot of random 39 s in our dataset which needed toberemoved,whenwerealisedonewordcomments\nwere invaluable for our study, and when we found out that some comments were repeated or non\nenglish comments. It was an iterative process of checking the data,findingoutwhatwaswrongwith\nit, and coming with the code to clean it more thoroughly. So do not expectthecleaningdataprocess\ntobeaoneshotfinishedprocess.\n3. Differentsourcesfordifferentproducts\nFor different products that you are interested to do more research on,therewillbedifferentwebsites\nthat have more data on certain products. For our project, we manually checked which were the best\nsources to get data about the nothing phone (1) from and landed with reddit, youtube, and top 10\ngoogle websites. So it is importanttonotethatfordifferentproducts,wecannotusethesamesources\nas some products may be more popular in some sources and some products may be less popular in\nsome sources. Some of the keyfactorsweidentifywitharebecausesomesocialsources,forexample\nyoutube/facebook/instagram, have very different users. For example facebook users are normally\nolder people, instagram users are normally more of a teenage person. Therefore the topics that the\nusers discuss in both these apps will be very different. So it is very important to do research on the\nproductofinteresttofindoutwhicharethesourcesyoucanachievethebestdataandresultoutofit.",
    "sections": [
      {
        "title": "Chin Wei Ming 1006264",
        "content": "Allteammemberscontributedequallytothetask.\n"
      },
      {
        "title": "Singapore Universityof Technologyand Design",
        "content": "60.002:AIApplicationsin Design\nDr.Kwan Wei Lek,Dr Edwin Koh,Mr Michael Alexander Reeves\nApril16,2023\n"
      },
      {
        "title": "Content",
        "content": "1.Introduction\nIn recent times, technology has been advancing at a rapid rate and in recent times,there has been a\nsudden boom in interesttowards Artificial Intelligenceand Machine Learning.Thisisnotunexpected\ngiven how society is progressing towards digitization where more and more people are becoming\nglued to their smartphones,beittobrowsethroughsocialmedia,doingonlineshoppingetc.Withthis\nimmense surge of people going online, there is also a huge amount of data being uploaded every\nsingle second, and these data are the honeypots of feedback and consumer taste and preferences\nwheremanycompanieswouldbeinterestedinobtainingtodeveloptheirproduct.\nHowever, the issue lies with the immense amount of data online, how are we able todiscernwhatis\nrelevant and irrelevant to what the companies are focusing on for design improvements. In this\nproject we will be exploring and categorising largequantitiesofdataextractedfromonlineplatforms\nthroughtheuseof Artificial Intelligencetofindpossibledesignimprovementsofaproduct.\n1.1 Reasonforchoosingthe Nothing Phone(1)\nWe have chosen to implement AI machine learning on the Nothing Phone (1), a phone that was\nannounced in January in 2021 and released in July 2022. It was a highly anticipated phone that\ncirculated a lot of discussions among the technology geek community, and the general view for this\nphone is that it is very controversial in thesensewhereitisnothinglikewehaveseenbefore.People\nwere excited for the final reveal and release of the phone back in July 2022, but just to be\ndisappointedonhowitactuallycameout.\nThe company that came out with this product is called ‘Nothing’. This company was foundedon29\nOctober 2020, so it's still a relatively newcompanywhencomparedtootherphonecompanies.Asof\nMarch 2023, the company has only released 4 products, 3 ofwhicharewirelessearbuds,andthelast\nproduct being a smartphone. Being a product which does not have a subsequent model, there are\nbound to be issues that the smartphone faces when used by consumers in the mass. Thus, our group\ndecidedtorideontheopportunityandfocuson Nothing Phone(1).\nOne application we can implement AI and machine learning is what we will be exploring in this\nproject where we would be utilising machine learning models tofindroomsforimprovementsofthe\nproduct Nothing Phone(1).\nWe have chosen to find design opportunities and improvements on “Nothing Phone 1” asitgotalot\noftractionduringitsprereleaseduetoitsuniquedesign.However,afteritgotreleasedtherewasalot\nof bad feedback. Hence we think that this product has potential areas for design opportunities and\nimprovements. Toidentifydesignopportunitiesandimprovements,weuseddata-driventextanalytics\nand classificationmodels.Ultimatelywewishtoseekspecificdesignrequirementswhichcanbeused\nasaguidelinewhendesigningthenextiterationofnothingphones.\n1.2 Methodology\nFirstly, we decided to get comments and reviews from varioussourcestogetthegeneralideaofhow\npeople view the Nothing Phone 1. We have narrowed downto3 maindatasources:Youtube,Reddit\nand Websites (web scraping). There are 3 main steps to go through in order to do text analysis and\nclassificationonaproduct.\nDataextraction,datacleaning,dataanalysis.\n"
      },
      {
        "title": "Content",
        "content": "1.3 Overviewand Workflowofproject\nSettingup:\nThis project requires the use of multiple libraries, classification models as well as text models that\nneedtobeinstalledatthebeginningoftheproject.\nLibrary/Models Description/Use\ngooglesearch ‘Search’:searchesthetermongoogle\n“build”:Thegoogleapiclient.discoverymoduleisusedtobuildaserviceobject\nthatcaninteractwitha Google API.Thebuild()functionisusedtocreatethis\nserviceobject.\nrequests Sendhttprequeststointeractwithweb APIstoscrapewebpages\nBeautifulsoup4 Converts HTMLcodeintoanobjectthatcanbeusedtofindelementsinapage\npickle Toserialize/deserializepythonobjectsandsavestateofprogramsforsending\ndatabetweenprocesses.\nos Thislibraryprovidesawaytointeractwiththeoperatingsystemin Python.It\nincludesfunctionsforworkingwithfilesanddirectories,launching\nsubprocesses,andmanagingenvironmentvariables.\npandas Datastructuresforefficientlystoringandmanipulatinglargedatasets,and\nfunctionsforcleaning,transformingandanalysingdata.\n"
      },
      {
        "title": "Content",
        "content": "google_key Useof Google APIkeystoauthenticateandauthorizeaccesstogoogle\nservices,inthisproject,itwillbeusedwhenwebscrapinggooglesearch\nresults\nre Usedforpatternmatchingandstringmanipulation,toextractinformationfrom\ntext,validateuserinputsetc.\n"
      },
      {
        "title": "Numpy Libraryforworkingwitharraysandnumericaloperations",
        "content": "praw Reddit APItoobtaincommentsfromreddit\nlangdetect Libraryfordetectinglanguageofagiventextstring,usedforcontentfiltering\nandlanguage-specifictextanalysis\nnltk Natural Language Toolkitfortextanalysis\nopenai Toenableustoaccessto Chat GPTusingpython\nBefore gathering data to perform text analysis on, a variety of websites were considered for data\ncollection, we mainly chose websites that are popular and tend to have a lot of reviewers. However\nduring our selectionphase,werealisedthatcertainmainstreamwebsitesdonothaveanythingtooffer\nfor the Nothing Phone(1), such asthecaseof Amazon,Shopeeand Lazada,wheresearchingtheterm\ndoesnotgivebacktheproduct.\nHence we decided to do data extraction on Youtube, Reddit and Websites that do have reviews and\ncommentsontheproduct.\n"
      },
      {
        "title": "You Tube Data Extraction:",
        "content": "Before we are able to extract data from youtube, we have to set up the You Tube Data API.APIsare\nunique to every individual’s google account. Itisneededforustobeabletoaccessallyoutube’sdata\nforexamplevideos,comments,titlesandmanymoreofyoutube’sdata.\nThe first step of extracting youtube data is first to specify what is the search terms that you are\nsearching for, which in our case will be the ‘nothing phone (1)’.Nextistospecifyhowmanyvideos\ndo you want the code to run through under the variable “max_results”. Afterwhich the code will\nessentially iterate through each video’s comments, and even furthermore each comment’s replies.\nWhichallthesesentenceswillbeappendedonebyonetoaninitiallyemptylist.\nWe utilised the pandas Dataframe to store each of the comments with the list of comments formed\nwhen running the code. And saved it as a csv file in the end to avoid needing to run it again in the\nfutureasthiscodetakesawhiletofinishrunning.\n"
      },
      {
        "title": "Reddit Data Extraction:",
        "content": "Firstly, create a reddit account.Then go to https://www.reddit.com/prefs/apps on your web browser\nandclickonapps>createapps\nChoosethe“script”optionandsetbothurlsas“https://localhost:8080”,thenclickcreateapp\n"
      },
      {
        "title": "Content",
        "content": "Aftercreationwewillbegiventheclient_secretsandclient_IDwhichwewillbeusinglater.\nNext,pipinstall PRAWlibrary,thisisthelibrarycreatedspecificallytowebscrapereddit.\nInattemptstokeepdataextractiontobeasautomatedaspossible,r/allwhichisourhomepage\nThe function ‘search_term’ is the value we would need to change depending on the product you\nchoose.\nInputs: ‘reddit.subreddit(‘all’)‘, searches through the whole of reddit, if we want to search througha\nspecific subreddit we just replace ‘all’ with the subreddit you want to do the scrapping\non.’search(search_term, sort=’relevance’, limit=100)’, ‘sort’ allows you to choose which\nrecommendation of subreddits to look into, relevance is just to keep the results closelyrelatedtothe\nproduct we have search. Whereas ‘limit’ just sets the maximum number of subreddits we will be\nscrappingfrom.\nThefunctionforpostinposttakesinthetitleandtheselftextandcollatesitas1 comment\nThenthecomments.replace_more(limit=25)functionloadsthatpostandtheircomments\n.append({‘Comment’:comment.body}) adds comments post and self text under a dataframe column\ncalled Comments\ndf[‘comment’].str.replace(‘\\n’,‘‘)removesthespacesandlinebreaksfromthecomments\n"
      },
      {
        "title": "Website Extraction",
        "content": "We also decided that the websites obtained from google will be useful for our project. Thereforewe\ncreated a code that is essentially able to get the comments and text from these websites to aid our\ndesignprocess.\nFirstly, we specified our search_terms, which is ‘nothing phone (1)’ for this project. Then using the\ngoogle_search library, wewillgetalistof URLofthetop10 websitesofour searchresultongoogle.\nAfterwhich, we defined a get_sentences(link) function which willbeabletogetallthesentencesina\nwebsite into a list when given a website link. And we iterate for each URL found using the\ngoogle_searchlibrary,werunitthroughtheget_sentences(link)functionandeventuallyendupwitha\nlistofallthesentencesofall10 websites.\nFinally using pandas dataframe, we store the list of sentences into a dataframe and exportitasacsv\nfile.\n"
      },
      {
        "title": "Content",
        "content": "Combinationofall3 datasources\nThe final step of our data extraction process is combining all comments retrieved from youtube,\nreddit and web scraping into a single dataframe and eventually saving it as a csv file as\n‘All_Combined Raw_Data.csv’.\nHow we did it is that we read all the csv files that were created when extracting data from youtube,\nreddit and web scraping as a pandas dataframe. Andconcatenatingall3 dataframesinto1 dataframe\nandexportingittocsv.\n2.Data Cleaning\nBefore running analysis and classification modelsonthedatasets,itisessentialtododatacleaningto\nremove anythingthatdoesnotprovidevaluetoourprojectgoalsoffindingareasofimprovementsfor\nthe Nothing Phone(1).Ourdatacleaningprocessinvolvesthesestepsbelow:\n"
      },
      {
        "title": "6. Remove1 wordcomments",
        "content": "The code for the data cleaning is all in the “Clean_data.py” code folder. Our inputs for this process\nwill be the extracted comments CSV file from our data collection process. In the code, all these\nfunctions would run on the same file but we will be going in depth to what each of these cleaning\nfunctionsdoesandthejustificationindoingso.\n2.1 Tokenisation,Removalstopwords,Lemmatization\nTokenization is the first step to do when doing data cleaning, it splits the comments into individual\nwords or terms called tokens. It is an important first step in data preprocessing as it is required for\nlemmatization. Lemmatization is used to remove inflection by determining the part of speech and\nutilising a detailed database of the language, reducing a given word to its root meaning to identify\nsimilarities e.g. from ‘better’ to ‘good’, it derives the meaning of a word from a dictionary. The\nfunction ‘Lemmastise_sentence’ tokenise the sentences and lemmatises it and then joins the words\nbacktogether.\nStopwords are frequent words used in the english language that do not have specific semanticssuch\nas “the”,”is” etc, removing them prevents common words from showing up forcommonwordsused\nduring the data analysis step such as when we wish to detect the most used word. The function\n“remove_stop_words”identifiesnon-semanticwordsandremovesthemfromthesentences.Doingall\nofthiswillincreasesearchperformanceastherearelesserwordstoprocess.\nIt is important to note that we also remove the words “nothing”, “phone”, “1”, and “one” fromeach\nsentence in the data too. Reason being wepredictthatthesewordswillaffectourresultwhenfinding\nout the words that appear the most in thelaterstage.Asoursearchtermis“nothingphone(1)”,most\ncomments describing this product will have a high probability of having the name of the product in\ntheir sentences too. So in order to avoid the top words being the name of theproduct,wedecidedto\nremovethenameoftheproductinourdata.\n"
      },
      {
        "title": "Content",
        "content": "2.3 Removalofemojis,links,duplicatedcommentsandlowercasingwords\nLinks, emojis and duplicated/repeated comments are then removed from our dataset through the\n“clean_comment()” function. These are removed as it will not be relevanttous;althoughemojiscan\ngive us an emotion towards something, it will not beabletoprovideusexactdetailsinwhattheyare\nreacting to, i.e. what specifications, hence we remove it as it will bedifficulttoobtainspecifications\nfromthat.\nAs we are focusing on text emojis, we cannot really discern much from emojis so we will be\nremoving it to reduce our data. Additionally, links incommentswouldnotberelevanttoourprojects\nasmostofthemwouldbeforadvertisement.whichisnotusefulforourproject.\nWe also used the pandas dataframe’s drop_duplicate() function to remove all the entries that were\nrepeated. And later on resetting the index of the pandas dataframe to make it easiertovisualisehow\nmanyentriesareleftinthedataframe.\nWe also converted all characters inthedatatobelowercase.Thisissothattherewontbeanyerrorof\nupper and lower case when we try to search for any specific words within a comment in the later\nstage. Therefore we standardise for every single character of every comment in the data to be\nlowercase.\n2.4 Splitneutralterms\nWe also have split long balanced comments with neutral words like 'but' and ‘however’' to rule out\nthe comments that are neutral, essentially splitting up one long comment into 2 comments; positive\nandnegative.\nWe proceeded to ask Chatgpt what are the possible words or terms that can make a sentence sound\nneutral. We then check if any comments consistofanyoftheseneutralwords,andifthereis,wewill\nsplitthesentencewiththatword.Belowisapictureofchatgpt’sresponseoftheneutralterms.\n2.5 Removalofnon-englishcomments\nAfter this, in the dataframe of comments, we added another column named “language”. In this\ncolumn, essentially we used the function detect() from the langdetect library in python, so that it is\n"
      },
      {
        "title": "Content",
        "content": "able to go through each sentence in the dataframe to determine whatlanguageitisin,andlabeleach\nsentence’slanguageinthecolumn“language”.\nAfter we have a column of language labels for each sentence, we proceed to onlytakethesentences\nwhichareonlyclassifiedas Englishtomovefurtherintoourproject.\n2.6 Remove1 wordcomments\nThis step is the last step of our data cleaning. How wedecidedtoremovethe1 wordedentriesinthe\ndata is because when we looked at the data, we realisedthattheseonewordingwillnotgiveusgood\ninformationregardinghowtoimprovetheproduct.\nTake for example in the data there is the entry“battery”.Withoutanymorewords,wewilldefinitely\nbe able to deduce whether this is a positive or negative comment. Which will be analysed in the\nfurther part of this project. So considering the time taken to run analysis and thesizeofthedata,we\ndecided that itisawasteoftimeandcomputationalefforttoletthemodeltrytodeduceaone-worded\nentry whether it is a positive or negative comment. We also triedthinkingofone-wordedentriesthat\nmay be beneficial data to our design requirements but we could not thinkofanexample.Whichled\nustoultimatelydecidetoremove1 wordcommentsfromthedata.\nHow wediditiswecheckifthereisanywhitespaceineachcomment.Andifthereisnowhitespace\ninthatcomment,indicatingthatitisaone-wordedentryinthedata,andsoweremovethisentry.\n3.Data Processing&Analysis\nAt this stage, we will have a huge data of all the cleaned and ready to be processed and analyse\ncommentsfromvarioussources.\nOur group is left with a total of 34425 comments from all 3 data sources. However, it is from here\nonwards we have decidedtoremovethedatafromscrapingwebsitesfromourdatasetasitcontainsa\nlot of irrelevant text due to it including words from advertisements, headers,directoryetcanditalso\nrequires huge amounts of time and computer resources to run the whole data set through Sentient\nAnalysis.\nRemoving it leaves us with 29155 comments. But the issue is thatbecausethemodelthatwewillbe\nputting these comments into is adeeplearningmodel.Meaningthatitwilltakeaconsiderablylonger\ntime to analyse one comment. Given that we have 29155 comments, we do not have the time and\nresourcestorunitforallofthesecommentsasweestimatedthetimetakenforall29155 commentsto\nbeanalysedbythemodelisaround22 hours.\nTherefore, we decided to randomise our comments from Youtube and Reddit and randomly picked\nout 15% of the data. Leaving us with a sampleof3340 commentsfromthehuge29155 commentsof\nthe original dataset. Webelievethatthisisasmartmoveasitwillhugelyreducethehoursrequiredto\ndo the analysis and classification of our data and we believe that it will not affect the final result of\nour project. But we noted that inanidealsituation,weshouldrunallofthedataintothemodeltoget\nthe maximal possible accurate final output of the project given that we have enoughtimeandstrong\nenoughcomputerstorunthecomputations.\nFor this part of the project, we did research and decided to try 4 models on our dataset.Namely,the\n"
      },
      {
        "title": "Siebert Sentiment Analysis and Zero shot classification which we retrieved from a website called",
        "content": "huggingface,Vader Analysis,aswellastopicmodelling.\n"
      },
      {
        "title": "Content",
        "content": "3.1 Siebert Sentiment Analysis\nTo run the Siebert Sentiment Analysis onourdatasetusingthehuggingfacewebsite,ourpythoncode\nuses a pipeline to connect the huggingface model to our code. There also is a function called\nanalyze_sentiment(comment) so that we can iterate through all comments and pass it on to this\nfunction for analysis. The function will then analyse whether a comment is positive or negative and\nlabelitinanewdataframecolumnaseither“POSITIVE”or“NEGATIVE”.\n3.2 VADERmodel\nAt the same time, we have decided to use the VADER model which is an alternative to Sentiment\nAnalysis that is more suitable in discerning social media comments . It is during this process we\nrealised that the VADER modeltooklessertimeinclassifyingthesentimentvalueshencewedecided\nto run our full Dataset through it (29155). The key difference in terms of output is that this model\nclassifiedtheneutralcommentsaswell.\n3.3 Extractingnegativesentimentcommentsfromboth VADERand Siebert Sentiment Analysis\nComparing the 2 modelswhichcandothesamethingcanbedoneverydifferently.Wedecidedtouse\nSiebert's Sentiment Analysisfromhuggingfaceover Vader.Herearethereasonswhy.\n"
      },
      {
        "title": "1. As seen in the figure below, Siebert’s Sentiment Analysis is able to identify more negative",
        "content": "comments in the dataset and Vader actually classified the least negative comments on the\nother hand. Looking at this bar chart, we deduced that Siebert is able to analyse a comment\nbetterthan Vader,whichiswhy Siebert’smodelwilltakealongertimetorun.\n"
      },
      {
        "title": "2. Siebert Sentiment Analysis is a deep learning model, which means it is able to analyse",
        "content": "commentsmoreaccurately.\n"
      },
      {
        "title": "3. Too manycommentsthatwentthroughthe Vadermodelareclassifiedas Neutral.Becausewe",
        "content": "believe that we already successfully ruled out all the neutral comments in the cleaning data\npart of the process. Therefore the fact that so many comments are still classified as neutral\ntellsusthatthe Vadermodelmaynotbesuitabletouseinourcase.\nTherefore moving on further into our project, we utilised the data that ran through the Siebert\n"
      },
      {
        "title": "Sentiment Analysis model. The output after running this model is a column in our dataframe,",
        "content": "indicating whether that comment is a positive comment “POSITIVE” or a negative comment\n“NEGATIVE”. We then took out all thecommentsthatwerelabelledas“NEGATIVE”becausethese\nwill be the comments that will be more valuable to us when deducing the finaldesignrequirements.\nWhereasthepositivecommentswillnotbeabletotellushowwemightimprovethephone.\n"
      },
      {
        "title": "Content",
        "content": "3.4 Comparisonsofresultsbetween SAand VADERforextractednegativecomments\nThe VADER model returns us with only 494 negative comments out of the full 29155 comments\nwithin 5 seconds and sentiment analysis returns us with 1773 negative comments out of3340 which\ntook68 minutes.\nFigure:Comparing VADERand SAmodels\n3.5 Topic Modelling\nThis model is actually suggested by Chatgpt. Upon finalising our work plan for this project, we\nwondered if there is any other way we could go about this project in a better way. So we turned to\nChat GPT.Belowisascreenshotofourconversationwith Chat GPT.\n"
      },
      {
        "title": "Content",
        "content": "Essentially what this model does is it goes through all of our comments, then it will try to find out\nwhatarethekeywordsineachofthesesentencesandgivesalistofkeywordsforeachcomment.\n3.6 Zeroshot Classification\n"
      },
      {
        "title": "Formingcategoriestoclassifycommentsintobeforeputtinginto Zeroshot:",
        "content": "We managed to write a python code that is able to ask Chat GPT to provideuswithcategoriestoput\ninto whendoingour Zeroshot Classification.Reasonwhywedidthisissothatevenwhentheproduct\nofinterestchanges,itwillstillbeabletogiveuscategoriesregardlessofwhateverproduct.\nTo perform Zeroshot Classification on our model, wefirsthavetohavealistofcategoriessothatthe\nmodel knows what are the options it can classify each sentence into and later on proceed to classify\nthem into the category which the model thinks the sentence is more related to. So for the output of\nthis model after we ran it with our data, it is an additionalcolumninthedataframe,labellingwhatis\nthecommenttalkingaboutgiventhatthecategoriesweregivenby Chat GPT.\n"
      },
      {
        "title": "Content",
        "content": "3.7 Zeroshot Classificationor Topic Modellingandtheinterpretationoftheresult\nWe eventually decided that Zeroshot Classificationwillbebetterthan Topic Modellinginourproject.\nAndhereisthereasonwhy.\n"
      },
      {
        "title": "Thekeywordsof Topic Modellingaretoowide.Becausefor Topic Modellingthemodelisidentifying",
        "content": "the keywords for each sentence by itself. Therefore for every sentence, there will be a very diverse\nrange of keywords, which we will not be able to really deduce what this sentence is talking about.\n"
      },
      {
        "title": "Whereasfor Zeroshotclassification,thecategoriesaregivenbyus.Soitwillclassifyeachsentenceto",
        "content": "what we need to know about. Therefore eachsentencewillbefinetunedtoourcategorieswhichwill\nbeusedtoidentifydesignopportunities.\nFrom the result of the Zeroshot classification, we didabarcharttofindoutwhichisthecategorythat\nhas the most comments. In other words, which are thecategoriesofthephonethatpeoplehavemore\nnegative things to talkabout.Fromthefigurebelow,wefoundoutthatthetop4 categoriesofnothing\nphone (1) that most people have bad things tosayaboutis Features,Design,Price,and Performance.\nWhich will be the 4 categories we choose to focus more on when deducing our final design\nrequirements.\n"
      },
      {
        "title": "Content",
        "content": "Figure:Distributionofnegativecommentsfrom Sentimentanalysis\nAs can beseenbytheabovefigure,mostofthenegativecommentsaredissatisfiedaboutthe Features\nof Nothing Phone (1), due to the overwhelming negative comments on this specifications as\ncompared to other specifications, we decided to focus on designing improvements for the features,\ndesign, price and performance of the Nothing Phone(1) which are the top 4 classes thatreceivedthe\nmostnegativecomments.\n3.7 Word Cloud Visualization\nFigure:Wordcloudon Sentiment Analysis\nWe ran all the negative comments we obtained into the nltk Wordcloud text analysis to get a\nvisualisation of the most common words that appearedforeverycategory,wherethelargertheword,\nthebiggerthelettersize.\nEssentially, for each class we counted how many times each word appears, when we take thetop25\nhighestfrequencywordsanddisplayitina Wordcloud.\n3.5 Extractedcommentsforfinalevaluation\nAfter we successfully obtained the words that appeared the most for each class of comets asseenas\nthe Wordcloud figure above. We thought that these words will not give us the full understanding of\nwhatthesereviewsaretrulytalkingabout.\nSo we track back a little to get the comments data that has the top3 wordsthatappearedthemostin\nthe comments. How itworksisthatwefoundoutwhatarethe3 highestappearancefrequencywords,\n"
      },
      {
        "title": "Content",
        "content": "thenwewrotethecodeinsuchawaythatforallcommentsthathaveanyofthe3 topwords,weputit\ninanewdataframeandexportitasacsvfile.\nLastly as final evaluation, we analyse each comment in the csv file and finally deduce our design\nrequirements.\n"
      },
      {
        "title": "1. Finding Nothing Phone’sownidentityinthesmartphonemarket",
        "content": "There are many comments on how the Nothing Phone (1) is highly similar in terms of looks to the\ni Phone, with some people even calling it a cheap version of the i Phone. As seen in the Wordcloud\nimage for ‘Features’, “iphone” is one of the top 3 words that were mentioned in the comments. The\nfact that most comments referred to the i Phonejustgoestoshowthatthe Nothing Phone(1)doesnot\nhave a trademark of its own that peoplecaninstantlylinktothe Nothingcompanyimmediatelyupon\nseeing one of its products. For example, the i Phones are recognised for their flat edged bezel, while\nthe Samsung Galaxy series are known for their curved edged screens. Ifthe Nothing Phone(1)hasa\nfeature unique to the brand, peoplewillstopcomparingthe Nothing Phonetootherphonebrandsjust\nbasedonhowitlooksatfirstglance.\n"
      },
      {
        "title": "2. Differentsubmodelswithdifferentspecifications",
        "content": "There were people who commented on how they want higher processing power while others\nmentioned that having a smaller battery and charging asmallerpricewouldmakethe Nothing Phone\n(1)’s price range be a good mid rangepriceforaphone.Sobyhavingdifferentsubmodels,therecan\nbe different priceschargedfordifferentcombinationsofspecificationsthatwilltailortodifferentuser\nneeds.\n"
      },
      {
        "title": "3. Newbackingforthephone",
        "content": "Concernswereraisedaboutthewaterproofingforthephone,andtherewerealsocomplaintsaboutthe\nbacklights either being excessive or useless. One commenter mentioned that instead of having a\nbacklight, there could be a stronger back casing that does not shatter. Hence, the Nothing Phone (1)\ncan have a completely new backing where it takes all these comments into account. In other words,\nbalancingthefunctionalityandpracticalityofthedesigntogetherwiththeaestheticsofit.\nReflections\n"
      },
      {
        "title": "1. Time taken:it take very long time to run SA to dataset, requires desktop/ stronger computer",
        "content": "thatcanrunforhours\n"
      },
      {
        "title": "2. Irrelevancy: Despite multiple cleaning,therewouldstillbecommentsthatareleftbehindthat",
        "content": "areirrelevanttoourprojectgoals\n"
      },
      {
        "title": "3. Detection isirrelevant,here,asourproductiscalled“Nothing Phone1”itleavesuswithalot",
        "content": "of ‘phone’ and ‘nothing’ in thewordclouds,wherenothingcanbeusedtorefertothephone,\nitcanalsorefertocriticismoftheproduct.\nSummaryoflessonlearnt\nTo conclude our report, we will be sharing a summary of our lessons learnt when doingthisproject,\nandwouldwehavedonedifferentlyifweevercomeacrossasimilarprojectagain.\n"
      },
      {
        "title": "1. Deeperresearchoneachmodel’salgorithm.",
        "content": "In our project, we rely heavily on existing models and what we did essentially isonlytoapplythem\nwithout cleaned data. While we did some experiments and hadourowninterpretationofwhycertain\nmodels are better and certain models are worse. A lesson learnt is to do more thorough research on\nthe models that may be useful to our project, and study its algorithm answer the question of, will it\ntake longer to run?, how is it classifying certain things?, how does it deal with bad and irrelevant\ndata?, is it really suitable for comments/reviews classification or is it actually more for essays or\narticles?\n"
      },
      {
        "title": "2. Datacleaningisnotalinearprocess,butmoreofatrialanderrorprocess.",
        "content": "When we were doing our data cleaning, it was an iterative process. Reason being it was a constant\nprocess of us completing one part of the cleaning, then checking manually on the data itself to\nidentify what could be done better to the data. Take for example when we realised that there were a\nlot of random 39 s in our dataset which needed toberemoved,whenwerealisedonewordcomments\nwere invaluable for our study, and when we found out that some comments were repeated or non\nenglish comments. It was an iterative process of checking the data,findingoutwhatwaswrongwith\nit, and coming with the code to clean it more thoroughly. So do not expectthecleaningdataprocess\ntobeaoneshotfinishedprocess.\n"
      },
      {
        "title": "3. Differentsourcesfordifferentproducts",
        "content": "For different products that you are interested to do more research on,therewillbedifferentwebsites\nthat have more data on certain products. For our project, we manually checked which were the best\nsources to get data about the nothing phone (1) from and landed with reddit, youtube, and top 10\ngoogle websites. So it is importanttonotethatfordifferentproducts,wecannotusethesamesources\nas some products may be more popular in some sources and some products may be less popular in\nsome sources. Some of the keyfactorsweidentifywitharebecausesomesocialsources,forexample\nyoutube/facebook/instagram, have very different users. For example facebook users are normally\nolder people, instagram users are normally more of a teenage person. Therefore the topics that the\nusers discuss in both these apps will be very different. So it is very important to do research on the\nproductofinteresttofindoutwhicharethesourcesyoucanachievethebestdataandresultoutofit.\n"
      }
    ],
    "metadata": {
      "title": "Opportunitiesfor Design Improvementsfor Nothing Phone1 using Artificial Intelligence",
      "category": "academic_paper",
      "file_name": "AID_Project_2",
      "relative_path": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/AID_Project_2.pdf",
      "page_count": 16,
      "project_name": null,
      "file_size": 1806470,
      "last_modified": 1749024948.2513995
    },
    "word_count": 3334,
    "page_count": 16
  },
  {
    "id": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f_team9",
    "source_file": "/Users/weimingchin/Desktop/weiming_chatbot/data/raw/notion_export/Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/team9.pdf",
    "type": "pdf",
    "title": "Natural Language Processing for Stock",
    "category": "team_document",
    "raw_content": "\n--- Page 1 ---\nNatural Language Processing for Stock\nMarket Indicators\nAUTHORS STUDENT ID\nZhuoer Feng 1004971\nGe Ziyu 1004880\nMuhammad Abid Firas 1006017\nRio Chan Yu hoe 1005975\nChin Wei Ming 1006264\nSingapore University of Technology and Design\n50.038: Computational Data Science\nProf. Dorien Herremans, Prof. Soujanya Poria\nApril 8, 2023\n1\n\n--- Page 2 ---\n1. Introduction 3\n2. Methodology 3\n2.1. Workflow 3\n2.2. Dataset 5\n2.2.1. Dataset Collection 5\n2.2.2. Dataset Manipulation 5\n2.3. Sentiment Analysis 6\n2.3.1. Fine-tuning 6\n2.2.2. Sentiment Aggregation 7\n2.2.2.1. Tweets 7\n2.2.2.2. News Headlines 7\n2.2.2.3. Integrated Dataset for Predicting Stock Price Movements 8\n2.4. Data Pre-processing 8\n2.4.1. Motivation 8\n2.4.2. Methodologies 8\n2.4.2.1. Data cleaning 8\n2.4.2.2. Data transformation 9\n2.4.2.3. Data scaling 9\n2.5. Model 9\n2.5.1. Objective 10\n2.5.2. Evaluation Methodology 10\n2.5.3. Naive Model 11\n2.5.3.1. Framework 11\n2.5.3.2. Implementation 11\n2.5.4. Models Tested 13\n2.5.4.1. FBProphet 13\n2.5.4.2. Recurrent Neural Networks (RNNs) 13\n2.5.4.3. Linear Regression 14\n2.5.4.4. Random Forest 17\n3. Results and Discussion 21\nReferences 23\n2\n\n--- Page 3 ---\n1. Introduction\nAccurately predicting stock prices’ rise and fall has long been a dream of traders\nand researchers alike. The instantaneous nature of Twitter means that people's\nsentiments may be felt directly on stock prices, which created a new prediction\nlayer. Elon Musk's announcement of Tesla's privatization and Kylie Jenner's\ncomment on Snapchat both had dramatically impacted the stock prices of the\ntwocompanies. Inrecentyears,withtheadvancementinNLP,suchasFinBERT,\npeople have tried to incorporate public and/or professional sentiments into stock\nprice prediction.\nForinstance,Mehtab&Sen(2019)attemptedtopredictthemovementofNIFTY\n50 index 1 week into the future based on Twitter sentiments using an LSTM\nmodel, while Sonkiya et al (2021) used GAN and BERT to predict the price of\nApple Inc. (ticker symbol: AAPL) 5, 15, and 30 days into the future. Besides\nTwitter, which reflects the views of the public and influencers, some such as Puh\n& Babac (2023) also used professional views from the Wall Street Journal to\npredict the stock market.\nHowever, we found that existing literature has largely focused on the movement\nof stock prices multiple days into the future, which is at odds with the common\ntradingpracticeofdaytradingatfinancialinstitutions. Hence,ourprojectwould\nlike to focus on developing a model to predict the movement of stocks within\nthe trading day based on public sentiments i.e., tweets and market information.\n2. Methodology\n2.1. Workflow\nTo explore the impact of sentiment analysis on stock price prediction, we devel-\noped two workflows: a baseline version and an enhanced version, which includes\nadditional data.\nAsshowninthefigurebelow,thebaselineworkflowhasthreemaincomponents: a\ndatasetcomposedoftweets,sentimentanalysisofthesetweets,andaclassification\nmodel to predict stock price movements.\n3\n\n--- Page 4 ---\nTo broaden our exploration of stock indicators, we've expanded our dataset\nto include news articles and fine-tuned the FinancialBERT (FinBERT) Model.\nThese improvements aim to increase the accuracy of our sentiment analysis\nwithin the updated workflow, as displayed in the diagram below.\n2.2. Dataset\n2.2.1. Dataset Collection\nAs outlined in the workflow diagram presented in the previous section, we utilize\nfive distinct sub-datasets. Each dataset has a specific source and purpose as\ndetailed below.\n4\n\n--- Page 5 ---\n1. stock_market_tweets\nSource: \"mjw/stock_market_tweets\" from HuggingFace\nDescription: This dataset consists of tweets specifically related\nto stocks, including text content and additional details such as\nthe number of retweets, likes, and comments.\n2. stock_information\nSource: Scraped from Nasdaq\nDescription: Information about stocks including details about\ntheir country, sector, and industry.\n3. news\nSource: \"ashraq/financial-news\" from HuggingFace\nDescription: A collection of financial news headlines.\n4. twitter_financial_news_sentiment\nSource: \"zeroshot/twitter-financial-news-sentiment\" from Hug-\ngingFace\nDescription: An English corpus of finance-related tweets anno-\ntated for sentiment, used to fine-tune the FinBert model.\n5. Yahoo_Finance_OHLC_stock\nSource: Extracted using Yahoo Python API\nDescription: Stock price information, specifically Open, High,\nLow, and Close (OHLC) data.\n2.2.2. Dataset Manipulation\nThedatasets'stock_market_tweets'and'stock_information'aremergedthrough\nan inner join operation on the stock symbol.\nIn the case of the 'news' dataset, which lacks a specific column identifying\nthe relevant stock, we first compile a list of distinct company names from\n'stock_information'. We then attempt to extract patterns of these company\nnames from the news headlines.\nAfterpredictingandaggregatingsentimentscores,thedatasets'stock_market_tweets',\n'news', and 'Yahoo_Finance_OHLC_stock' are merged together based on\nthe stock name and date information. Further details about the sentiment\naggregation process will be discussed in the following section.\n2.3. Sentiment Analysis\nWe selected the pretrained FinBERT model, 'ahmedrachid/FinancialBERT-\nSentiment-Analysis,' to serve both as the tokenizer and the prediction model.\nThistransformer-based modeloutperforms traditionalBERT models inhandling\nfinancial texts. Optimized specifically for financial contexts, the sentiment labels\naredesignatedasfollows: thepositivelabelsuggestsa'buy'action,zeroindicates\n'do nothing,' and the negative label advises to 'sell'.\n5\n\n--- Page 6 ---\n2.3.1. Fine-tuning\nIn the baseline architecture, the model was not fine-tuned. However, for the\nenhanced version, we fine-tuned the model using 11,931 labeled financial tweets\nfrom the 'twitter_financial_news_sentiment' dataset. The training parameters\nemployed were as follows:\nlearning_rate=2e-5,\nper_device_train_batch_size=16,\nper_device_eval_batch_size=16,\nnum_train_epochs=20,\nweight_decay=0.01,\nevaluation_strategy=\"epoch\",\nsave_strategy=\"epoch\",\nmetric_for_best_model='accuracy'\nIn addition, we quantified the effects of preprocessing steps, which included\nremoving repeated punctuations, cleaning hashtags, and replacing URLs with\nthe '[URL]' token. As shown in the table, the sentiment prediction accuracy on\nthe test dataset remains relatively consistent, both before and after fine-tuning.\nThis indicates the tokenizer's effectiveness in handling messy text.\nBefore Fine-tuning After Fine-tuning\nWith Preprocessing 0.7428810720268006 0.8454773869346733\nWithout Preprocessing 0.7462311557788944 0.8408710217755444\n2.2.2. Sentiment Aggregation\n2.2.2.1. Tweets Given the large volume of tweets related to each stock on\nany given day, it becomes necessary to aggregate the respective sentiment scores\nto obtain a coherent overall sentiment. Furthermore, to gauge the social impact\nof specific tweets, we also consider engagement metrics to measure the public\ninfluence exerted by each tweet.\nThe sentiment score, however, originally a categorical variable with three labels\n(0 for negative, 1 for neutral, and 2 for positive), is not directly suitable for\nmultiplication with quantitative metrics like retweet counts, like counts, or\ncomment counts, as this operation lacks intuitive sense with categorical labels.\nToaddressthis,weconvertthesentimentscoreintoanumericalvaluebyassigning\nspecific weights to each category, allowing for the calculation of a weighted\nsentiment score. This method involves mapping the sentiment categories to\nnumerical values:\n6\n\n--- Page 7 ---\nsentiment_weights = {0: -1, 1: 0, 2: 1}\n• 0 represents negative sentiment (assigned a weight of -1)\n• 1 represents neutral sentiment (assigned a weight of 0)\n• 2 represents positive sentiment (assigned a weight of 1)\nThe formula for weighted average score calculation:\nThis formula calculates the weighted average of sentiment scores, taking into\naccount the weights provided by the retweet counts, like counts, and comment\ncounts for each tweet.\n2.2.2.2. News Headlines Fornewsitems, wecalculatetheaveragesentiment\nscoreofallheadlinesrelatedtoaspecificstockonthesameday. Additionally,we\ninclude an average sentiment score for news related to the stock's industry, thus\nincorporating an industry sentiment indicator. This dual-layered approach helps\nprovide a broader perspective on the sentiment influencing both the specific\nstock and its wider industry.\n2.2.2.3. Integrated Dataset for Predicting Stock Price Movements\nThe aggregated sentiment scores from tweets and news are combined with the\nOHLC (Open, High, Low, Close) pricing data based on stock name and tickers.\nThe resulting dataset, prepared for stock price prediction, includes the following\ncolumns:\n• post_date\n• ticker_symbol\n• Country\n• IPO Year\n• Sector\n• Industry\n• Open\n• High\n• Low\n• Close\n• Adj Close\n• Volume\n• Shifted_Open\n7\n\n--- Page 8 ---\n• Shifted_Adj Close\n• Shifted_Close\n• prev_day_Open\n• prev_day_Open_AdjClose\n• prev_day_Close\n• news_sentiment\n• industry_sentiment\n• weighted_tweet_sentiment\n2.4. Data Pre-processing\n2.4.1. Motivation\nThe data preprocessing consists of five major tasks, i.e., data cleaning, reduction,\nscaling, transformation and partitioning (Xiao and Fan, 2014; Fan et al., 2015a;\nFan et al., 2015b)\n. These techniques effectively address inconsistencies, redundancies, and irrele-\nvant information within the data, ultimately enhancing data quality and model\naccuracy (Gandomi and Haider, 2015)\n2.4.2. Methodologies\nThe techniques we used to preprocess the dataset, ensures its compatibility with\nvarious machine learning models. Through a series of strategies, the raw data\nunderwent preprocessing to enhance its usability and effectiveness in subsequent\nmodeling tasks.\n2.4.2.1. Data cleaning We clean the data to handle missing data effec-\ntively. Initially, missing values were addressed using the forward fill method\n(method='ffill') to propagate non-null values forward along the specified axis.\nSubsequently, rows with missing values in critical columns such as 'Open',\n'High', 'Low', 'Close', and 'Adj Close' were removed using the ‘dropna’ method.\nThis ensured that the dataset was cleansed of incomplete or erroneous records,\nfacilitating robust analysis and a viable dataset for training the models.\nInaddition,arollingaccumulationofsentimentscoreswasperformedtoaugment\nthe dataset's richness. This process involved carrying forward and averaging\nsentiment scores in rows where price columns contained missing values. By\naligning sentiment scores with price data through this rolling accumulation, the\ndataset's temporal coherence was maintained, facilitating a more comprehensive\nanalysis of market sentiment trends.\n8\n\n--- Page 9 ---\n2.4.2.2. Data transformation The dataset was restructured into numerical\nandcategoricaltoensurethatdifferenttypesoffeaturesareappropriatelyhandled\nduring preprocessing. Categorical features : ['ticker_symbol', 'Country', 'IPO\nYear', 'Sector', 'Industry'], within the dataset were transformed into a numerical\nrepresentation to make them compatible with machine learning algorithms. This\ntransformation was achieved through label encoding, wherein each categorical\nfeature's values were converted into numerical labels using the LabelEncoder\nmodule from the scikit-learn library. By encoding categorical variables into\nnumerical form, the dataset was prepared for subsequent analysis and model\ntraining.\nA new binary target variable, denoted as 'y', was generated to facilitate binary\nclassification. This variable was derived from the 'Close' and 'Open' price\ncolumns, where 'y' was assigned a value of 1 if the 'Close' price was greater\nthan the 'Open' price, indicating a positive price change. Conversely, 'y' was\nassignedavalueof0ifthe'Close'pricewaslessthanorequaltothe'Open'price,\nindicating a negative or neutral price change. This labeling process enhanced\nthe dataset's suitability for binary classification tasks.\n2.4.2.3. Data scaling To ensure consistency in feature scales and facilitate\nconvergence during model training, numerical features underwent scaling. The\nMin-Max scaling technique was employed, which rescales each feature to a\nspecified range (typically 0 to 1) using the MinMaxScaler module from scikit-\nlearn. By scaling numerical features, the dataset's features were normalized,\npreventing features with larger scales from dominating the learning process.\n2.5. Model\n2.5.1. Objective\nWe follow a typical day trading scenario, in which all positions are closed before\nthe trading day to minimize risks and price change after the market closes. The\ndecision to long, in anticipation of an increase in the stock price, or short, in\nanticipation of an decrease, is made at the opening of a day’s market, with the\nstock being sold or bought at the closing to close the position.\nTherefore, our algorithm attempts at answering the classification problem: will\nthestock’sclosingpricebehigherthantheopeningprice,givenhistoricalmarket\n9\n\n--- Page 10 ---\ndata and recent tweets & news about the company and/or industry?\n2.5.2. Evaluation Methodology\nIn our evaluation methodology, we will utilize different metrics depending on\nthe target variable type. For continuous, we employ Mean Squared Error\n(MSE) (Hyndman & Athanasopoulos, 2014). MSE measures the average squared\ndifferencebetweenpredictedandactualvalues,withlowerMSEindicatingbetter\nmodel performance for continuous prediction tasks.\nFor discrete (classification tasks), we use a combination of metrics. Firstly , we\nlook towards accuracy measurement, the ratio of correctly classified instances\n(Japkowicz & Stephen, 2016), as a common starting point. We define correctly\nclassifiedinstancesasactuallabeledyinputstobethesameaspredictedyinputs.\nAdditionally, we will employ Binary Cross-Entropy (BCE) loss (Kingma & Ba,\n2017). BCE loss measures the difference between predicted probabilities and\nactual class labels, and it's often used during model training as a loss function\nto minimize classification error.\nWewillfurtherexploreclassifierscores(precision,recall,F1-score)forimbalanced\nclasses (Sokolova et al., 2006) and consider confusion matrices (Powers, 2020) to\ngain deeper insights into our random forest model's performance.\n2.5.3. Naive Model\n2.5.3.1. Framework Twonaivemodelswereusedtohelpusgetatasteofthe\nproblem. Forthefirstnaivemodel,weframedthequestionasapureclassification\nproblem, combining the NLP result (the stock’s aggregated sentiment score) and\nthe market information (exogenous information e.g., sector) to predict whether\nthe closing price will be higher than the opening price.\nWe also tried to first frame the question as a regression problem, where we use\nthe NLP result, market information, and the opening price to predict the exact\nexpected closing price of the stock. The predicted closing price will then be\ncompared with the opening price to answer whether the price is expected to\ngo up during the day. We believe by framing the problem in two ways, we will\nbe able to test both classification and regression algorithms and make a more\n10\n\n--- Page 11 ---\ninformed decision on how to approach the problem for subsequent modeling. A\nsimple illustration of the logic flow of our naive model is shown below.\n2.5.3.2. Implementation We used PyCaret as our naive model implementa-\ntion, which trains and compares across different machine learning models, and\nselects the one with the best accuracy after 10-fold validation.\nFor the classification approach, the highest accuracy of 57.01% was obtained\nby an AdaBoost Classifier model. In comparison, the dummy model returned a\n51.25% accuracy. The confusion matrix and the top 3 performing models are\ngiven below.\n11\n\n--- Page 12 ---\nFor the regression approach, the highest accuracy obtained was slightly worse at\n55.79%. However, we were able to confirm the importance of twitter sentiment\non stock price movement. As shown in the feature importance plot below, the\nvariable weighted_sentiment_score is the second most important feature among\nall, only falling behind the opening price but vastly exceeds the importance of\nother exogenous variables like the sector and the companies’ IPO year. This\nconfirms the role twitter sentiment plays in dictating stock price movement and\nprompted us to fine-tune the sentiment evaluation and improve our model.\n2.5.4. Models Tested\n2.5.4.1. FBProphet FBProphet is a time series forecasting tool by Facebook\nOpenSource. Besidesthetimeseriesofclosingpriceitself,FBProphetalsoallows\nus to use other non-temporal regressors to perform a multivariate prediction. In\nourmodel,wetriedtopredictthenextday’sclosingprice,basedonthehistorical\ntime series of: the closing price, the trading day’s opening price, stock-specific\ntwittersentiment, newssentiment, andindustrysentiment. Wealsoacknowledge\nthat there may be a “delay” in a tweet’s impact on the stock price, for instance\nwhen the tweet was posted between the closure of the previous day’s market\nand the opening of the present day’s market. Hence, for all sentiment-related\nvariables, we included both sentiment scores on the day itself and the previous\nday. As an example, for Twitter sentiment, we use the following two variables as\nour regressors:\n• weighted_sentiment_score: Tweets’ weighted aggregate senti-\nment on the trading day\n• weighted_sentiment_score_-1: Tweets’ weighted aggregate sen-\n12\n\n--- Page 13 ---\ntiment from the previous trading day.\nTheresultswereobtainediteratively,forwhichtheFBProphetmodelisretrained\neveryday,withnewinformationadded. Toensuresufficienttrainingdatainitially,\nwe start training the time series model on the 366th trading day, which is equal\nto around 30% of all data. However, we did not witness a significant accuracy\nincrease for later days as we accumulate a longer time series. We were able to\nobtain an overall testing accuracy of around 58.61% from our FBProphet model.\n2.5.4.2. Recurrent Neural Networks (RNNs) Our dataset inherently\nembodies a time series prediction framework, characterized by timestamps for\neach entry. In light of this temporal structure, Recurrent Neural Networks\n(RNNs) were selected for their capability to leverage sequential data. Various\nRNN architectures, including Gated Recurrent Units (GRU) with attention\nmechanismsandLongShort-TermMemory(LSTM)networks,wereimplemented\nto capture the intricate patterns and features embedded within the dataset,\nultimately aiming to predict asset prices.\nOur initial approach involved amalgamating data from all ticker symbols into\na unified dataset. This merged dataset facilitated the collective analysis of\ncategorical and numerical features. The training and testing datasets were then\npartitioned, with the training set sequenced to preserve temporal dependencies.\nEvaluation of the model on the test data revealed an accuracy of 55.68%.\nSubsequently, we pursued a more nuanced analysis by segregating the dataset\nbased on individual ticker symbols. This approach unveiled distinct performance\nvariationsacrossdifferentdatasets. Notably,themodelexhibitedhigheraccuracy\n(61.62%) for the Microsoft (MSFT) ticker symbol, while yielding lower accuracy\n(51.14%) for Tesla (TSLA). These discrepancies underscored the model's propen-\nsity to excel in specific contexts, hinting at potential limitations in its robustness\nand adaptability.\nTo further elucidate the underlying dynamics, we explored alternative RNN\narchitectures, including GRU and attention models. Despite variations in model\ncomplexity, the results remained largely consistent. This convergence suggested\nthat performance trends were less influenced by architectural nuances and more\nby shared factors such as hyperparameters, initializations, and loss functions.\nOur emphasis remained on understanding the fundamental characteristics of the\nbase model, with minimal fine-tuning deemed necessary.\nIncorporating sentiment accumulation strategies into the dataset yielded promis-\ning results, with each model experiencing a modest increase in accuracy ranging\nfrom 1% to 2%. However, a compelling discovery emerged during feature impor-\ntance analysis. Contrary to expectations, sentiment scores exhibited uniform\nweightage alongside other inputs, implying that they were not predominant\nfactors driving model predictions. This revelation prompted a reevaluation of\nour approach, signaling the need for alternative methodologies to enhance model\naccuracy and efficacy.\n13\n\n--- Page 14 ---\n2.5.4.3. Linear Regression Inourpursuitofmodelingtemporaldata,Linear\nRegression emerged as a candidate worthy of exploration. Employing various\nmethodologies,includingSupportVectorMachines(SVM)andHuberRegression,\nwe sought to evaluate their predictive accuracy.\nInitially, we assessed the performance of these models on a merged dataset,\nwhere all ticker symbols were consolidated. The Mean Squared Error (MSE) for\nLinear Regression was 0.713, for Huber Regression it was 0.686, and for SVM it\nwas notably higher at 78.24. These results provided an initial glimpse into the\nmodels' efficacy in handling the combined dataset.\nSubsequently, we conducted a more granular analysis by splitting the dataset\nbased on individual ticker symbols. For instance, for the AAPL_data.csv, the\nMSE for Linear Regression was 2.373, for Huber Regression it was 2.371, and\nfor SVM it was 25.05. Despite achieving high accuracies ranging from 88.06% to\n98.50%, it became evident that the models' performance varied across different\ndatasets.\n14\n\n--- Page 15 ---\nFurtheranalysisrevealedthedominanceofthe'Open'pricefeatureininfluencing\npredictions, indicating a misalignment with our goal of leveraging sentiment\nscores for price prediction.\nAttempts to de-emphasize the 'Open' price and prioritize sentiment scores led to\na drastic increase in MSE and a decline in accuracy. For instance, excluding the\n'Open' price feature resulted in significantly higher MSE values (Linear Regres-\nsion: 163.64, Huber Regression: 162.11, SVM: 158.94) and reduced accuracies\n(Linear Regression: 50.75%, Huber Regression: 47.76%, SVM: 52.24%).\n15\n\n--- Page 16 ---\nSplit Dataset\nMerged Dataset Split Dataset without ‘Open’\nModel (MSE) (MSE) (MSE)\nLinear Regression 0.713 2.373 163.64\nHuber 0.686 2.371 162.11\nSVM 78.24 25.05 158.94\nFrom the above table, the findings underscore the challenge of reconciling model\nobjectives with feature importance. While Linear Regression demonstrated high\naccuracy, its reliance on 'Open' price undermined the incorporation of sentiment\nscores. This highlights the need for tailored approaches that strike a balance\nbetween feature relevance and predictive performance, paving the way for more\nnuanced modeling strategies.\n2.5.4.4. Random Forest ThechoiceofutilizingRandomForestasourmodel\nwas inspired by a reference project on a GitHub repository by Anubhav Anand\non his stock market prediction using sentiment analysis project. In his project,\nby utilizing Random Forest, he successfully achieve the accuracy of 91.96% on\nUnited Airlines stock.\nWe utilized Apple stock for this experimentation. Firstly, we ran the base model\nof random forest taking in account weighted_sentiment_score, Open, Close,\nLow, High, news_sentiment, industry_sentiment as features and the output will\nbe an indicator of 1 or 0. An indicator of 1 will signal that the closing price is\ngreater than the opening price, and an indicator of 0 will signal otherwise.\n16\n\n--- Page 17 ---\nOur test result for the base random forest model is as shown:\nAs well as the confusion matrix:\nWealsoexperimentedwiththetrainedrandomforestmodelontheoriginaltrain\ndataset. The results is as follows:\nAs well as the confusion matrix:\n17\n\n--- Page 18 ---\nAnalyzing the results of how well the model performed with the train data and\ntest data, we hypothesize that the model is overfitting on the train data. The\nhuge difference in accuracy of train data (98.60%) and test data (59.72%) shows\nthat the model does not perform well with unseen data.\nFollowing this observation, we looked further into the parameters of the Random\nForest model. We found an article that talks about hyperparameter tuning\nwith Random Forest and attempted to follow its approach in finding the best\nparameters for our model.\nWe will be trying to adjust the following hyperparameters:\n• N_estimators - This parameter allow us to specify the number\nof trees in the Random Forest\n• Max_depth - Controlling the maximum depth of each tree in\nthe Forest\n• Min_samples_split-Specifyingtheminimumnumberofsamples\nrequired to split an internal node.\n• Max_features - Determines the number of features to consider.\n• Min_samples_leaf - Setting the minimum number of samples\nrequired to be at a leaf node.\n• Bootstrap-Determinewhetherbootstrapsamplesareusedwhen\nconstructing trees.\nUsing a GridSearchCV, we create a parameter grid and perform 5-fold cross\nvalidation on each combination of the parameters we defined to search through.\nThe 5-fold cross validation will give us the most extensive evaluation of the\nperformance of each combination of parameters. It will split the data to 5 sets,\n18\n\n--- Page 19 ---\nand it will fit the model 5 times and take a different set as a test set to evaluate\nthe performance for each 5 times.\nFollowing the search, we found the best performing parameters:\nBest Hyperparameters:\n{’bootstrap’ : True ,\n‘max_depth’ : None ,\n‘max_features’ : ‘sqrt’ ,\n‘min_sample_leaf’ : 1 ,\n‘min_sample_split’: 2 ,\n‘n_estimators’: 100}\nEvaluating the performance of the model under these hyperparameters, we can\nsee an improvement of robustness of the model.\nPerformance of best hyperparameter model on train dataset:\nAs well as its confusion matrix:\nPerformance of best hyperparameter model on test dataset:\n19\n\n--- Page 20 ---\nAs well as its confusion matrix:\nWeseeasignificantimprovementintherobustnessofthemodelafterperforming\nhyperparameter tuning for our Random Forest Model. With the a smaller\ndifference between the accuracy of test and train data as compared with the\nbase model without hyperparameter tuning. Furthermore, we see a significant\nincrease in the performance of the model from 59.72% to 90.28%.\nNext we evaluate the feature importance of the model:\n20\n\n--- Page 21 ---\nAs seen in the chart above, we gladly see the weighted_sentiment_score has the\nbiggest importance, which supports our initial hypothesis that twitter sentiment\ndoes play a part in the stock market.\n3. Results and Discussion\nIn this paper, we experimented with multiple machine learning models and\nour 2 highest performing models are Linear Regression and Random Forest.\nBut looking at the feature importance chart of the Linear Regression model,\ntwitter sentiment does not play a big part when predicting the stock movement,\nwhereas for the Random Forest model, we see a significant improvement in both\nrobustness and performance after hyperparameter tuning. And we can see that\ntwitter sentiment does play a part in predicting stock movement in the Random\nForest model.\nAcknowledging a limitation that we have in our dataset is there is an imbalance\nin the 1s and 0s as we can see in the bar chart below for Apple Stock:\n21\n\n--- Page 22 ---\nAlthough our best performing Random Forest model has the potential of pre-\ndicting stock prices with a relatively high accuracy, this limitation would hugely\ninfluence the credibility of our model.\nWe would like to acknowledge that there are plenty of approaches when tackling\nthisproblemandourapproachmaynotbethebestway. Tofurtherevaluateand\ncreate a model that is able to predict stock market movement with a credible\naccuracy,wewouldhavetoexperimentwithmultipleotherapproachesintackling\nthis problem as well as other machine learning models which we did not manage\nto explore given our time constraint.\nBut in this paper, we can conclude that twitter sentiment does play a part in\npredicting stock market movement as we can observe in the feature importance\nchart.\nReferences\nMehtab,S.,Sen,J.(2019). ARobustPredictiveModelforStockPricePrediction\nUsing Deep Learning and Natural Language Processing. arXiv:1912.07700 [q-\nfin.ST].\nhttps://doi.org/10.48550/arXiv.1912.07700\nPuh, K., & Bagić Babac, M. (2023). Predicting stock market using natural\nlanguage processing. American Journal of Business, 38(2), 41-61.\nhttps://doi.org/10.1108/AJB-08-2022-0124\nSonkiya, P., et al. (2021). Stock price prediction using BERT and GAN.\narXiv:2107.09055 [q-fin.ST].\nhttps://doi.org/10.48550/arXiv.2107.09055\n22\n\n--- Page 23 ---\n(PDF) machine learning: A review on Binary Classification. (n.d.).\nhttps://www.researchgate.net/publication/313779520_Machine_Learning_A_\nReview_on_Binary_Classification\nSabaren,L.N.,Mascheroni,M.A.,Greiner,C.L.,&Irrazábal,E.(1970,January\n1). A systematic literature review in cross-browser testing. Journal of Computer\nScience and Technology.\nhttp://www.redalyc.org/journal/6380/638067784001/html/\nText mining in medicine – knowledge discovery and intelligent systems – KDIS\n– University of Córdoba. Knowledge Discovery and Intelligent Systems KDIS\nUniversity of Crdoba. (n.d.).\nhttp://www.uco.es/kdis/textminingmedicine/\nForecasting: Principles and practice (3rd ed). OTexts. (n.d.).\nhttps://otexts.com/fpp3/\nFan, C., Chen, M., Wang, X., Wang, J., & Huang, B. (2021, February 15). A\nreview on data preprocessing techniques toward efficient and reliable knowledge\ndiscovery from building operational data. Frontiers.\nhttps://www.frontiersin.org/articles/10.3389/fenrg.2021.652801/full\nhttps://github.com/anubhavanand12qw/\nSTOCK-PRICE-PREDICTION-USING-TWITTER-SENTIMENT-ANALYSIS\nKoehrsen, W. (2018, January 10). Hyperparameter tuning the random forest in\npython. Medium.\nhttps://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-\npython-using-scikit-learn-28d2aa77dd74#:~:text=In%20the%20case%20of%20a\n,each%20node%20learned%20during%20training\n23\n",
    "cleaned_content": "[PAGE BREAK]\nNatural Language Processing for Stock\nMarket Indicators\nAUTHORS STUDENT ID\nZhuoer Feng 1004971\nGe Ziyu 1004880\nMuhammad Abid Firas 1006017\nRio Chan Yu hoe 1005975\nChin Wei Ming 1006264\nSingapore University of Technology and Design\n50.038: Computational Data Science\nProf. Dorien Herremans, Prof. Soujanya Poria\nApril 8, 2023\n[PAGE BREAK]\n1. Introduction 3\n2. Methodology 3\n2.1. Workflow 3\n2.2. Dataset 5\n2.2.1. Dataset Collection 5\n2.2.2. Dataset Manipulation 5\n2.3. Sentiment Analysis 6\n2.3.1. Fine-tuning 6\n2.2.2. Sentiment Aggregation 7\n2.2.2.1. Tweets 7\n2.2.2.2. News Headlines 7\n2.2.2.3. Integrated Dataset for Predicting Stock Price Movements 8\n2.4. Data Pre-processing 8\n2.4.1. Motivation 8\n2.4.2. Methodologies 8\n2.4.2.1. Data cleaning 8\n2.4.2.2. Data transformation 9\n2.4.2.3. Data scaling 9\n2.5. Model 9\n2.5.1. Objective 10\n2.5.2. Evaluation Methodology 10\n2.5.3. Naive Model 11\n2.5.3.1. Framework 11\n2.5.3.2. Implementation 11\n2.5.4. Models Tested 13\n2.5.4.1. FBProphet 13\n2.5.4.2. Recurrent Neural Networks (RNNs) 13\n2.5.4.3. Linear Regression 14\n2.5.4.4. Random Forest 17\n3. Results and Discussion 21\nReferences 23\n[PAGE BREAK]\n1. Introduction\nAccurately predicting stock prices’ rise and fall has long been a dream of traders\nand researchers alike. The instantaneous nature of Twitter means that people's\nsentiments may be felt directly on stock prices, which created a new prediction\nlayer. Elon Musk's announcement of Tesla's privatization and Kylie Jenner's\ncomment on Snapchat both had dramatically impacted the stock prices of the\ntwocompanies. Inrecentyears,withtheadvancementin NLP,suchas Fin BERT,\npeople have tried to incorporate public and/or professional sentiments into stock\nprice prediction.\nForinstance,Mehtab&Sen(2019)attemptedtopredictthemovementof NIFTY\n50 index 1 week into the future based on Twitter sentiments using an LSTM\nmodel, while Sonkiya et al (2021) used GAN and BERT to predict the price of\nApple Inc. (ticker symbol: AAPL) 5, 15, and 30 days into the future. Besides\nTwitter, which reflects the views of the public and influencers, some such as Puh\n& Babac (2023) also used professional views from the Wall Street Journal to\npredict the stock market.\nHowever, we found that existing literature has largely focused on the movement\nof stock prices multiple days into the future, which is at odds with the common\ntradingpracticeofdaytradingatfinancialinstitutions. Hence,ourprojectwould\nlike to focus on developing a model to predict the movement of stocks within\nthe trading day based on public sentiments i.e., tweets and market information.\n2. Methodology\n2.1. Workflow\nTo explore the impact of sentiment analysis on stock price prediction, we devel-\noped two workflows: a baseline version and an enhanced version, which includes\nadditional data.\nAsshowninthefigurebelow,thebaselineworkflowhasthreemaincomponents: a\ndatasetcomposedoftweets,sentimentanalysisofthesetweets,andaclassification\nmodel to predict stock price movements.\n[PAGE BREAK]\nTo broaden our exploration of stock indicators, we've expanded our dataset\nto include news articles and fine-tuned the Financial BERT (Fin BERT) Model.\nThese improvements aim to increase the accuracy of our sentiment analysis\nwithin the updated workflow, as displayed in the diagram below.\n2.2. Dataset\n2.2.1. Dataset Collection\nAs outlined in the workflow diagram presented in the previous section, we utilize\nfive distinct sub-datasets. Each dataset has a specific source and purpose as\ndetailed below.\n[PAGE BREAK]\n1. stock_market_tweets\nSource: \"mjw/stock_market_tweets\" from Hugging Face\nDescription: This dataset consists of tweets specifically related\nto stocks, including text content and additional details such as\nthe number of retweets, likes, and comments.\n2. stock_information\nSource: Scraped from Nasdaq\nDescription: Information about stocks including details about\ntheir country, sector, and industry.\n3. news\nSource: \"ashraq/financial-news\" from Hugging Face\nDescription: A collection of financial news headlines.\n4. twitter_financial_news_sentiment\nSource: \"zeroshot/twitter-financial-news-sentiment\" from Hug-\nging Face\nDescription: An English corpus of finance-related tweets anno-\ntated for sentiment, used to fine-tune the Fin Bert model.\n5. Yahoo_Finance_OHLC_stock\nSource: Extracted using Yahoo Python API\nDescription: Stock price information, specifically Open, High,\nLow, and Close (OHLC) data.\n2.2.2. Dataset Manipulation\nThedatasets'stock_market_tweets'and'stock_information'aremergedthrough\nan inner join operation on the stock symbol.\nIn the case of the 'news' dataset, which lacks a specific column identifying\nthe relevant stock, we first compile a list of distinct company names from\n'stock_information'. We then attempt to extract patterns of these company\nnames from the news headlines.\nAfterpredictingandaggregatingsentimentscores,thedatasets'stock_market_tweets',\n'news', and 'Yahoo_Finance_OHLC_stock' are merged together based on\nthe stock name and date information. Further details about the sentiment\naggregation process will be discussed in the following section.\n2.3. Sentiment Analysis\nWe selected the pretrained Fin BERT model, 'ahmedrachid/Financial BERT-\nSentiment-Analysis,' to serve both as the tokenizer and the prediction model.\nThistransformer-based modeloutperforms traditional BERT models inhandling\nfinancial texts. Optimized specifically for financial contexts, the sentiment labels\naredesignatedasfollows: thepositivelabelsuggestsa'buy'action,zeroindicates\n'do nothing,' and the negative label advises to 'sell'.\n[PAGE BREAK]\n2.3.1. Fine-tuning\nIn the baseline architecture, the model was not fine-tuned. However, for the\nenhanced version, we fine-tuned the model using 11,931 labeled financial tweets\nfrom the 'twitter_financial_news_sentiment' dataset. The training parameters\nemployed were as follows:\nlearning_rate=2 e-5,\nper_device_train_batch_size=16,\nper_device_eval_batch_size=16,\nnum_train_epochs=20,\nweight_decay=0.01,\nevaluation_strategy=\"epoch\",\nsave_strategy=\"epoch\",\nmetric_for_best_model='accuracy'\nIn addition, we quantified the effects of preprocessing steps, which included\nremoving repeated punctuations, cleaning hashtags, and replacing URLs with\nthe '[URL]' token. As shown in the table, the sentiment prediction accuracy on\nthe test dataset remains relatively consistent, both before and after fine-tuning.\nThis indicates the tokenizer's effectiveness in handling messy text.\nBefore Fine-tuning After Fine-tuning\nWith Preprocessing 0.7428810720268006 0.8454773869346733\nWithout Preprocessing 0.7462311557788944 0.8408710217755444\n2.2.2. Sentiment Aggregation\n2.2.2.1. Tweets Given the large volume of tweets related to each stock on\nany given day, it becomes necessary to aggregate the respective sentiment scores\nto obtain a coherent overall sentiment. Furthermore, to gauge the social impact\nof specific tweets, we also consider engagement metrics to measure the public\ninfluence exerted by each tweet.\nThe sentiment score, however, originally a categorical variable with three labels\n(0 for negative, 1 for neutral, and 2 for positive), is not directly suitable for\nmultiplication with quantitative metrics like retweet counts, like counts, or\ncomment counts, as this operation lacks intuitive sense with categorical labels.\nToaddressthis,weconvertthesentimentscoreintoanumericalvaluebyassigning\nspecific weights to each category, allowing for the calculation of a weighted\nsentiment score. This method involves mapping the sentiment categories to\nnumerical values:\n[PAGE BREAK]\nsentiment_weights = {0: -1, 1: 0, 2: 1}\n• 0 represents negative sentiment (assigned a weight of -1)\n• 1 represents neutral sentiment (assigned a weight of 0)\n• 2 represents positive sentiment (assigned a weight of 1)\nThe formula for weighted average score calculation:\nThis formula calculates the weighted average of sentiment scores, taking into\naccount the weights provided by the retweet counts, like counts, and comment\ncounts for each tweet.\n2.2.2.2. News Headlines Fornewsitems, wecalculatetheaveragesentiment\nscoreofallheadlinesrelatedtoaspecificstockonthesameday. Additionally,we\ninclude an average sentiment score for news related to the stock's industry, thus\nincorporating an industry sentiment indicator. This dual-layered approach helps\nprovide a broader perspective on the sentiment influencing both the specific\nstock and its wider industry.\n2.2.2.3. Integrated Dataset for Predicting Stock Price Movements\nThe aggregated sentiment scores from tweets and news are combined with the\nOHLC (Open, High, Low, Close) pricing data based on stock name and tickers.\nThe resulting dataset, prepared for stock price prediction, includes the following\ncolumns:\n• post_date\n• ticker_symbol\n• Country\n• IPO Year\n• Sector\n• Industry\n• Open\n• High\n• Low\n• Close\n• Adj Close\n• Volume\n• Shifted_Open\n[PAGE BREAK]\n• Shifted_Adj Close\n• Shifted_Close\n• prev_day_Open\n• prev_day_Open_Adj Close\n• prev_day_Close\n• news_sentiment\n• industry_sentiment\n• weighted_tweet_sentiment\n2.4. Data Pre-processing\n2.4.1. Motivation\nThe data preprocessing consists of five major tasks, i.e., data cleaning, reduction,\nscaling, transformation and partitioning (Xiao and Fan, 2014; Fan et al., 2015 a;\nFan et al., 2015 b)\n. These techniques effectively address inconsistencies, redundancies, and irrele-\nvant information within the data, ultimately enhancing data quality and model\naccuracy (Gandomi and Haider, 2015)\n2.4.2. Methodologies\nThe techniques we used to preprocess the dataset, ensures its compatibility with\nvarious machine learning models. Through a series of strategies, the raw data\nunderwent preprocessing to enhance its usability and effectiveness in subsequent\nmodeling tasks.\n2.4.2.1. Data cleaning We clean the data to handle missing data effec-\ntively. Initially, missing values were addressed using the forward fill method\n(method='ffill') to propagate non-null values forward along the specified axis.\nSubsequently, rows with missing values in critical columns such as 'Open',\n'High', 'Low', 'Close', and 'Adj Close' were removed using the ‘dropna’ method.\nThis ensured that the dataset was cleansed of incomplete or erroneous records,\nfacilitating robust analysis and a viable dataset for training the models.\nInaddition,arollingaccumulationofsentimentscoreswasperformedtoaugment\nthe dataset's richness. This process involved carrying forward and averaging\nsentiment scores in rows where price columns contained missing values. By\naligning sentiment scores with price data through this rolling accumulation, the\ndataset's temporal coherence was maintained, facilitating a more comprehensive\nanalysis of market sentiment trends.\n[PAGE BREAK]\n2.4.2.2. Data transformation The dataset was restructured into numerical\nandcategoricaltoensurethatdifferenttypesoffeaturesareappropriatelyhandled\nduring preprocessing. Categorical features : ['ticker_symbol', 'Country', 'IPO\nYear', 'Sector', 'Industry'], within the dataset were transformed into a numerical\nrepresentation to make them compatible with machine learning algorithms. This\ntransformation was achieved through label encoding, wherein each categorical\nfeature's values were converted into numerical labels using the Label Encoder\nmodule from the scikit-learn library. By encoding categorical variables into\nnumerical form, the dataset was prepared for subsequent analysis and model\ntraining.\nA new binary target variable, denoted as 'y', was generated to facilitate binary\nclassification. This variable was derived from the 'Close' and 'Open' price\ncolumns, where 'y' was assigned a value of 1 if the 'Close' price was greater\nthan the 'Open' price, indicating a positive price change. Conversely, 'y' was\nassignedavalueof0 ifthe'Close'pricewaslessthanorequaltothe'Open'price,\nindicating a negative or neutral price change. This labeling process enhanced\nthe dataset's suitability for binary classification tasks.\n2.4.2.3. Data scaling To ensure consistency in feature scales and facilitate\nconvergence during model training, numerical features underwent scaling. The\nMin-Max scaling technique was employed, which rescales each feature to a\nspecified range (typically 0 to 1) using the Min Max Scaler module from scikit-\nlearn. By scaling numerical features, the dataset's features were normalized,\npreventing features with larger scales from dominating the learning process.\n2.5. Model\n2.5.1. Objective\nWe follow a typical day trading scenario, in which all positions are closed before\nthe trading day to minimize risks and price change after the market closes. The\ndecision to long, in anticipation of an increase in the stock price, or short, in\nanticipation of an decrease, is made at the opening of a day’s market, with the\nstock being sold or bought at the closing to close the position.\nTherefore, our algorithm attempts at answering the classification problem: will\nthestock’sclosingpricebehigherthantheopeningprice,givenhistoricalmarket\n[PAGE BREAK]\ndata and recent tweets & news about the company and/or industry?\n2.5.2. Evaluation Methodology\nIn our evaluation methodology, we will utilize different metrics depending on\nthe target variable type. For continuous, we employ Mean Squared Error\n(MSE) (Hyndman & Athanasopoulos, 2014). MSE measures the average squared\ndifferencebetweenpredictedandactualvalues,withlower MSEindicatingbetter\nmodel performance for continuous prediction tasks.\nFor discrete (classification tasks), we use a combination of metrics. Firstly , we\nlook towards accuracy measurement, the ratio of correctly classified instances\n(Japkowicz & Stephen, 2016), as a common starting point. We define correctly\nclassifiedinstancesasactuallabeledyinputstobethesameaspredictedyinputs.\nAdditionally, we will employ Binary Cross-Entropy (BCE) loss (Kingma & Ba,\n2017). BCE loss measures the difference between predicted probabilities and\nactual class labels, and it's often used during model training as a loss function\nto minimize classification error.\nWewillfurtherexploreclassifierscores(precision,recall,F1-score)forimbalanced\nclasses (Sokolova et al., 2006) and consider confusion matrices (Powers, 2020) to\ngain deeper insights into our random forest model's performance.\n2.5.3. Naive Model\n2.5.3.1. Framework Twonaivemodelswereusedtohelpusgetatasteofthe\nproblem. Forthefirstnaivemodel,weframedthequestionasapureclassification\nproblem, combining the NLP result (the stock’s aggregated sentiment score) and\nthe market information (exogenous information e.g., sector) to predict whether\nthe closing price will be higher than the opening price.\nWe also tried to first frame the question as a regression problem, where we use\nthe NLP result, market information, and the opening price to predict the exact\nexpected closing price of the stock. The predicted closing price will then be\ncompared with the opening price to answer whether the price is expected to\ngo up during the day. We believe by framing the problem in two ways, we will\nbe able to test both classification and regression algorithms and make a more\n[PAGE BREAK]\ninformed decision on how to approach the problem for subsequent modeling. A\nsimple illustration of the logic flow of our naive model is shown below.\n2.5.3.2. Implementation We used Py Caret as our naive model implementa-\ntion, which trains and compares across different machine learning models, and\nselects the one with the best accuracy after 10-fold validation.\nFor the classification approach, the highest accuracy of 57.01% was obtained\nby an Ada Boost Classifier model. In comparison, the dummy model returned a\n51.25% accuracy. The confusion matrix and the top 3 performing models are\ngiven below.\n[PAGE BREAK]\nFor the regression approach, the highest accuracy obtained was slightly worse at\n55.79%. However, we were able to confirm the importance of twitter sentiment\non stock price movement. As shown in the feature importance plot below, the\nvariable weighted_sentiment_score is the second most important feature among\nall, only falling behind the opening price but vastly exceeds the importance of\nother exogenous variables like the sector and the companies’ IPO year. This\nconfirms the role twitter sentiment plays in dictating stock price movement and\nprompted us to fine-tune the sentiment evaluation and improve our model.\n2.5.4. Models Tested\n2.5.4.1. FBProphet FBProphet is a time series forecasting tool by Facebook\nOpen Source. Besidesthetimeseriesofclosingpriceitself,FBProphetalsoallows\nus to use other non-temporal regressors to perform a multivariate prediction. In\nourmodel,wetriedtopredictthenextday’sclosingprice,basedonthehistorical\ntime series of: the closing price, the trading day’s opening price, stock-specific\ntwittersentiment, newssentiment, andindustrysentiment. Wealsoacknowledge\nthat there may be a “delay” in a tweet’s impact on the stock price, for instance\nwhen the tweet was posted between the closure of the previous day’s market\nand the opening of the present day’s market. Hence, for all sentiment-related\nvariables, we included both sentiment scores on the day itself and the previous\nday. As an example, for Twitter sentiment, we use the following two variables as\nour regressors:\n• weighted_sentiment_score: Tweets’ weighted aggregate senti-\nment on the trading day\n• weighted_sentiment_score_-1: Tweets’ weighted aggregate sen-\n[PAGE BREAK]\ntiment from the previous trading day.\nTheresultswereobtainediteratively,forwhichthe FBProphetmodelisretrained\neveryday,withnewinformationadded. Toensuresufficienttrainingdatainitially,\nwe start training the time series model on the 366 th trading day, which is equal\nto around 30% of all data. However, we did not witness a significant accuracy\nincrease for later days as we accumulate a longer time series. We were able to\nobtain an overall testing accuracy of around 58.61% from our FBProphet model.\n2.5.4.2. Recurrent Neural Networks (RNNs) Our dataset inherently\nembodies a time series prediction framework, characterized by timestamps for\neach entry. In light of this temporal structure, Recurrent Neural Networks\n(RNNs) were selected for their capability to leverage sequential data. Various\nRNN architectures, including Gated Recurrent Units (GRU) with attention\nmechanismsand Long Short-Term Memory(LSTM)networks,wereimplemented\nto capture the intricate patterns and features embedded within the dataset,\nultimately aiming to predict asset prices.\nOur initial approach involved amalgamating data from all ticker symbols into\na unified dataset. This merged dataset facilitated the collective analysis of\ncategorical and numerical features. The training and testing datasets were then\npartitioned, with the training set sequenced to preserve temporal dependencies.\nEvaluation of the model on the test data revealed an accuracy of 55.68%.\nSubsequently, we pursued a more nuanced analysis by segregating the dataset\nbased on individual ticker symbols. This approach unveiled distinct performance\nvariationsacrossdifferentdatasets. Notably,themodelexhibitedhigheraccuracy\n(61.62%) for the Microsoft (MSFT) ticker symbol, while yielding lower accuracy\n(51.14%) for Tesla (TSLA). These discrepancies underscored the model's propen-\nsity to excel in specific contexts, hinting at potential limitations in its robustness\nand adaptability.\nTo further elucidate the underlying dynamics, we explored alternative RNN\narchitectures, including GRU and attention models. Despite variations in model\ncomplexity, the results remained largely consistent. This convergence suggested\nthat performance trends were less influenced by architectural nuances and more\nby shared factors such as hyperparameters, initializations, and loss functions.\nOur emphasis remained on understanding the fundamental characteristics of the\nbase model, with minimal fine-tuning deemed necessary.\nIncorporating sentiment accumulation strategies into the dataset yielded promis-\ning results, with each model experiencing a modest increase in accuracy ranging\nfrom 1% to 2%. However, a compelling discovery emerged during feature impor-\ntance analysis. Contrary to expectations, sentiment scores exhibited uniform\nweightage alongside other inputs, implying that they were not predominant\nfactors driving model predictions. This revelation prompted a reevaluation of\nour approach, signaling the need for alternative methodologies to enhance model\naccuracy and efficacy.\n[PAGE BREAK]\n2.5.4.3. Linear Regression Inourpursuitofmodelingtemporaldata,Linear\nRegression emerged as a candidate worthy of exploration. Employing various\nmethodologies,including Support Vector Machines(SVM)and Huber Regression,\nwe sought to evaluate their predictive accuracy.\nInitially, we assessed the performance of these models on a merged dataset,\nwhere all ticker symbols were consolidated. The Mean Squared Error (MSE) for\nLinear Regression was 0.713, for Huber Regression it was 0.686, and for SVM it\nwas notably higher at 78.24. These results provided an initial glimpse into the\nmodels' efficacy in handling the combined dataset.\nSubsequently, we conducted a more granular analysis by splitting the dataset\nbased on individual ticker symbols. For instance, for the AAPL_data.csv, the\nMSE for Linear Regression was 2.373, for Huber Regression it was 2.371, and\nfor SVM it was 25.05. Despite achieving high accuracies ranging from 88.06% to\n98.50%, it became evident that the models' performance varied across different\ndatasets.\n[PAGE BREAK]\nFurtheranalysisrevealedthedominanceofthe'Open'pricefeatureininfluencing\npredictions, indicating a misalignment with our goal of leveraging sentiment\nscores for price prediction.\nAttempts to de-emphasize the 'Open' price and prioritize sentiment scores led to\na drastic increase in MSE and a decline in accuracy. For instance, excluding the\n'Open' price feature resulted in significantly higher MSE values (Linear Regres-\nsion: 163.64, Huber Regression: 162.11, SVM: 158.94) and reduced accuracies\n(Linear Regression: 50.75%, Huber Regression: 47.76%, SVM: 52.24%).\n[PAGE BREAK]\nSplit Dataset\nMerged Dataset Split Dataset without ‘Open’\nModel (MSE) (MSE) (MSE)\nLinear Regression 0.713 2.373 163.64\nHuber 0.686 2.371 162.11\nSVM 78.24 25.05 158.94\nFrom the above table, the findings underscore the challenge of reconciling model\nobjectives with feature importance. While Linear Regression demonstrated high\naccuracy, its reliance on 'Open' price undermined the incorporation of sentiment\nscores. This highlights the need for tailored approaches that strike a balance\nbetween feature relevance and predictive performance, paving the way for more\nnuanced modeling strategies.\n2.5.4.4. Random Forest Thechoiceofutilizing Random Forestasourmodel\nwas inspired by a reference project on a Git Hub repository by Anubhav Anand\non his stock market prediction using sentiment analysis project. In his project,\nby utilizing Random Forest, he successfully achieve the accuracy of 91.96% on\nUnited Airlines stock.\nWe utilized Apple stock for this experimentation. Firstly, we ran the base model\nof random forest taking in account weighted_sentiment_score, Open, Close,\nLow, High, news_sentiment, industry_sentiment as features and the output will\nbe an indicator of 1 or 0. An indicator of 1 will signal that the closing price is\ngreater than the opening price, and an indicator of 0 will signal otherwise.\n[PAGE BREAK]\nOur test result for the base random forest model is as shown:\nAs well as the confusion matrix:\nWealsoexperimentedwiththetrainedrandomforestmodelontheoriginaltrain\ndataset. The results is as follows:\nAs well as the confusion matrix:\n[PAGE BREAK]\nAnalyzing the results of how well the model performed with the train data and\ntest data, we hypothesize that the model is overfitting on the train data. The\nhuge difference in accuracy of train data (98.60%) and test data (59.72%) shows\nthat the model does not perform well with unseen data.\nFollowing this observation, we looked further into the parameters of the Random\nForest model. We found an article that talks about hyperparameter tuning\nwith Random Forest and attempted to follow its approach in finding the best\nparameters for our model.\nWe will be trying to adjust the following hyperparameters:\n• N_estimators - This parameter allow us to specify the number\nof trees in the Random Forest\n• Max_depth - Controlling the maximum depth of each tree in\nthe Forest\n• Min_samples_split-Specifyingtheminimumnumberofsamples\nrequired to split an internal node.\n• Max_features - Determines the number of features to consider.\n• Min_samples_leaf - Setting the minimum number of samples\nrequired to be at a leaf node.\n• Bootstrap-Determinewhetherbootstrapsamplesareusedwhen\nconstructing trees.\nUsing a Grid Search CV, we create a parameter grid and perform 5-fold cross\nvalidation on each combination of the parameters we defined to search through.\nThe 5-fold cross validation will give us the most extensive evaluation of the\nperformance of each combination of parameters. It will split the data to 5 sets,\n[PAGE BREAK]\nand it will fit the model 5 times and take a different set as a test set to evaluate\nthe performance for each 5 times.\nFollowing the search, we found the best performing parameters:\nBest Hyperparameters:\n{’bootstrap’ : True ,\n‘max_depth’ : None ,\n‘max_features’ : ‘sqrt’ ,\n‘min_sample_leaf’ : 1 ,\n‘min_sample_split’: 2 ,\n‘n_estimators’: 100}\nEvaluating the performance of the model under these hyperparameters, we can\nsee an improvement of robustness of the model.\nPerformance of best hyperparameter model on train dataset:\nAs well as its confusion matrix:\nPerformance of best hyperparameter model on test dataset:\n[PAGE BREAK]\nAs well as its confusion matrix:\nWeseeasignificantimprovementintherobustnessofthemodelafterperforming\nhyperparameter tuning for our Random Forest Model. With the a smaller\ndifference between the accuracy of test and train data as compared with the\nbase model without hyperparameter tuning. Furthermore, we see a significant\nincrease in the performance of the model from 59.72% to 90.28%.\nNext we evaluate the feature importance of the model:\n[PAGE BREAK]\nAs seen in the chart above, we gladly see the weighted_sentiment_score has the\nbiggest importance, which supports our initial hypothesis that twitter sentiment\ndoes play a part in the stock market.\n3. Results and Discussion\nIn this paper, we experimented with multiple machine learning models and\nour 2 highest performing models are Linear Regression and Random Forest.\nBut looking at the feature importance chart of the Linear Regression model,\ntwitter sentiment does not play a big part when predicting the stock movement,\nwhereas for the Random Forest model, we see a significant improvement in both\nrobustness and performance after hyperparameter tuning. And we can see that\ntwitter sentiment does play a part in predicting stock movement in the Random\nForest model.\nAcknowledging a limitation that we have in our dataset is there is an imbalance\nin the 1 s and 0 s as we can see in the bar chart below for Apple Stock:\n[PAGE BREAK]\nAlthough our best performing Random Forest model has the potential of pre-\ndicting stock prices with a relatively high accuracy, this limitation would hugely\ninfluence the credibility of our model.\nWe would like to acknowledge that there are plenty of approaches when tackling\nthisproblemandourapproachmaynotbethebestway. Tofurtherevaluateand\ncreate a model that is able to predict stock market movement with a credible\naccuracy,wewouldhavetoexperimentwithmultipleotherapproachesintackling\nthis problem as well as other machine learning models which we did not manage\nto explore given our time constraint.\nBut in this paper, we can conclude that twitter sentiment does play a part in\npredicting stock market movement as we can observe in the feature importance\nchart.\nReferences\nMehtab,S.,Sen,J.(2019). ARobust Predictive Modelfor Stock Price Prediction\nUsing Deep Learning and Natural Language Processing. ar Xiv:1912.07700 [q-\nfin.ST].\nhttps://doi.org/10.48550/ar Xiv.1912.07700\nPuh, K., & Bagić Babac, M. (2023). Predicting stock market using natural\nlanguage processing. American Journal of Business, 38(2), 41-61.\nhttps://doi.org/10.1108/AJB-08-2022-0124\nSonkiya, P., et al. (2021). Stock price prediction using BERT and GAN.\nar Xiv:2107.09055 [q-fin.ST].\nhttps://doi.org/10.48550/ar Xiv.2107.09055\n[PAGE BREAK]\n(PDF) machine learning: A review on Binary Classification. (n.d.).\nhttps://www.researchgate.net/publication/313779520_Machine_Learning_A_\nReview_on_Binary_Classification\nSabaren,L.N.,Mascheroni,M.A.,Greiner,C.L.,&Irrazábal,E.(1970,January\n1). A systematic literature review in cross-browser testing. Journal of Computer\nScience and Technology.\nhttp://www.redalyc.org/journal/6380/638067784001/html/\nText mining in medicine – knowledge discovery and intelligent systems – KDIS\n– University of Córdoba. Knowledge Discovery and Intelligent Systems KDIS\nUniversity of Crdoba. (n.d.).\nhttp://www.uco.es/kdis/textminingmedicine/\nForecasting: Principles and practice (3 rd ed). OTexts. (n.d.).\nhttps://otexts.com/fpp3/\nFan, C., Chen, M., Wang, X., Wang, J., & Huang, B. (2021, February 15). A\nreview on data preprocessing techniques toward efficient and reliable knowledge\ndiscovery from building operational data. Frontiers.\nhttps://www.frontiersin.org/articles/10.3389/fenrg.2021.652801/full\nhttps://github.com/anubhavanand12 qw/\nSTOCK-PRICE-PREDICTION-USING-TWITTER-SENTIMENT-ANALYSIS\nKoehrsen, W. (2018, January 10). Hyperparameter tuning the random forest in\npython. Medium.\nhttps://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-\npython-using-scikit-learn-28 d2 aa77 dd74#:~:text=In%20 the%20 case%20 of%20 a\n,each%20 node%20 learned%20 during%20 training",
    "sections": [
      {
        "title": "Singapore University of Technology and Design",
        "content": "50.038: Computational Data Science\nProf. Dorien Herremans, Prof. Soujanya Poria\nApril 8, 2023\n"
      },
      {
        "title": "2. Methodology 3",
        "content": "2.1. Workflow 3\n2.2. Dataset 5\n2.2.1. Dataset Collection 5\n2.2.2. Dataset Manipulation 5\n2.3. Sentiment Analysis 6\n2.3.1. Fine-tuning 6\n2.2.2. Sentiment Aggregation 7\n2.2.2.1. Tweets 7\n2.2.2.2. News Headlines 7\n2.2.2.3. Integrated Dataset for Predicting Stock Price Movements 8\n2.4. Data Pre-processing 8\n2.4.1. Motivation 8\n2.4.2. Methodologies 8\n2.4.2.1. Data cleaning 8\n2.4.2.2. Data transformation 9\n2.4.2.3. Data scaling 9\n2.5. Model 9\n2.5.1. Objective 10\n2.5.2. Evaluation Methodology 10\n2.5.3. Naive Model 11\n2.5.3.1. Framework 11\n2.5.3.2. Implementation 11\n2.5.4. Models Tested 13\n2.5.4.1. FBProphet 13\n2.5.4.2. Recurrent Neural Networks (RNNs) 13\n2.5.4.3. Linear Regression 14\n2.5.4.4. Random Forest 17\n"
      },
      {
        "title": "3. Results and Discussion 21",
        "content": "References 23\n"
      },
      {
        "title": "1. Introduction",
        "content": "Accurately predicting stock prices’ rise and fall has long been a dream of traders\nand researchers alike. The instantaneous nature of Twitter means that people's\nsentiments may be felt directly on stock prices, which created a new prediction\nlayer. Elon Musk's announcement of Tesla's privatization and Kylie Jenner's\ncomment on Snapchat both had dramatically impacted the stock prices of the\ntwocompanies. Inrecentyears,withtheadvancementin NLP,suchas Fin BERT,\npeople have tried to incorporate public and/or professional sentiments into stock\nprice prediction.\nForinstance,Mehtab&Sen(2019)attemptedtopredictthemovementof NIFTY\n50 index 1 week into the future based on Twitter sentiments using an LSTM\nmodel, while Sonkiya et al (2021) used GAN and BERT to predict the price of\n"
      },
      {
        "title": "Apple Inc. (ticker symbol: AAPL) 5, 15, and 30 days into the future. Besides",
        "content": "Twitter, which reflects the views of the public and influencers, some such as Puh\n& Babac (2023) also used professional views from the Wall Street Journal to\npredict the stock market.\nHowever, we found that existing literature has largely focused on the movement\nof stock prices multiple days into the future, which is at odds with the common\ntradingpracticeofdaytradingatfinancialinstitutions. Hence,ourprojectwould\nlike to focus on developing a model to predict the movement of stocks within\nthe trading day based on public sentiments i.e., tweets and market information.\n"
      },
      {
        "title": "2. Methodology",
        "content": "2.1. Workflow\nTo explore the impact of sentiment analysis on stock price prediction, we devel-\noped two workflows: a baseline version and an enhanced version, which includes\nadditional data.\nAsshowninthefigurebelow,thebaselineworkflowhasthreemaincomponents: a\ndatasetcomposedoftweets,sentimentanalysisofthesetweets,andaclassification\nmodel to predict stock price movements.\n"
      },
      {
        "title": "Content",
        "content": "To broaden our exploration of stock indicators, we've expanded our dataset\nto include news articles and fine-tuned the Financial BERT (Fin BERT) Model.\nThese improvements aim to increase the accuracy of our sentiment analysis\nwithin the updated workflow, as displayed in the diagram below.\n2.2. Dataset\n2.2.1. Dataset Collection\nAs outlined in the workflow diagram presented in the previous section, we utilize\nfive distinct sub-datasets. Each dataset has a specific source and purpose as\ndetailed below.\n"
      },
      {
        "title": "Content",
        "content": "1. stock_market_tweets\nSource: \"mjw/stock_market_tweets\" from Hugging Face\nDescription: This dataset consists of tweets specifically related\nto stocks, including text content and additional details such as\nthe number of retweets, likes, and comments.\n2. stock_information\nSource: Scraped from Nasdaq\nDescription: Information about stocks including details about\ntheir country, sector, and industry.\n3. news\nSource: \"ashraq/financial-news\" from Hugging Face\nDescription: A collection of financial news headlines.\n4. twitter_financial_news_sentiment\nSource: \"zeroshot/twitter-financial-news-sentiment\" from Hug-\nging Face\nDescription: An English corpus of finance-related tweets anno-\ntated for sentiment, used to fine-tune the Fin Bert model.\n"
      },
      {
        "title": "5. Yahoo_Finance_OHLC_stock",
        "content": "Source: Extracted using Yahoo Python API\nDescription: Stock price information, specifically Open, High,\nLow, and Close (OHLC) data.\n2.2.2. Dataset Manipulation\nThedatasets'stock_market_tweets'and'stock_information'aremergedthrough\nan inner join operation on the stock symbol.\nIn the case of the 'news' dataset, which lacks a specific column identifying\nthe relevant stock, we first compile a list of distinct company names from\n'stock_information'. We then attempt to extract patterns of these company\nnames from the news headlines.\nAfterpredictingandaggregatingsentimentscores,thedatasets'stock_market_tweets',\n'news', and 'Yahoo_Finance_OHLC_stock' are merged together based on\nthe stock name and date information. Further details about the sentiment\naggregation process will be discussed in the following section.\n2.3. Sentiment Analysis\nWe selected the pretrained Fin BERT model, 'ahmedrachid/Financial BERT-\nSentiment-Analysis,' to serve both as the tokenizer and the prediction model.\nThistransformer-based modeloutperforms traditional BERT models inhandling\nfinancial texts. Optimized specifically for financial contexts, the sentiment labels\naredesignatedasfollows: thepositivelabelsuggestsa'buy'action,zeroindicates\n'do nothing,' and the negative label advises to 'sell'.\n"
      },
      {
        "title": "Content",
        "content": "2.3.1. Fine-tuning\nIn the baseline architecture, the model was not fine-tuned. However, for the\nenhanced version, we fine-tuned the model using 11,931 labeled financial tweets\nfrom the 'twitter_financial_news_sentiment' dataset. The training parameters\nemployed were as follows:\nlearning_rate=2 e-5,\nper_device_train_batch_size=16,\nper_device_eval_batch_size=16,\nnum_train_epochs=20,\nweight_decay=0.01,\nevaluation_strategy=\"epoch\",\nsave_strategy=\"epoch\",\nmetric_for_best_model='accuracy'\nIn addition, we quantified the effects of preprocessing steps, which included\nremoving repeated punctuations, cleaning hashtags, and replacing URLs with\nthe '[URL]' token. As shown in the table, the sentiment prediction accuracy on\nthe test dataset remains relatively consistent, both before and after fine-tuning.\nThis indicates the tokenizer's effectiveness in handling messy text.\n"
      },
      {
        "title": "Without Preprocessing 0.7462311557788944 0.8408710217755444",
        "content": "2.2.2. Sentiment Aggregation\n2.2.2.1. Tweets Given the large volume of tweets related to each stock on\nany given day, it becomes necessary to aggregate the respective sentiment scores\nto obtain a coherent overall sentiment. Furthermore, to gauge the social impact\nof specific tweets, we also consider engagement metrics to measure the public\ninfluence exerted by each tweet.\nThe sentiment score, however, originally a categorical variable with three labels\n(0 for negative, 1 for neutral, and 2 for positive), is not directly suitable for\nmultiplication with quantitative metrics like retweet counts, like counts, or\ncomment counts, as this operation lacks intuitive sense with categorical labels.\nToaddressthis,weconvertthesentimentscoreintoanumericalvaluebyassigning\nspecific weights to each category, allowing for the calculation of a weighted\nsentiment score. This method involves mapping the sentiment categories to\nnumerical values:\n"
      },
      {
        "title": "Content",
        "content": "sentiment_weights = {0: -1, 1: 0, 2: 1}\n• 0 represents negative sentiment (assigned a weight of -1)\n• 1 represents neutral sentiment (assigned a weight of 0)\n• 2 represents positive sentiment (assigned a weight of 1)\nThe formula for weighted average score calculation:\nThis formula calculates the weighted average of sentiment scores, taking into\naccount the weights provided by the retweet counts, like counts, and comment\ncounts for each tweet.\n2.2.2.2. News Headlines Fornewsitems, wecalculatetheaveragesentiment\nscoreofallheadlinesrelatedtoaspecificstockonthesameday. Additionally,we\ninclude an average sentiment score for news related to the stock's industry, thus\nincorporating an industry sentiment indicator. This dual-layered approach helps\nprovide a broader perspective on the sentiment influencing both the specific\nstock and its wider industry.\n2.2.2.3. Integrated Dataset for Predicting Stock Price Movements\nThe aggregated sentiment scores from tweets and news are combined with the\nOHLC (Open, High, Low, Close) pricing data based on stock name and tickers.\nThe resulting dataset, prepared for stock price prediction, includes the following\ncolumns:\n• post_date\n• ticker_symbol\n• Country\n• IPO Year\n• Sector\n• Industry\n• Open\n• High\n• Low\n• Close\n• Adj Close\n• Volume\n• Shifted_Open\n"
      },
      {
        "title": "Content",
        "content": "• Shifted_Adj Close\n• Shifted_Close\n• prev_day_Open\n• prev_day_Open_Adj Close\n• prev_day_Close\n• news_sentiment\n• industry_sentiment\n• weighted_tweet_sentiment\n2.4. Data Pre-processing\n2.4.1. Motivation\nThe data preprocessing consists of five major tasks, i.e., data cleaning, reduction,\nscaling, transformation and partitioning (Xiao and Fan, 2014; Fan et al., 2015 a;\nFan et al., 2015 b)\n. These techniques effectively address inconsistencies, redundancies, and irrele-\nvant information within the data, ultimately enhancing data quality and model\naccuracy (Gandomi and Haider, 2015)\n2.4.2. Methodologies\nThe techniques we used to preprocess the dataset, ensures its compatibility with\nvarious machine learning models. Through a series of strategies, the raw data\nunderwent preprocessing to enhance its usability and effectiveness in subsequent\nmodeling tasks.\n2.4.2.1. Data cleaning We clean the data to handle missing data effec-\ntively. Initially, missing values were addressed using the forward fill method\n(method='ffill') to propagate non-null values forward along the specified axis.\nSubsequently, rows with missing values in critical columns such as 'Open',\n'High', 'Low', 'Close', and 'Adj Close' were removed using the ‘dropna’ method.\nThis ensured that the dataset was cleansed of incomplete or erroneous records,\nfacilitating robust analysis and a viable dataset for training the models.\nInaddition,arollingaccumulationofsentimentscoreswasperformedtoaugment\nthe dataset's richness. This process involved carrying forward and averaging\nsentiment scores in rows where price columns contained missing values. By\naligning sentiment scores with price data through this rolling accumulation, the\ndataset's temporal coherence was maintained, facilitating a more comprehensive\nanalysis of market sentiment trends.\n"
      },
      {
        "title": "Content",
        "content": "2.4.2.2. Data transformation The dataset was restructured into numerical\nandcategoricaltoensurethatdifferenttypesoffeaturesareappropriatelyhandled\nduring preprocessing. Categorical features : ['ticker_symbol', 'Country', 'IPO\nYear', 'Sector', 'Industry'], within the dataset were transformed into a numerical\nrepresentation to make them compatible with machine learning algorithms. This\ntransformation was achieved through label encoding, wherein each categorical\nfeature's values were converted into numerical labels using the Label Encoder\nmodule from the scikit-learn library. By encoding categorical variables into\nnumerical form, the dataset was prepared for subsequent analysis and model\ntraining.\nA new binary target variable, denoted as 'y', was generated to facilitate binary\nclassification. This variable was derived from the 'Close' and 'Open' price\ncolumns, where 'y' was assigned a value of 1 if the 'Close' price was greater\nthan the 'Open' price, indicating a positive price change. Conversely, 'y' was\nassignedavalueof0 ifthe'Close'pricewaslessthanorequaltothe'Open'price,\nindicating a negative or neutral price change. This labeling process enhanced\nthe dataset's suitability for binary classification tasks.\n2.4.2.3. Data scaling To ensure consistency in feature scales and facilitate\nconvergence during model training, numerical features underwent scaling. The\nMin-Max scaling technique was employed, which rescales each feature to a\nspecified range (typically 0 to 1) using the Min Max Scaler module from scikit-\nlearn. By scaling numerical features, the dataset's features were normalized,\npreventing features with larger scales from dominating the learning process.\n2.5. Model\n2.5.1. Objective\nWe follow a typical day trading scenario, in which all positions are closed before\nthe trading day to minimize risks and price change after the market closes. The\ndecision to long, in anticipation of an increase in the stock price, or short, in\nanticipation of an decrease, is made at the opening of a day’s market, with the\nstock being sold or bought at the closing to close the position.\nTherefore, our algorithm attempts at answering the classification problem: will\nthestock’sclosingpricebehigherthantheopeningprice,givenhistoricalmarket\n"
      },
      {
        "title": "Content",
        "content": "data and recent tweets & news about the company and/or industry?\n2.5.2. Evaluation Methodology\nIn our evaluation methodology, we will utilize different metrics depending on\nthe target variable type. For continuous, we employ Mean Squared Error\n(MSE) (Hyndman & Athanasopoulos, 2014). MSE measures the average squared\ndifferencebetweenpredictedandactualvalues,withlower MSEindicatingbetter\nmodel performance for continuous prediction tasks.\nFor discrete (classification tasks), we use a combination of metrics. Firstly , we\nlook towards accuracy measurement, the ratio of correctly classified instances\n(Japkowicz & Stephen, 2016), as a common starting point. We define correctly\nclassifiedinstancesasactuallabeledyinputstobethesameaspredictedyinputs.\nAdditionally, we will employ Binary Cross-Entropy (BCE) loss (Kingma & Ba,\n2017). BCE loss measures the difference between predicted probabilities and\nactual class labels, and it's often used during model training as a loss function\nto minimize classification error.\nWewillfurtherexploreclassifierscores(precision,recall,F1-score)forimbalanced\nclasses (Sokolova et al., 2006) and consider confusion matrices (Powers, 2020) to\ngain deeper insights into our random forest model's performance.\n2.5.3. Naive Model\n2.5.3.1. Framework Twonaivemodelswereusedtohelpusgetatasteofthe\nproblem. Forthefirstnaivemodel,weframedthequestionasapureclassification\nproblem, combining the NLP result (the stock’s aggregated sentiment score) and\nthe market information (exogenous information e.g., sector) to predict whether\nthe closing price will be higher than the opening price.\nWe also tried to first frame the question as a regression problem, where we use\nthe NLP result, market information, and the opening price to predict the exact\nexpected closing price of the stock. The predicted closing price will then be\ncompared with the opening price to answer whether the price is expected to\ngo up during the day. We believe by framing the problem in two ways, we will\nbe able to test both classification and regression algorithms and make a more\n"
      },
      {
        "title": "Content",
        "content": "informed decision on how to approach the problem for subsequent modeling. A\nsimple illustration of the logic flow of our naive model is shown below.\n2.5.3.2. Implementation We used Py Caret as our naive model implementa-\ntion, which trains and compares across different machine learning models, and\nselects the one with the best accuracy after 10-fold validation.\nFor the classification approach, the highest accuracy of 57.01% was obtained\nby an Ada Boost Classifier model. In comparison, the dummy model returned a\n51.25% accuracy. The confusion matrix and the top 3 performing models are\ngiven below.\n"
      },
      {
        "title": "Content",
        "content": "For the regression approach, the highest accuracy obtained was slightly worse at\n55.79%. However, we were able to confirm the importance of twitter sentiment\non stock price movement. As shown in the feature importance plot below, the\nvariable weighted_sentiment_score is the second most important feature among\nall, only falling behind the opening price but vastly exceeds the importance of\nother exogenous variables like the sector and the companies’ IPO year. This\nconfirms the role twitter sentiment plays in dictating stock price movement and\nprompted us to fine-tune the sentiment evaluation and improve our model.\n2.5.4. Models Tested\n2.5.4.1. FBProphet FBProphet is a time series forecasting tool by Facebook\n"
      },
      {
        "title": "Open Source. Besidesthetimeseriesofclosingpriceitself,FBProphetalsoallows",
        "content": "us to use other non-temporal regressors to perform a multivariate prediction. In\nourmodel,wetriedtopredictthenextday’sclosingprice,basedonthehistorical\ntime series of: the closing price, the trading day’s opening price, stock-specific\ntwittersentiment, newssentiment, andindustrysentiment. Wealsoacknowledge\nthat there may be a “delay” in a tweet’s impact on the stock price, for instance\nwhen the tweet was posted between the closure of the previous day’s market\nand the opening of the present day’s market. Hence, for all sentiment-related\nvariables, we included both sentiment scores on the day itself and the previous\nday. As an example, for Twitter sentiment, we use the following two variables as\nour regressors:\n• weighted_sentiment_score: Tweets’ weighted aggregate senti-\nment on the trading day\n• weighted_sentiment_score_-1: Tweets’ weighted aggregate sen-\n"
      },
      {
        "title": "Content",
        "content": "timent from the previous trading day.\nTheresultswereobtainediteratively,forwhichthe FBProphetmodelisretrained\neveryday,withnewinformationadded. Toensuresufficienttrainingdatainitially,\nwe start training the time series model on the 366 th trading day, which is equal\nto around 30% of all data. However, we did not witness a significant accuracy\nincrease for later days as we accumulate a longer time series. We were able to\nobtain an overall testing accuracy of around 58.61% from our FBProphet model.\n2.5.4.2. Recurrent Neural Networks (RNNs) Our dataset inherently\nembodies a time series prediction framework, characterized by timestamps for\neach entry. In light of this temporal structure, Recurrent Neural Networks\n(RNNs) were selected for their capability to leverage sequential data. Various\nRNN architectures, including Gated Recurrent Units (GRU) with attention\nmechanismsand Long Short-Term Memory(LSTM)networks,wereimplemented\nto capture the intricate patterns and features embedded within the dataset,\nultimately aiming to predict asset prices.\nOur initial approach involved amalgamating data from all ticker symbols into\na unified dataset. This merged dataset facilitated the collective analysis of\ncategorical and numerical features. The training and testing datasets were then\npartitioned, with the training set sequenced to preserve temporal dependencies.\nEvaluation of the model on the test data revealed an accuracy of 55.68%.\nSubsequently, we pursued a more nuanced analysis by segregating the dataset\nbased on individual ticker symbols. This approach unveiled distinct performance\nvariationsacrossdifferentdatasets. Notably,themodelexhibitedhigheraccuracy\n(61.62%) for the Microsoft (MSFT) ticker symbol, while yielding lower accuracy\n(51.14%) for Tesla (TSLA). These discrepancies underscored the model's propen-\nsity to excel in specific contexts, hinting at potential limitations in its robustness\nand adaptability.\nTo further elucidate the underlying dynamics, we explored alternative RNN\narchitectures, including GRU and attention models. Despite variations in model\ncomplexity, the results remained largely consistent. This convergence suggested\nthat performance trends were less influenced by architectural nuances and more\nby shared factors such as hyperparameters, initializations, and loss functions.\nOur emphasis remained on understanding the fundamental characteristics of the\nbase model, with minimal fine-tuning deemed necessary.\nIncorporating sentiment accumulation strategies into the dataset yielded promis-\ning results, with each model experiencing a modest increase in accuracy ranging\nfrom 1% to 2%. However, a compelling discovery emerged during feature impor-\ntance analysis. Contrary to expectations, sentiment scores exhibited uniform\nweightage alongside other inputs, implying that they were not predominant\nfactors driving model predictions. This revelation prompted a reevaluation of\nour approach, signaling the need for alternative methodologies to enhance model\naccuracy and efficacy.\n"
      },
      {
        "title": "Content",
        "content": "2.5.4.3. Linear Regression Inourpursuitofmodelingtemporaldata,Linear\nRegression emerged as a candidate worthy of exploration. Employing various\nmethodologies,including Support Vector Machines(SVM)and Huber Regression,\nwe sought to evaluate their predictive accuracy.\nInitially, we assessed the performance of these models on a merged dataset,\nwhere all ticker symbols were consolidated. The Mean Squared Error (MSE) for\n"
      },
      {
        "title": "Linear Regression was 0.713, for Huber Regression it was 0.686, and for SVM it",
        "content": "was notably higher at 78.24. These results provided an initial glimpse into the\nmodels' efficacy in handling the combined dataset.\nSubsequently, we conducted a more granular analysis by splitting the dataset\nbased on individual ticker symbols. For instance, for the AAPL_data.csv, the\nMSE for Linear Regression was 2.373, for Huber Regression it was 2.371, and\nfor SVM it was 25.05. Despite achieving high accuracies ranging from 88.06% to\n98.50%, it became evident that the models' performance varied across different\ndatasets.\n"
      },
      {
        "title": "Content",
        "content": "Furtheranalysisrevealedthedominanceofthe'Open'pricefeatureininfluencing\npredictions, indicating a misalignment with our goal of leveraging sentiment\nscores for price prediction.\nAttempts to de-emphasize the 'Open' price and prioritize sentiment scores led to\na drastic increase in MSE and a decline in accuracy. For instance, excluding the\n'Open' price feature resulted in significantly higher MSE values (Linear Regres-\nsion: 163.64, Huber Regression: 162.11, SVM: 158.94) and reduced accuracies\n(Linear Regression: 50.75%, Huber Regression: 47.76%, SVM: 52.24%).\n"
      },
      {
        "title": "Merged Dataset Split Dataset without ‘Open’",
        "content": "Model (MSE) (MSE) (MSE)\n"
      },
      {
        "title": "Linear Regression 0.713 2.373 163.64",
        "content": "Huber 0.686 2.371 162.11\nSVM 78.24 25.05 158.94\nFrom the above table, the findings underscore the challenge of reconciling model\nobjectives with feature importance. While Linear Regression demonstrated high\naccuracy, its reliance on 'Open' price undermined the incorporation of sentiment\nscores. This highlights the need for tailored approaches that strike a balance\nbetween feature relevance and predictive performance, paving the way for more\nnuanced modeling strategies.\n2.5.4.4. Random Forest Thechoiceofutilizing Random Forestasourmodel\nwas inspired by a reference project on a Git Hub repository by Anubhav Anand\non his stock market prediction using sentiment analysis project. In his project,\nby utilizing Random Forest, he successfully achieve the accuracy of 91.96% on\n"
      },
      {
        "title": "United Airlines stock.",
        "content": "We utilized Apple stock for this experimentation. Firstly, we ran the base model\nof random forest taking in account weighted_sentiment_score, Open, Close,\nLow, High, news_sentiment, industry_sentiment as features and the output will\nbe an indicator of 1 or 0. An indicator of 1 will signal that the closing price is\ngreater than the opening price, and an indicator of 0 will signal otherwise.\n"
      },
      {
        "title": "Content",
        "content": "Our test result for the base random forest model is as shown:\nAs well as the confusion matrix:\nWealsoexperimentedwiththetrainedrandomforestmodelontheoriginaltrain\ndataset. The results is as follows:\nAs well as the confusion matrix:\n"
      },
      {
        "title": "Content",
        "content": "Analyzing the results of how well the model performed with the train data and\ntest data, we hypothesize that the model is overfitting on the train data. The\nhuge difference in accuracy of train data (98.60%) and test data (59.72%) shows\nthat the model does not perform well with unseen data.\nFollowing this observation, we looked further into the parameters of the Random\nForest model. We found an article that talks about hyperparameter tuning\nwith Random Forest and attempted to follow its approach in finding the best\nparameters for our model.\nWe will be trying to adjust the following hyperparameters:\n• N_estimators - This parameter allow us to specify the number\nof trees in the Random Forest\n• Max_depth - Controlling the maximum depth of each tree in\nthe Forest\n• Min_samples_split-Specifyingtheminimumnumberofsamples\nrequired to split an internal node.\n• Max_features - Determines the number of features to consider.\n• Min_samples_leaf - Setting the minimum number of samples\nrequired to be at a leaf node.\n• Bootstrap-Determinewhetherbootstrapsamplesareusedwhen\nconstructing trees.\nUsing a Grid Search CV, we create a parameter grid and perform 5-fold cross\nvalidation on each combination of the parameters we defined to search through.\nThe 5-fold cross validation will give us the most extensive evaluation of the\nperformance of each combination of parameters. It will split the data to 5 sets,\n"
      },
      {
        "title": "Content",
        "content": "and it will fit the model 5 times and take a different set as a test set to evaluate\nthe performance for each 5 times.\nFollowing the search, we found the best performing parameters:\n"
      },
      {
        "title": "Best Hyperparameters:",
        "content": "{’bootstrap’ : True ,\n‘max_depth’ : None ,\n‘max_features’ : ‘sqrt’ ,\n‘min_sample_leaf’ : 1 ,\n‘min_sample_split’: 2 ,\n‘n_estimators’: 100}\nEvaluating the performance of the model under these hyperparameters, we can\nsee an improvement of robustness of the model.\nPerformance of best hyperparameter model on train dataset:\nAs well as its confusion matrix:\nPerformance of best hyperparameter model on test dataset:\n"
      },
      {
        "title": "Content",
        "content": "As well as its confusion matrix:\nWeseeasignificantimprovementintherobustnessofthemodelafterperforming\nhyperparameter tuning for our Random Forest Model. With the a smaller\ndifference between the accuracy of test and train data as compared with the\nbase model without hyperparameter tuning. Furthermore, we see a significant\nincrease in the performance of the model from 59.72% to 90.28%.\nNext we evaluate the feature importance of the model:\n"
      },
      {
        "title": "Content",
        "content": "As seen in the chart above, we gladly see the weighted_sentiment_score has the\nbiggest importance, which supports our initial hypothesis that twitter sentiment\ndoes play a part in the stock market.\n"
      },
      {
        "title": "3. Results and Discussion",
        "content": "In this paper, we experimented with multiple machine learning models and\nour 2 highest performing models are Linear Regression and Random Forest.\nBut looking at the feature importance chart of the Linear Regression model,\ntwitter sentiment does not play a big part when predicting the stock movement,\nwhereas for the Random Forest model, we see a significant improvement in both\nrobustness and performance after hyperparameter tuning. And we can see that\ntwitter sentiment does play a part in predicting stock movement in the Random\nForest model.\nAcknowledging a limitation that we have in our dataset is there is an imbalance\nin the 1 s and 0 s as we can see in the bar chart below for Apple Stock:\n"
      },
      {
        "title": "Content",
        "content": "Although our best performing Random Forest model has the potential of pre-\ndicting stock prices with a relatively high accuracy, this limitation would hugely\ninfluence the credibility of our model.\nWe would like to acknowledge that there are plenty of approaches when tackling\nthisproblemandourapproachmaynotbethebestway. Tofurtherevaluateand\ncreate a model that is able to predict stock market movement with a credible\naccuracy,wewouldhavetoexperimentwithmultipleotherapproachesintackling\nthis problem as well as other machine learning models which we did not manage\nto explore given our time constraint.\nBut in this paper, we can conclude that twitter sentiment does play a part in\npredicting stock market movement as we can observe in the feature importance\nchart.\nReferences\nMehtab,S.,Sen,J.(2019). ARobust Predictive Modelfor Stock Price Prediction\n"
      },
      {
        "title": "Using Deep Learning and Natural Language Processing. ar Xiv:1912.07700 [q-",
        "content": "fin.ST].\nhttps://doi.org/10.48550/ar Xiv.1912.07700\nPuh, K., & Bagić Babac, M. (2023). Predicting stock market using natural\nlanguage processing. American Journal of Business, 38(2), 41-61.\nhttps://doi.org/10.1108/AJB-08-2022-0124\nSonkiya, P., et al. (2021). Stock price prediction using BERT and GAN.\nar Xiv:2107.09055 [q-fin.ST].\nhttps://doi.org/10.48550/ar Xiv.2107.09055\n"
      },
      {
        "title": "Content",
        "content": "(PDF) machine learning: A review on Binary Classification. (n.d.).\nhttps://www.researchgate.net/publication/313779520_Machine_Learning_A_\nReview_on_Binary_Classification\nSabaren,L.N.,Mascheroni,M.A.,Greiner,C.L.,&Irrazábal,E.(1970,January\n1). A systematic literature review in cross-browser testing. Journal of Computer\nScience and Technology.\nhttp://www.redalyc.org/journal/6380/638067784001/html/\nText mining in medicine – knowledge discovery and intelligent systems – KDIS\n– University of Córdoba. Knowledge Discovery and Intelligent Systems KDIS\nUniversity of Crdoba. (n.d.).\nhttp://www.uco.es/kdis/textminingmedicine/\nForecasting: Principles and practice (3 rd ed). OTexts. (n.d.).\nhttps://otexts.com/fpp3/\nFan, C., Chen, M., Wang, X., Wang, J., & Huang, B. (2021, February 15). A\nreview on data preprocessing techniques toward efficient and reliable knowledge\ndiscovery from building operational data. Frontiers.\nhttps://www.frontiersin.org/articles/10.3389/fenrg.2021.652801/full\nhttps://github.com/anubhavanand12 qw/\nSTOCK-PRICE-PREDICTION-USING-TWITTER-SENTIMENT-ANALYSIS\nKoehrsen, W. (2018, January 10). Hyperparameter tuning the random forest in\npython. Medium.\nhttps://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-\npython-using-scikit-learn-28 d2 aa77 dd74#:~:text=In%20 the%20 case%20 of%20 a\n,each%20 node%20 learned%20 during%20 training\n"
      }
    ],
    "metadata": {
      "title": "Natural Language Processing for Stock",
      "category": "team_document",
      "file_name": "team9",
      "relative_path": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/team9.pdf",
      "page_count": 24,
      "project_name": null,
      "file_size": 1452741,
      "last_modified": 1749024949.5866673
    },
    "word_count": 4009,
    "page_count": 24
  },
  {
    "id": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f_EVAM_rocker",
    "source_file": "/Users/weimingchin/Desktop/weiming_chatbot/data/raw/notion_export/Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/EVAM_rocker.pdf",
    "type": "pdf",
    "title": "Generative Design of Electric Vehicle Additive Manufacturing (EVAM) Suspension Rocker",
    "category": "resume",
    "raw_content": "\n--- Page 1 ---\nGenerative Design of Electric Vehicle Additive Manufacturing (EVAM) Suspension Rocker\nAUTHORS STUDENT ID\nHe Tianli 1006151\nLim Yu Jie 1005999\nAditya Kumar 1006300\nEthan Choo E-Rhen 1006324\nCaitlin Daphne Tan Chiang 1006537\nWei Ming 1006264\nAll team members contributed equally to the task.\nSingapore University of Technology and Design\n60.002: AI Applications in Design\nDr. Kwan Wei Lek, Dr Edwin Koh, MrMichael Alexander Reeves\nFebruary 26, 2023\n\n--- Page 2 ---\n1\nGenerative Design of Electric Vehicle Additive Manufacturing (EVAM) Suspension Rocker\nElectric vehiclesarebecomingamorepopularchoiceinrecentyears,withanincreaseof40%\nin 2022(Powell,2023).Ingeneral,thesevehiclesarecomparativelymoresustainablecomparedtothe\nconventional combustion engine, simply because they do not produce harmful carbon emissions.\nAdditionally, most vehicle charging stations use renewable energy. Similarly, manufacturing has\ncontinually been improving by creating a smaller carbon footprint through using eco-friendly\nmaterials.\nAt the forefront of design and innovation, Singapore University of Technology and Design\n(SUTD) is manufacturing components for their electric vehicle project, known as Electric Vehicle\nAdditive Manufacturing (EVAM). As the name suggests, they employ fabricating methods such as\nSelective Laser Melting (SLM), Fused Deposition Modeling (FDM), Multi Jet Fusion (MJF) and\nStereolithography (SLA).\nIn this project, we will be exploring the use of generative design tools to propose practical\nsolutions that can promote material and energy savings. We will be focusing on the Electric Vehicle\nAdditive Manufacturing (EVAM) suspension rocker.\nMethodology\nFirstly, we had to understand the loadconstraintsoftheEVAMrocker.Forthis,weneededto\nmap out the component’s function within the mechanism of the system, whichhasbeendrawnoutin\nFigure 1.\nFigure 1\nIllustration of the EVAM Rocker Mechanism\n\n--- Page 3 ---\n2\nThe purpose of this mechanism is to stabilize the control of thevehicleandincreasecomfort\nin the vehicle (Universal Technical Institute, 2021). This can be illustrated in the stages of motion\nwhen the vehicle travels over a bump.\nIn the first stage when the wheel first hits thebump,thereisanupwardforceonthewheelof\nthe car. This pushes the suspension rod upwards at a 55 degree angle to the vertical. Through the\nrocker, the force is translated to a downward force onto the coil spring. This first stage is illustrated\nusing the green arrows.\nThe second stage is when the wheel fully travels over the bump. The suspension coil spring\nexerts a force back on the rocker. The rockertranslatestheforcebackontothesuspensionrod,which\npushesthewheelbackintotheground.Thismaximizesthecontactofthetireswiththeground,sothat\ntraction can be kept, and the driver will not lose control of the car. This is illustrated by the red\narrows.\nWithin thesetwostages,therockertranslatesthedirectionsoftheforces.Thisisillustratedby\nthe pink arrows, as the rocker rotates about the axis (pink dotted line).\nLoads and Constraints\nFigure 2\nIllustration of loads and constraints\nNext, we needed to interpret our force analysis from Figure 1 into the generative design\nsoftware that we would be using, which is Autodesk Fusion 360. A generative study was created as\nseen in Appendix A.\nThe generative study requires preserve geometries and obstacle geometries. The preserve\ngeometries are the structures that will hold pins that are attached to the suspension rod, coil spring,\nand main chassis.Meanwhiletheobstaclegeometriesareforthesepinstogothrough.Thecomponent\nis then sandwiched by two more obstacle geometries so that its width does not increase.\n\n--- Page 4 ---\n3\nThe design of mechanical systems often requires the use ofconstraintstoachievethedesired\nmotion or behavior. In the case of the rocker and connector bearing support, a pin constraint and a\nfixed constraint were used according to the specifications provided in the design information. The\nfixed constraint was applied to the largestholeintherockertopreventanymovementwhileallowing\nit to act as a hinge, while the pin constraint was appliedtothebearingsupporttopreventdeformities\nin the radial and axial directions.\nThe pin constraint on the bearing support allowed for movement in response to the force\nbeing applied on the other hole by the pushrods, while preventing undesirable deformities. The\npushrods werefoundtobeexertingaloadof5000Natanangleof55degreesfromthevertical,which\nrequired careful consideration of the constraints to ensure proper function of the system.\nBy using textbook constraints and following the design information, the system was able to\nachieve the desired motion and withstandtheappliedforces.Theproperuseofconstraintsisessential\nin mechanical design to ensure safe and reliable operation of the system.\nMaterials\nIn order to generate design alternatives for evaluation, we decided tovarythematerialofthe\ngenerative design study. We narrowed down the materials down to Aluminium AlSi (10Mg), Cobalt\nChrome, Inconel 718, Stainless Steel 17-4 PH, Titanium 6Al-4V, from the Fusion 360 Additive\nMaterial Library. We chose Inconel 718 and Stainless Steel 17-4 PH from their other variants of\nInconels and Stainless Steels because both materials have better properties in general, in terms of\nyield and tensile strength, density and cost.\nWe are aiming to settle for materials that have the highest yield and tensile strength, lowest\ndensity in order to minimize its weight contribution to the vehicle, and lowest cost for better\nmanufacturing. Although Inconel 718 has lower ultimate tensile strength than its other 2 variants\n(Inconel 625 and Inconel 718 plus), it also has the lowest density and highest yield strength, whilst\nbeing cheaper. The Stainless Steel that we picked (Stainless Steel 17-4 PH)haslowerdensity,higher\nyield, and higher tensile strength as compared to its alternative (Stainless Steel AISI 304) in the\nFusion 360 Additive Material Library (Ansys Granta, 2019, 2). Since wearedesigningarockerfora\nformula 1 themed electric vehicle, although StainlessSteel17-4PHismorecostly,thebenefitsofthe\nmaterial being light and strong exceeded the drawback of the cost of the material.\nFrom there, we performed a Life Cycle Assessment (LCA) on these materials to assess and\nunderstand the environmental impacts in each stage of the material’s life cycle. This is important in\norder to be inline withtheproject’sgoalofsustainabledesign.Inthiscase,wefocusedonthemassof\ncarbon dioxide (CO2) produced during material extraction, component fabrication, and component\nrecycling.ThisisbecausethebenefitofanelectricvehicleisthatitdoesnotreleaseCO2whileinuse.\nTo simplify the analysis, we had to make some assumptions:\n(1) No CO2 is produced in use andduringdistributionofcomponents,asthecomponentswillbe\nmanufactured in-house.\n(2) Data will be collected from Casting CO2, due to lack of information aboutCO2producedin\nadditive manufacturing\n\n--- Page 5 ---\n4\nThe LCA can hence be summarized in Figure 13, with additional LCA charts and graphs in\nAppendix A.\nTable 1\nLife Cycle Assessment of Aluminium, Cobalt Chrome, Inconel, Stainless Steel and Titanium\nMaterial Stage of LCA CO2 per kg (kg/kg) Total CO2 (kg/kg)\nExtraction 42.2\nAluminium AlSi\nManufacturing 1.16 61.06\n(10Mg)\nRecycling 17.7\nExtraction 12.5\nCobalt Chrome Manufacturing 0.909 16.79\nRecycling 3.38\nExtraction 18.3\nInconel 718 Manufacturing 0.996 23.18\nRecycling 3.88\nExtraction 9.58\nStainless Steel 17-4\nManufacturing 0.894 12.49\nPH\nRecycling 2.02\nExtraction 42.2\nTitanium 6Al-4V Manufacturing 1.16 50.53\nRecycling 7.17\nFrom this, wecanconcludethatStainlessSteel17-4PHhastheleastenvironmentalimpactin\nregards to CO2 produced. However, the different materials used in formgenerationwillusedifferent\nvolumesofmaterial.Hence,weneedtofactorinthevolumeofmaterialusedinordertodeterminethe\nactual total CO2 produced.\nWe also researched material cost for our evaluation to optimize the design from a business\nperspective. The cost of the Aluminium A1SI (10Mg), Cobalt Chrome, Inconel 718, Stainless Steel\n17-4 PH, Titanium 6A1-4V, and the original Aluminum 7075-T6 are (in SGD per m3) is\n8.08e3-9.46e3, 3.88e5-5.26e5, 1.47e5-1.69e5, 2.81e4-3.22e4, 1.2e5-1.43e5, and 1.66e4 - 2e4\nrespectively (Ansys Granta, 2019, 2). The product of the prices per unit volume and the volume of\nmaterial used will then be used in our evaluation.\nAdditive Manufacturing vs Die Casting\nWhile additive manufacturing is of the future, we took into consideration die casting aswell\nas it was one of the manufacturing methods offered in Fusion360. Using the default material,\naluminum,wegeneratedtwomodelsusingthesetwofabricationmethods,asseeninAppendixB.The\n\n--- Page 6 ---\n5\nresults show that the volume of material used in die casting is much greater than in additive\nmanufacturing, due to the precision that it offers ascomparedtodiecasting.Thismeansthatadditive\nmanufacturing will produce less CO2 than die casting as well.\nControl\nTo aid in our evaluation, we set the control to be the original design fabricated using\nAluminum 7075-T6, using milling. A simulation was conducted and the results are in Table 2.\nTable 2\nOriginal rocker design specifications\nMin Safety Factor 3.459 Mass of component 1323\n(kg)\nMax Stress (MPa) 59.85 CO2 produced (kg) 23.63\nMax Displacement 0.03264\n(mm)\nStrain 4.843e-04\nVolume of Material 1.686e5\n(mm3)\nResults\nGenerated Designs\nThus,wegenerated10modelswith5differentmaterialsand2differentpresettedsafetyfactor\n. To aid our evaluation, we extracted simulation data regarding the (A) Safety Factor;(B)Stress;(C)\nDisplacement; (D) Strain. Additionally, fabrication costs such as (E) Volume of material, and\ncomponent specifics such as(F)Mass,wereextracted.Figure5to9showstheoutcomesofthemodel\ngeneration, while Appendix C presents the heat maps of the additional data.\nAluminium AlSi (10Mg)\nFigure 3 and 4\nGenerated Structural Component EVAM Rocker using Aluminium AlSi (10Mg)\n*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 4\n\n--- Page 7 ---\n6\nA. Minimum Safety Factor: 1.305 A. Minimum Safety Factor: 3.86\nB. Max Stress: 184 MPa B. Max Stress: 62.18 Mpa\nC. Displacement: 0.13 mm C. Displacement: 0.05767 mm\nD. Strain: 0.003632 D. Strain: 0.001543\nE. Volume of Material: 73.597 cm3 E. Volume of Material:116.959 cm3\nF. Mass of component: 196.504 g F. Mass of component: 312.218 g\nCobalt Chrome\nFigure 5 and 6\nGenerated Structural Component EVAM Rocker using Cobalt Chrome\n*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Safety Factor: 4.267 A. Safety Factor: 4.633\nB. Stress: 137.3 MPa B. Stress: 126.5 MPa\nC. Displacement: 0.05363 mm C. Displacement: 0.05359 mm\nD. Strain: 9.804e-04 D. Strain: 0.00101\nE. Volume of Material: 68.669 cm3 E. Volume of Material:68.648 cm3\nF. Mass of component: 569.269 g F. Mass of component: 569.093\nInconel 718\nFigure 7 and 8\nGenerated Structural Component EVAM Rocker using Inconel 718\n*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Safety Factor: 6.204 A. Safety Factor: 4.906\nB. Max Stress: 124.4 Mpa B. Max Stress: 157.4 MPa\n\n--- Page 8 ---\n7\nC. Displacement: 0.06011 mm C. Displacement: 0.06146 mm\nD. Strain: 9.97e-04 D. Strain: 0.001386\nE. Volume of Material: 68.518 cm3 E. Volume of Material:68.807 cm3\nF. Mass of component: 554.035 g F. Mass of component: 556.374 g\nStainless Steel 17-4 PH\nFigure 9 and 10\nGenerated Structural Component EVAM Rocker using Stainless Steel 17-4 PH\n*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Safety Factor: 4.904 A. Safety Factor: 3.674\nB. Max Stress: 122.4 MPa B. Max Stress: 163.3 MPa\nC. Displacement: 0.05241 mm C. Displacement: 0.05239 mm\nD. Strain: 9.315e-04 D. Strain: 0.001249\nE. Volume of Material: 68.636 cm3 E. Volume of Material:68.636 cm3\nF. Mass of component: 535.36 g F. Mass of component: 535.36 g\nTitanium 6Al-4V\nFigure 11 and 12\nGenerated Structural Component EVAM Rocker using Titanium 6Al-4V\n*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Minimum Safety Factor: 7.667 A. Minimum Safety Factor: 4.358\nB. Max Stress: 115.1 MPa B. Max Stress: 202.5 MPa\nC. Displacement: 0.09401 mm C. Displacement: 0.1566 mm\n\n--- Page 9 ---\n8\nD. Strain: 0.001387 D. Strain: 0.002449\nE. Volume of Material: 68.645 cm3 E. Volume of Material:68.573 cm3\nF. Mass of component: 304.097 g F. Mass of component: 303.773 g\nLife Cycle Assessment\nWith the mass of component generated, the total mass of CO2 can be calculated using\nequation (1):\n𝑡𝑜𝑡𝑎𝑙 𝐶𝑂 𝑝𝑟𝑜𝑑𝑢𝑐𝑒𝑑 = 𝑚𝑎𝑠𝑠 𝑜𝑓 𝑐𝑜𝑚𝑝𝑜𝑛𝑒𝑛𝑡 × 𝐶𝑂 𝑝𝑒𝑟 𝑘𝑔 (1)\n2 2\nThe calculated data is computed in Table 2 and represented in Figure 8.\nTable 3\nTotal mass of CO2 produced based on mass of rocker component\nMaterial Stage of LCA CO2 produced (kg) Total CO2 (kg)\nExtraction 13.17\nAluminium AlSi\nManufacturing 0.36 19.05\n(10Mg)\nRecycling 5.52\nExtraction 7.11\nCobalt Chrome Manufacturing 0.52 9.55\nRecycling 1.92\nExtraction 10.17\nInconel 718 Manufacturing 0.55 12.89\nRecycling 2.16\nExtraction 5.13\nStainless Steel 17-4\nManufacturing 0.48 6.69\nPH\nRecycling 1.08\nExtraction 12.79\nTitanium 6Al-4V Manufacturing 0.35 15.31\nRecycling 2.17\n\n--- Page 10 ---\n9\nFigure 13\nImpact Assessment\nEvaluation\nThe generated results can be summarized in Table 4 below:\nTable 4\nSummarized data from generated models with preset safety factor to 2.5 of different materials\nMaterial Aluminium Cobalt Inconel Stainless Titanium Aluminum\nAlSi Chrome 718 Steel 17-4 6Al-4V 7075-T6\n(10Mg) PH\n(Original\ndesign)\nPreset 2.5 2.5 2.5 2.5 2.5 NA\nsafety\nfactor\nMin Safety 1.305 4.267 6.204 4.904 7.667 3.459\nFactor\nMax Stress 184 137.3 124.4 122.4 115.1 49.85\n(MPa)\nMax 0.13 0.05363 0.06011 0.05241 0.09401 0.03264\nDisplaceme\nnt (mm)\n\n--- Page 11 ---\n10\nStrain 0.003632 9.804e-04 9.97e-04 9.315e-04 0.001387 4.843e-04\nYield 240 586 772 600 882.528 145\nStrength\n(MPa)\nVolume of 73.597 68.669 68.518 68.636 68.645 1.686e5\nMaterial\n(cm3)\nMaterial 594,663.76 26,643,572 10,072,146 1,928,671.6 8,237,400 2,798,760,\nCost (SGD) 000\nMass (g) 196.504 569.296 554.035 535.36 304.097 1323\nCO2 11.97 9.55 12.84 6.68 15.36 23.63\nProduced\n(kg)\n* Improvement in green and worsened in red, from the original design control\nAccording to the results from fusion 360’s simulation, Aluminium’s design is marginal and\noutside factors could cause it to bend or break. Inconel and Titanium’s design is over engineered\nwhich also means it is too strong for the conditions the design is under, which signals wasting\nmaterials and cost.\nFigure 14 and 15\nExample of Fusion360’s warning after running static stress simulations\nFurther Evaluation\nWe wanttoexplorehowthegenerateddesignwouldchangeifwevarythepresetsafetyfactor\naccording to the previous results (table 3). We predicted that if wepresetthesafetyfactortoahigher\nvalue, the generated design would turn out to be over engineered and therefore having additional\nunnecessary mass and materials, and if we preset the safety factor to a lower value, the generated\ndesign would turn out to be fragile and would break under external forces.\nTherefore to further evaluate our designs, we tried to find the optimal preset safety factor\nvalue for each material, and generated a new design under changed preset safety factor as shown in\ntable 4.\nThe generated results can be summarized in Table 5 below:\n\n--- Page 12 ---\n11\nTable 5\nSummarized data from generated models with revised preset safety factor of different materials\nMaterial Aluminium Cobalt Inconel Stainless Titanium Aluminum\nAlSi Chrome 718 Steel 17-4 6Al-4V 7075-T6\n(10Mg) PH\n(Original\ndesign)\nPreset 4 2 2 2 1 NA\nsafety\nfactor\nMin Safety 3.86 4.633 4.906 3.674 4.358 3.459\nFactor\nMax Stress 62.18 126.5 157.4 163.3 202.5 49.85\n(MPa)\nMax 0.05767 0.05359 0.06146 0.05239 0.1566 0.03264\nDisplaceme\nnt (mm)\nStrain 0.001543 0.00101 0.001386 0.001249 0.002449 4.843e-04\nYield 240 586 772 600 882.528 145\nStrength\n(MPa)\nVolume of 116.959 68.648 68.807 68.636 68.573 1.686e5\nMaterial\n(cm3)\nMaterial 680,012.8 26,635,424 10,114,629 1,928,671.6 8,249,880 2,798,760,\nCost (SGD) 000\nMass (g) 312.218 569.093 556.374 535.36 303.773 1323\nCO2 19.05 9.55 12.89 6.69 15.31 23.63\nProduced\n(kg)\n* Improvement in green and worsened in red, from the original design control\nFrom the second run of generation and simulation, only Aluminium’s simulation appears a\nwarning from fusion 360 that the design is marginal; it will break or bend under external\nconditions. Cobalt’s minimum safety factorhasincreased(worsen)fromthefirstrunofsimulation\nwith preset safety factor of 2.5. Inconel, Stainless Steel , and titanium’s simulation resultisbetter\nthan their first designs (no warning shown from fusion 360). Although each of their masses did\nincrease or remain the same, it is not a huge change (the biggest changeofmassfromtable1and\ntable 2 is 2.34 g).\n\n--- Page 13 ---\n12\nTherefore we willbeusingInconel718,StainlessSteel17-4PH,AluminiumAlSi(10Mg)and\nTitanium 6AI-4V from table 4 and Cobalt Chrome from table 3 in the final comparison.\nTable 6\nFinalized table of the optimal preset safety factor of each material for final comparison.\nMaterial Aluminium Cobalt Inconel Stainless Titanium Aluminum\nAlSi Chrome 718 Steel 17-4 6Al-4V 7075-T6\n(10Mg) PH\n(Original\ndesign)\nPreset 4 2.5 2 2 1 NA\nsafety\nfactor\nMin Safety 3.86 4.267 4.906 3.674 4.358 3.459\nFactor\nMax Stress 62.18 137.3 157.4 163.3 202.5 49.85\n(MPa)\nMax 0.05767 0.05363 0.06146 0.05239 0.1566 0.03264\nDisplaceme\nnt (mm)\nStrain 0.001543 9.804e-04 0.001386 0.001249 0.002449 4.843e-04\nYield 240 586 772 600 882.528 145\nStrength\n(MPa)\nVolume of 116.959 68.669 68.807 68.636 68.573 1.686e5\nMaterial\n(cm3)\nMaterial 680,012.8 26,643,572 10,114,629 1,928,671.6 8,249,880 2,798,760,\nCost (SGD) 000\nMass (g) 312.218 569.296 556.374 535.36 303.773 1323\nCO2 19.05 9.55 12.89 6.69 15.31 23.63\nProduced\n(kg)\n* Improvement in green and worsened in red, from the original design control\nPerspectives\nBecause there are manyvariablesandfactorstotakeintoaccountforouranalysis,wedivided\nour analysis into three perspectives (not ordered by importance): Engineering, Business and\nEnvironment.\n\n--- Page 14 ---\n13\nEngineering Perspective\nEngineering is concerned with designing the strongest structure, which we will be taking in\nconsideration the material’s yield strength as well as each material’s generated design’s maximum\nstress data from the static stress simulation done on Fusion360. Material’syieldstrengthtellsushow\nmuch load the material can take before permanent plastic deformation. And maximum stress from\nFusion360’s simulation workspace tells us what isthemaximumstressexperiencedbythedesignata\ncertain area of the design. Therefore if the value ishigher,itwouldmeanthatahighamountofstress\nis focused on one point as all designs face the same loadsandconstraintswhileperformingthestatic\nstress simulation. Which also means that the design is not good at distributing thestressexperienced\nby it.\nTo judge which material design isthestrongest,wewouldwantthatdesign’smaterialtohave\nthe highest yield strength, and the lowest max stress value from Fusion360 as this will give us the\ndesign that is the strongest yet having a good stress distribution. Therefore we havecameupwithan\nequation 𝑌𝑖𝑒𝑙𝑑 𝑆𝑡𝑟𝑒𝑛𝑔𝑡ℎ − 𝑀𝑎𝑥 𝑆𝑡𝑟𝑒𝑠𝑠 in order to judge which design is the best from an\nengineering perspective.\nTable 7\nTable of the results of𝑌𝑖𝑒𝑙𝑑 𝑆𝑡𝑟𝑒𝑛𝑔𝑡ℎ − 𝑀𝑎𝑥 𝑆𝑡𝑟𝑒𝑠𝑠of each material.\nMaterial Aluminium Cobalt Inconel Stainless Titanium Aluminum\nAlSi Chrome 718 Steel 17-4 6Al-4V 7075-T6\n(10Mg) PH\n(Original\ndesign)\nYield 143.71 448.7 614.6 436.7 680.03 95.15\nStrength -\nMax Stress\n(MPa)\nFrom the results in Table 7, Titanium 6A1-4V is the best choice of material from an\nengineering lens, as it has the highest high yield strength of 882.528 MPa. Though the lowest\nmaximum stress is 49.85 MPa from Aluminum 7075-T6, it also has the lowest yield strength of 145\nMPa, hence why it will not be the best option of material.\nBusiness Perspective\nFrom a business pointofview,wewanttoreducetheoverallcostofthematerial.Thiswillbe\nderived by taking the minimum cost per volume (SGD / m3), multiplied by the volume that was\ngenerated in Fusion360. From the data in Table 6, Aluminum A1SI (10Mg) has the lowest overall\nminimum cost at SGD 680,012.80. The next lowest minimum material cost would be StainlessSteel\n17-4 PH which is at SGD 1,928,671.60, a largedifferenceofSGD$1,248,658.20.Withthat,interms\nof minimizing cost, Aluminum A1SI (10Mg) is the best option of material.\nEnvironment Perspective\nWe want to reduce the overall CO2 produced by the materials, since increasedproductionof\nCO2 contributes more harm to the environment and people’s health. In Table 6, Stainless Steel 17-4\n\n--- Page 15 ---\n14\nPH produces the least amount of CO2 at 6.69 kg/kg, therefore it will be the material chosen in\nconsideration of the environment.\nRankings\nTo simplify our analysis, we used a ranking system to determine the best material in\nconsideration of all 3 perspectives, asseeninTable8.Weassignedpointstoour6differentmaterials,\nwith 0 being the lowest and 5 being the highest.\nTable 8\nRanking of materials from engineering, business and environmental perspectives\nMaterial Aluminium Cobalt Inconel Stainless Titanium Aluminum\nAlSi Chrome 718 Steel 17-4 6Al-4V 7075-T6\n(10Mg) PH (Original\ndesign)\nEngineering 1 3 4 2 5 0\nBusiness 5 1 2 4 3 0\nEnvironment 1 4 3 5 2 0\nTotal Points 7 8 9 11 10 0\nRecommendations\nWith the rankings presented in Table 8, the best overall option of material will be Stainless\nSteel 17-4 PH, which ranked up the most points. The second best option will be Titanium 6A1-4V\nwhich ranked 10 points. Next in line will be Inconel 718 whichranked9points,thenCobaltChrome\nwith 8 points, and Aluminum A1SI (10Mg) which is at 7 points. Lastly, there isAluminum7075-T6\nwhich performed the worst in all perspectives given that it has accumulated 0 points total.\nThough Stainless Steel 17-4 PH was not ranked the highest from an engineering point of\nview, in terms of sustainable development, it is the best. ItproducestheleastmassofCO2emissions\nwhich is incredibly important in the modern world where there are serious implications of pursuing\nharmful practices that contribute to climate change. In developing for the long term where future\ngenerations are taken into consideration, it is the best decision to pursue a practice thatbenefitsboth\nthe environment and business. In this case, because Stainless Steel 17-4PHisthesecondcheapestin\nterms of cost, it will be greatforthebusinessastheycankeepmanufacturingwiththematerialwhilst\nbeing considerate to the environment and health of people.\nIf we were to proceed with the best performing material in terms of strength, then Titanium\n6A1-4V would be the option to go for. However, this will not be a sustainable choice as it is ranked\nthe third lowest in terms of the environment’s perspective. It produces thethirdhighestmassofCO2\nat 15.31kg, which is a difference of 8.72kg of CO2 from Stainless Steel 17-4 PH.Andthoughitwill\n\n--- Page 16 ---\n15\nbe tough to break and effective product wise, this will only be beneficial short term, and its\nunsustainable practices cannot be carried forward for the long term and future generations.\nSummary of Lessons Learnt\n1. The use of generative design can help identify new design possibilities that may not have\nbeen considered before.\n2. In generating designs, we must use preserve and obstacle geometries in order to output\ndesired outcome.\n3. Failure to properly define project requirements and constraintscouldresultindesignsthatdo\nnot meet the necessary specifications. Constraints are needed to achieve the motion or\nbehavior needed.\n4. Careful consideration should be given to the selection of materials based on their\nperformance, cost, and sustainability.\n5. In analyzing the sustainability factor of materials, a life cycle assessment is an appropriate\nmethod in identifying how much CO2 is being produced.\n6. Testing and evaluation of the generative design outcomes is crucial to ensure that the design\nmeets all project requirements and constraints.\n7. An evaluation of the original design mustbeconductedinordertohaveabenchmarkthatthe\ngenerated designs can compare to.\n8. Continuous improvement and iterationarenecessaryforrefiningthedesignandimprovingits\nperformance over time.\n9. Observing safety factors and stress levels is crucial to determine the design's real-life\nperformance. A design with a low safety factor or high stress may not be reliable, requiring\nmodifications. High values of safety factors can lead to over-engineering.\n10. There are alwaysmultipleperspectivesinevaluatingtherightmaterialformanufacturing,and\nwe should never rule outtheotherperspectivesinfavorofone.Andjustbecauseamaterialis\nthe best performing, doesn’t always mean thatit’sthematerialthatshouldbepicked,because\nsustainability has become an important topic for consideration and we should start learning\nhow to build with the future in mind.\nReferences\nAnsys Granta. (2019).Nickel-chromium alloy, INCONEL.CES 2019 Edupack.\nhttps://www.ansys.com/products/materials\nPowell, D. (2023, January 10).Electric car statistics- EV Data [Update: Jan 23]. heycar. Retrieved\nFebruary 22, 2023, from https://heycar.co.uk/blog/electric-cars-statistics-and-projections\n\n--- Page 17 ---\n16\nUniversal Technical Institute. (2021, October 5).How Do Car Suspension Systems Work? | UTI.\nUniversal Technical Institute. Retrieved February 22, 2023, from\nhttps://www.uti.edu/blog/automotive/car-suspension\n",
    "cleaned_content": "[PAGE BREAK]\nGenerative Design of Electric Vehicle Additive Manufacturing (EVAM) Suspension Rocker\nAUTHORS STUDENT ID\nHe Tianli 1006151\nLim Yu Jie 1005999\nAditya Kumar 1006300\nEthan Choo E-Rhen 1006324\nCaitlin Daphne Tan Chiang 1006537\nWei Ming 1006264\nAll team members contributed equally to the task.\nSingapore University of Technology and Design\n60.002: AI Applications in Design\nDr. Kwan Wei Lek, Dr Edwin Koh, Mr Michael Alexander Reeves\nFebruary 26, 2023\n[PAGE BREAK]\nGenerative Design of Electric Vehicle Additive Manufacturing (EVAM) Suspension Rocker\nElectric vehiclesarebecomingamorepopularchoiceinrecentyears,withanincreaseof40%\nin 2022(Powell,2023).Ingeneral,thesevehiclesarecomparativelymoresustainablecomparedtothe\nconventional combustion engine, simply because they do not produce harmful carbon emissions.\nAdditionally, most vehicle charging stations use renewable energy. Similarly, manufacturing has\ncontinually been improving by creating a smaller carbon footprint through using eco-friendly\nmaterials.\nAt the forefront of design and innovation, Singapore University of Technology and Design\n(SUTD) is manufacturing components for their electric vehicle project, known as Electric Vehicle\nAdditive Manufacturing (EVAM). As the name suggests, they employ fabricating methods such as\nSelective Laser Melting (SLM), Fused Deposition Modeling (FDM), Multi Jet Fusion (MJF) and\nStereolithography (SLA).\nIn this project, we will be exploring the use of generative design tools to propose practical\nsolutions that can promote material and energy savings. We will be focusing on the Electric Vehicle\nAdditive Manufacturing (EVAM) suspension rocker.\nMethodology\nFirstly, we had to understand the loadconstraintsofthe EVAMrocker.Forthis,weneededto\nmap out the component’s function within the mechanism of the system, whichhasbeendrawnoutin\nFigure 1.\nFigure 1\nIllustration of the EVAM Rocker Mechanism\n[PAGE BREAK]\nThe purpose of this mechanism is to stabilize the control of thevehicleandincreasecomfort\nin the vehicle (Universal Technical Institute, 2021). This can be illustrated in the stages of motion\nwhen the vehicle travels over a bump.\nIn the first stage when the wheel first hits thebump,thereisanupwardforceonthewheelof\nthe car. This pushes the suspension rod upwards at a 55 degree angle to the vertical. Through the\nrocker, the force is translated to a downward force onto the coil spring. This first stage is illustrated\nusing the green arrows.\nThe second stage is when the wheel fully travels over the bump. The suspension coil spring\nexerts a force back on the rocker. The rockertranslatestheforcebackontothesuspensionrod,which\npushesthewheelbackintotheground.Thismaximizesthecontactofthetireswiththeground,sothat\ntraction can be kept, and the driver will not lose control of the car. This is illustrated by the red\narrows.\nWithin thesetwostages,therockertranslatesthedirectionsoftheforces.Thisisillustratedby\nthe pink arrows, as the rocker rotates about the axis (pink dotted line).\nLoads and Constraints\nFigure 2\nIllustration of loads and constraints\nNext, we needed to interpret our force analysis from Figure 1 into the generative design\nsoftware that we would be using, which is Autodesk Fusion 360. A generative study was created as\nseen in Appendix A.\nThe generative study requires preserve geometries and obstacle geometries. The preserve\ngeometries are the structures that will hold pins that are attached to the suspension rod, coil spring,\nand main chassis.Meanwhiletheobstaclegeometriesareforthesepinstogothrough.Thecomponent\nis then sandwiched by two more obstacle geometries so that its width does not increase.\n[PAGE BREAK]\nThe design of mechanical systems often requires the use ofconstraintstoachievethedesired\nmotion or behavior. In the case of the rocker and connector bearing support, a pin constraint and a\nfixed constraint were used according to the specifications provided in the design information. The\nfixed constraint was applied to the largestholeintherockertopreventanymovementwhileallowing\nit to act as a hinge, while the pin constraint was appliedtothebearingsupporttopreventdeformities\nin the radial and axial directions.\nThe pin constraint on the bearing support allowed for movement in response to the force\nbeing applied on the other hole by the pushrods, while preventing undesirable deformities. The\npushrods werefoundtobeexertingaloadof5000 Natanangleof55 degreesfromthevertical,which\nrequired careful consideration of the constraints to ensure proper function of the system.\nBy using textbook constraints and following the design information, the system was able to\nachieve the desired motion and withstandtheappliedforces.Theproperuseofconstraintsisessential\nin mechanical design to ensure safe and reliable operation of the system.\nMaterials\nIn order to generate design alternatives for evaluation, we decided tovarythematerialofthe\ngenerative design study. We narrowed down the materials down to Aluminium Al Si (10 Mg), Cobalt\nChrome, Inconel 718, Stainless Steel 17-4 PH, Titanium 6 Al-4 V, from the Fusion 360 Additive\nMaterial Library. We chose Inconel 718 and Stainless Steel 17-4 PH from their other variants of\nInconels and Stainless Steels because both materials have better properties in general, in terms of\nyield and tensile strength, density and cost.\nWe are aiming to settle for materials that have the highest yield and tensile strength, lowest\ndensity in order to minimize its weight contribution to the vehicle, and lowest cost for better\nmanufacturing. Although Inconel 718 has lower ultimate tensile strength than its other 2 variants\n(Inconel 625 and Inconel 718 plus), it also has the lowest density and highest yield strength, whilst\nbeing cheaper. The Stainless Steel that we picked (Stainless Steel 17-4 PH)haslowerdensity,higher\nyield, and higher tensile strength as compared to its alternative (Stainless Steel AISI 304) in the\nFusion 360 Additive Material Library (Ansys Granta, 2019, 2). Since wearedesigningarockerfora\nformula 1 themed electric vehicle, although Stainless Steel17-4 PHismorecostly,thebenefitsofthe\nmaterial being light and strong exceeded the drawback of the cost of the material.\nFrom there, we performed a Life Cycle Assessment (LCA) on these materials to assess and\nunderstand the environmental impacts in each stage of the material’s life cycle. This is important in\norder to be inline withtheproject’sgoalofsustainabledesign.Inthiscase,wefocusedonthemassof\ncarbon dioxide (CO2) produced during material extraction, component fabrication, and component\nrecycling.Thisisbecausethebenefitofanelectricvehicleisthatitdoesnotrelease CO2 whileinuse.\nTo simplify the analysis, we had to make some assumptions:\n(1) No CO2 is produced in use andduringdistributionofcomponents,asthecomponentswillbe\nmanufactured in-house.\n(2) Data will be collected from Casting CO2, due to lack of information about CO2 producedin\nadditive manufacturing\n[PAGE BREAK]\nThe LCA can hence be summarized in Figure 13, with additional LCA charts and graphs in\nAppendix A.\nTable 1\nLife Cycle Assessment of Aluminium, Cobalt Chrome, Inconel, Stainless Steel and Titanium\nMaterial Stage of LCA CO2 per kg (kg/kg) Total CO2 (kg/kg)\nExtraction 42.2\nAluminium Al Si\nManufacturing 1.16 61.06\n(10 Mg)\nRecycling 17.7\nExtraction 12.5\nCobalt Chrome Manufacturing 0.909 16.79\nRecycling 3.38\nExtraction 18.3\nInconel 718 Manufacturing 0.996 23.18\nRecycling 3.88\nExtraction 9.58\nStainless Steel 17-4\nManufacturing 0.894 12.49\nRecycling 2.02\nExtraction 42.2\nTitanium 6 Al-4 V Manufacturing 1.16 50.53\nRecycling 7.17\nFrom this, wecanconcludethat Stainless Steel17-4 PHhastheleastenvironmentalimpactin\nregards to CO2 produced. However, the different materials used in formgenerationwillusedifferent\nvolumesofmaterial.Hence,weneedtofactorinthevolumeofmaterialusedinordertodeterminethe\nactual total CO2 produced.\nWe also researched material cost for our evaluation to optimize the design from a business\nperspective. The cost of the Aluminium A1 SI (10 Mg), Cobalt Chrome, Inconel 718, Stainless Steel\n17-4 PH, Titanium 6 A1-4 V, and the original Aluminum 7075-T6 are (in SGD per m3) is\n8.08 e3-9.46 e3, 3.88 e5-5.26 e5, 1.47 e5-1.69 e5, 2.81 e4-3.22 e4, 1.2 e5-1.43 e5, and 1.66 e4 - 2 e4\nrespectively (Ansys Granta, 2019, 2). The product of the prices per unit volume and the volume of\nmaterial used will then be used in our evaluation.\nAdditive Manufacturing vs Die Casting\nWhile additive manufacturing is of the future, we took into consideration die casting aswell\nas it was one of the manufacturing methods offered in Fusion360. Using the default material,\naluminum,wegeneratedtwomodelsusingthesetwofabricationmethods,asseenin Appendix B.The\n[PAGE BREAK]\nresults show that the volume of material used in die casting is much greater than in additive\nmanufacturing, due to the precision that it offers ascomparedtodiecasting.Thismeansthatadditive\nmanufacturing will produce less CO2 than die casting as well.\nControl\nTo aid in our evaluation, we set the control to be the original design fabricated using\nAluminum 7075-T6, using milling. A simulation was conducted and the results are in Table 2.\nTable 2\nOriginal rocker design specifications\nMin Safety Factor 3.459 Mass of component 1323\n(kg)\nMax Stress (MPa) 59.85 CO2 produced (kg) 23.63\nMax Displacement 0.03264\n(mm)\nStrain 4.843 e-04\nVolume of Material 1.686 e5\n(mm3)\nResults\nGenerated Designs\nThus,wegenerated10 modelswith5 differentmaterialsand2 differentpresettedsafetyfactor\n. To aid our evaluation, we extracted simulation data regarding the (A) Safety Factor;(B)Stress;(C)\nDisplacement; (D) Strain. Additionally, fabrication costs such as (E) Volume of material, and\ncomponent specifics such as(F)Mass,wereextracted.Figure5 to9 showstheoutcomesofthemodel\ngeneration, while Appendix C presents the heat maps of the additional data.\nAluminium Al Si (10 Mg)\nFigure 3 and 4\nGenerated Structural Component EVAM Rocker using Aluminium Al Si (10 Mg)\n*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 4\n[PAGE BREAK]\nA. Minimum Safety Factor: 1.305 A. Minimum Safety Factor: 3.86\nB. Max Stress: 184 MPa B. Max Stress: 62.18 Mpa\nC. Displacement: 0.13 mm C. Displacement: 0.05767 mm\nD. Strain: 0.003632 D. Strain: 0.001543\nE. Volume of Material: 73.597 cm3 E. Volume of Material:116.959 cm3\nF. Mass of component: 196.504 g F. Mass of component: 312.218 g\nCobalt Chrome\nFigure 5 and 6\nGenerated Structural Component EVAM Rocker using Cobalt Chrome\n*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Safety Factor: 4.267 A. Safety Factor: 4.633\nB. Stress: 137.3 MPa B. Stress: 126.5 MPa\nC. Displacement: 0.05363 mm C. Displacement: 0.05359 mm\nD. Strain: 9.804 e-04 D. Strain: 0.00101\nE. Volume of Material: 68.669 cm3 E. Volume of Material:68.648 cm3\nF. Mass of component: 569.269 g F. Mass of component: 569.093\nInconel 718\nFigure 7 and 8\nGenerated Structural Component EVAM Rocker using Inconel 718\n*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Safety Factor: 6.204 A. Safety Factor: 4.906\nB. Max Stress: 124.4 Mpa B. Max Stress: 157.4 MPa\n[PAGE BREAK]\nC. Displacement: 0.06011 mm C. Displacement: 0.06146 mm\nD. Strain: 9.97 e-04 D. Strain: 0.001386\nE. Volume of Material: 68.518 cm3 E. Volume of Material:68.807 cm3\nF. Mass of component: 554.035 g F. Mass of component: 556.374 g\nStainless Steel 17-4 PH\nFigure 9 and 10\nGenerated Structural Component EVAM Rocker using Stainless Steel 17-4 PH\n*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Safety Factor: 4.904 A. Safety Factor: 3.674\nB. Max Stress: 122.4 MPa B. Max Stress: 163.3 MPa\nC. Displacement: 0.05241 mm C. Displacement: 0.05239 mm\nD. Strain: 9.315 e-04 D. Strain: 0.001249\nE. Volume of Material: 68.636 cm3 E. Volume of Material:68.636 cm3\nF. Mass of component: 535.36 g F. Mass of component: 535.36 g\nTitanium 6 Al-4 V\nFigure 11 and 12\nGenerated Structural Component EVAM Rocker using Titanium 6 Al-4 V\n*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Minimum Safety Factor: 7.667 A. Minimum Safety Factor: 4.358\nB. Max Stress: 115.1 MPa B. Max Stress: 202.5 MPa\nC. Displacement: 0.09401 mm C. Displacement: 0.1566 mm\n[PAGE BREAK]\nD. Strain: 0.001387 D. Strain: 0.002449\nE. Volume of Material: 68.645 cm3 E. Volume of Material:68.573 cm3\nF. Mass of component: 304.097 g F. Mass of component: 303.773 g\nLife Cycle Assessment\nWith the mass of component generated, the total mass of CO2 can be calculated using\nequation (1):\n𝑡𝑜𝑡𝑎𝑙 𝐶𝑂 𝑝𝑟𝑜𝑑𝑢𝑐𝑒𝑑 = 𝑚𝑎𝑠𝑠 𝑜𝑓 𝑐𝑜𝑚𝑝𝑜𝑛𝑒𝑛𝑡 × 𝐶𝑂 𝑝𝑒𝑟 𝑘𝑔 (1)\nThe calculated data is computed in Table 2 and represented in Figure 8.\nTable 3\nTotal mass of CO2 produced based on mass of rocker component\nMaterial Stage of LCA CO2 produced (kg) Total CO2 (kg)\nExtraction 13.17\nAluminium Al Si\nManufacturing 0.36 19.05\n(10 Mg)\nRecycling 5.52\nExtraction 7.11\nCobalt Chrome Manufacturing 0.52 9.55\nRecycling 1.92\nExtraction 10.17\nInconel 718 Manufacturing 0.55 12.89\nRecycling 2.16\nExtraction 5.13\nStainless Steel 17-4\nManufacturing 0.48 6.69\nRecycling 1.08\nExtraction 12.79\nTitanium 6 Al-4 V Manufacturing 0.35 15.31\nRecycling 2.17\n[PAGE BREAK]\nFigure 13\nImpact Assessment\nEvaluation\nThe generated results can be summarized in Table 4 below:\nTable 4\nSummarized data from generated models with preset safety factor to 2.5 of different materials\nMaterial Aluminium Cobalt Inconel Stainless Titanium Aluminum\nAl Si Chrome 718 Steel 17-4 6 Al-4 V 7075-T6\n(10 Mg) PH\n(Original\ndesign)\nPreset 2.5 2.5 2.5 2.5 2.5 NA\nsafety\nfactor\nMin Safety 1.305 4.267 6.204 4.904 7.667 3.459\nFactor\nMax Stress 184 137.3 124.4 122.4 115.1 49.85\n(MPa)\nMax 0.13 0.05363 0.06011 0.05241 0.09401 0.03264\nDisplaceme\nnt (mm)\n[PAGE BREAK]\nStrain 0.003632 9.804 e-04 9.97 e-04 9.315 e-04 0.001387 4.843 e-04\nYield 240 586 772 600 882.528 145\nStrength\n(MPa)\nVolume of 73.597 68.669 68.518 68.636 68.645 1.686 e5\nMaterial\n(cm3)\nMaterial 594,663.76 26,643,572 10,072,146 1,928,671.6 8,237,400 2,798,760,\nCost (SGD) 000\nMass (g) 196.504 569.296 554.035 535.36 304.097 1323\nCO2 11.97 9.55 12.84 6.68 15.36 23.63\nProduced\n(kg)\n* Improvement in green and worsened in red, from the original design control\nAccording to the results from fusion 360’s simulation, Aluminium’s design is marginal and\noutside factors could cause it to bend or break. Inconel and Titanium’s design is over engineered\nwhich also means it is too strong for the conditions the design is under, which signals wasting\nmaterials and cost.\nFigure 14 and 15\nExample of Fusion360’s warning after running static stress simulations\nFurther Evaluation\nWe wanttoexplorehowthegenerateddesignwouldchangeifwevarythepresetsafetyfactor\naccording to the previous results (table 3). We predicted that if wepresetthesafetyfactortoahigher\nvalue, the generated design would turn out to be over engineered and therefore having additional\nunnecessary mass and materials, and if we preset the safety factor to a lower value, the generated\ndesign would turn out to be fragile and would break under external forces.\nTherefore to further evaluate our designs, we tried to find the optimal preset safety factor\nvalue for each material, and generated a new design under changed preset safety factor as shown in\ntable 4.\nThe generated results can be summarized in Table 5 below:\n[PAGE BREAK]\nTable 5\nSummarized data from generated models with revised preset safety factor of different materials\nMaterial Aluminium Cobalt Inconel Stainless Titanium Aluminum\nAl Si Chrome 718 Steel 17-4 6 Al-4 V 7075-T6\n(10 Mg) PH\n(Original\ndesign)\nPreset 4 2 2 2 1 NA\nsafety\nfactor\nMin Safety 3.86 4.633 4.906 3.674 4.358 3.459\nFactor\nMax Stress 62.18 126.5 157.4 163.3 202.5 49.85\n(MPa)\nMax 0.05767 0.05359 0.06146 0.05239 0.1566 0.03264\nDisplaceme\nnt (mm)\nStrain 0.001543 0.00101 0.001386 0.001249 0.002449 4.843 e-04\nYield 240 586 772 600 882.528 145\nStrength\n(MPa)\nVolume of 116.959 68.648 68.807 68.636 68.573 1.686 e5\nMaterial\n(cm3)\nMaterial 680,012.8 26,635,424 10,114,629 1,928,671.6 8,249,880 2,798,760,\nCost (SGD) 000\nMass (g) 312.218 569.093 556.374 535.36 303.773 1323\nCO2 19.05 9.55 12.89 6.69 15.31 23.63\nProduced\n(kg)\n* Improvement in green and worsened in red, from the original design control\nFrom the second run of generation and simulation, only Aluminium’s simulation appears a\nwarning from fusion 360 that the design is marginal; it will break or bend under external\nconditions. Cobalt’s minimum safety factorhasincreased(worsen)fromthefirstrunofsimulation\nwith preset safety factor of 2.5. Inconel, Stainless Steel , and titanium’s simulation resultisbetter\nthan their first designs (no warning shown from fusion 360). Although each of their masses did\nincrease or remain the same, it is not a huge change (the biggest changeofmassfromtable1 and\ntable 2 is 2.34 g).\n[PAGE BREAK]\nTherefore we willbeusing Inconel718,Stainless Steel17-4 PH,Aluminium Al Si(10 Mg)and\nTitanium 6 AI-4 V from table 4 and Cobalt Chrome from table 3 in the final comparison.\nTable 6\nFinalized table of the optimal preset safety factor of each material for final comparison.\nMaterial Aluminium Cobalt Inconel Stainless Titanium Aluminum\nAl Si Chrome 718 Steel 17-4 6 Al-4 V 7075-T6\n(10 Mg) PH\n(Original\ndesign)\nPreset 4 2.5 2 2 1 NA\nsafety\nfactor\nMin Safety 3.86 4.267 4.906 3.674 4.358 3.459\nFactor\nMax Stress 62.18 137.3 157.4 163.3 202.5 49.85\n(MPa)\nMax 0.05767 0.05363 0.06146 0.05239 0.1566 0.03264\nDisplaceme\nnt (mm)\nStrain 0.001543 9.804 e-04 0.001386 0.001249 0.002449 4.843 e-04\nYield 240 586 772 600 882.528 145\nStrength\n(MPa)\nVolume of 116.959 68.669 68.807 68.636 68.573 1.686 e5\nMaterial\n(cm3)\nMaterial 680,012.8 26,643,572 10,114,629 1,928,671.6 8,249,880 2,798,760,\nCost (SGD) 000\nMass (g) 312.218 569.296 556.374 535.36 303.773 1323\nCO2 19.05 9.55 12.89 6.69 15.31 23.63\nProduced\n(kg)\n* Improvement in green and worsened in red, from the original design control\nPerspectives\nBecause there are manyvariablesandfactorstotakeintoaccountforouranalysis,wedivided\nour analysis into three perspectives (not ordered by importance): Engineering, Business and\nEnvironment.\n[PAGE BREAK]\nEngineering Perspective\nEngineering is concerned with designing the strongest structure, which we will be taking in\nconsideration the material’s yield strength as well as each material’s generated design’s maximum\nstress data from the static stress simulation done on Fusion360. Material’syieldstrengthtellsushow\nmuch load the material can take before permanent plastic deformation. And maximum stress from\nFusion360’s simulation workspace tells us what isthemaximumstressexperiencedbythedesignata\ncertain area of the design. Therefore if the value ishigher,itwouldmeanthatahighamountofstress\nis focused on one point as all designs face the same loadsandconstraintswhileperformingthestatic\nstress simulation. Which also means that the design is not good at distributing thestressexperienced\nby it.\nTo judge which material design isthestrongest,wewouldwantthatdesign’smaterialtohave\nthe highest yield strength, and the lowest max stress value from Fusion360 as this will give us the\ndesign that is the strongest yet having a good stress distribution. Therefore we havecameupwithan\nequation 𝑌𝑖𝑒𝑙𝑑 𝑆𝑡𝑟𝑒𝑛𝑔𝑡ℎ − 𝑀𝑎𝑥 𝑆𝑡𝑟𝑒𝑠𝑠 in order to judge which design is the best from an\nengineering perspective.\nTable 7\nTable of the results of𝑌𝑖𝑒𝑙𝑑 𝑆𝑡𝑟𝑒𝑛𝑔𝑡ℎ − 𝑀𝑎𝑥 𝑆𝑡𝑟𝑒𝑠𝑠of each material.\nMaterial Aluminium Cobalt Inconel Stainless Titanium Aluminum\nAl Si Chrome 718 Steel 17-4 6 Al-4 V 7075-T6\n(10 Mg) PH\n(Original\ndesign)\nYield 143.71 448.7 614.6 436.7 680.03 95.15\nStrength -\nMax Stress\n(MPa)\nFrom the results in Table 7, Titanium 6 A1-4 V is the best choice of material from an\nengineering lens, as it has the highest high yield strength of 882.528 MPa. Though the lowest\nmaximum stress is 49.85 MPa from Aluminum 7075-T6, it also has the lowest yield strength of 145\nMPa, hence why it will not be the best option of material.\nBusiness Perspective\nFrom a business pointofview,wewanttoreducetheoverallcostofthematerial.Thiswillbe\nderived by taking the minimum cost per volume (SGD / m3), multiplied by the volume that was\ngenerated in Fusion360. From the data in Table 6, Aluminum A1 SI (10 Mg) has the lowest overall\nminimum cost at SGD 680,012.80. The next lowest minimum material cost would be Stainless Steel\n17-4 PH which is at SGD 1,928,671.60, a largedifferenceof SGD$1,248,658.20.Withthat,interms\nof minimizing cost, Aluminum A1 SI (10 Mg) is the best option of material.\nEnvironment Perspective\nWe want to reduce the overall CO2 produced by the materials, since increasedproductionof\nCO2 contributes more harm to the environment and people’s health. In Table 6, Stainless Steel 17-4\n[PAGE BREAK]\nPH produces the least amount of CO2 at 6.69 kg/kg, therefore it will be the material chosen in\nconsideration of the environment.\nRankings\nTo simplify our analysis, we used a ranking system to determine the best material in\nconsideration of all 3 perspectives, asseenin Table8.Weassignedpointstoour6 differentmaterials,\nwith 0 being the lowest and 5 being the highest.\nTable 8\nRanking of materials from engineering, business and environmental perspectives\nMaterial Aluminium Cobalt Inconel Stainless Titanium Aluminum\nAl Si Chrome 718 Steel 17-4 6 Al-4 V 7075-T6\n(10 Mg) PH (Original\ndesign)\nEngineering 1 3 4 2 5 0\nBusiness 5 1 2 4 3 0\nEnvironment 1 4 3 5 2 0\nTotal Points 7 8 9 11 10 0\nRecommendations\nWith the rankings presented in Table 8, the best overall option of material will be Stainless\nSteel 17-4 PH, which ranked up the most points. The second best option will be Titanium 6 A1-4 V\nwhich ranked 10 points. Next in line will be Inconel 718 whichranked9 points,then Cobalt Chrome\nwith 8 points, and Aluminum A1 SI (10 Mg) which is at 7 points. Lastly, there is Aluminum7075-T6\nwhich performed the worst in all perspectives given that it has accumulated 0 points total.\nThough Stainless Steel 17-4 PH was not ranked the highest from an engineering point of\nview, in terms of sustainable development, it is the best. Itproducestheleastmassof CO2 emissions\nwhich is incredibly important in the modern world where there are serious implications of pursuing\nharmful practices that contribute to climate change. In developing for the long term where future\ngenerations are taken into consideration, it is the best decision to pursue a practice thatbenefitsboth\nthe environment and business. In this case, because Stainless Steel 17-4 PHisthesecondcheapestin\nterms of cost, it will be greatforthebusinessastheycankeepmanufacturingwiththematerialwhilst\nbeing considerate to the environment and health of people.\nIf we were to proceed with the best performing material in terms of strength, then Titanium\n6 A1-4 V would be the option to go for. However, this will not be a sustainable choice as it is ranked\nthe third lowest in terms of the environment’s perspective. It produces thethirdhighestmassof CO2\nat 15.31 kg, which is a difference of 8.72 kg of CO2 from Stainless Steel 17-4 PH.Andthoughitwill\n[PAGE BREAK]\nbe tough to break and effective product wise, this will only be beneficial short term, and its\nunsustainable practices cannot be carried forward for the long term and future generations.\nSummary of Lessons Learnt\n1. The use of generative design can help identify new design possibilities that may not have\nbeen considered before.\n2. In generating designs, we must use preserve and obstacle geometries in order to output\ndesired outcome.\n3. Failure to properly define project requirements and constraintscouldresultindesignsthatdo\nnot meet the necessary specifications. Constraints are needed to achieve the motion or\nbehavior needed.\n4. Careful consideration should be given to the selection of materials based on their\nperformance, cost, and sustainability.\n5. In analyzing the sustainability factor of materials, a life cycle assessment is an appropriate\nmethod in identifying how much CO2 is being produced.\n6. Testing and evaluation of the generative design outcomes is crucial to ensure that the design\nmeets all project requirements and constraints.\n7. An evaluation of the original design mustbeconductedinordertohaveabenchmarkthatthe\ngenerated designs can compare to.\n8. Continuous improvement and iterationarenecessaryforrefiningthedesignandimprovingits\nperformance over time.\n9. Observing safety factors and stress levels is crucial to determine the design's real-life\nperformance. A design with a low safety factor or high stress may not be reliable, requiring\nmodifications. High values of safety factors can lead to over-engineering.\n10. There are alwaysmultipleperspectivesinevaluatingtherightmaterialformanufacturing,and\nwe should never rule outtheotherperspectivesinfavorofone.Andjustbecauseamaterialis\nthe best performing, doesn’t always mean thatit’sthematerialthatshouldbepicked,because\nsustainability has become an important topic for consideration and we should start learning\nhow to build with the future in mind.\nReferences\nAnsys Granta. (2019).Nickel-chromium alloy, INCONEL.CES 2019 Edupack.\nhttps://www.ansys.com/products/materials\nPowell, D. (2023, January 10).Electric car statistics- EV Data [Update: Jan 23]. heycar. Retrieved\nFebruary 22, 2023, from https://heycar.co.uk/blog/electric-cars-statistics-and-projections\n[PAGE BREAK]\nUniversal Technical Institute. (2021, October 5).How Do Car Suspension Systems Work? | UTI.\nUniversal Technical Institute. Retrieved February 22, 2023, from\nhttps://www.uti.edu/blog/automotive/car-suspension",
    "sections": [
      {
        "title": "Wei Ming 1006264",
        "content": "All team members contributed equally to the task.\n"
      },
      {
        "title": "Singapore University of Technology and Design",
        "content": "60.002: AI Applications in Design\nDr. Kwan Wei Lek, Dr Edwin Koh, Mr Michael Alexander Reeves\nFebruary 26, 2023\n"
      },
      {
        "title": "Generative Design of Electric Vehicle Additive Manufacturing (EVAM) Suspension Rocker",
        "content": "Electric vehiclesarebecomingamorepopularchoiceinrecentyears,withanincreaseof40%\nin 2022(Powell,2023).Ingeneral,thesevehiclesarecomparativelymoresustainablecomparedtothe\nconventional combustion engine, simply because they do not produce harmful carbon emissions.\nAdditionally, most vehicle charging stations use renewable energy. Similarly, manufacturing has\ncontinually been improving by creating a smaller carbon footprint through using eco-friendly\nmaterials.\nAt the forefront of design and innovation, Singapore University of Technology and Design\n(SUTD) is manufacturing components for their electric vehicle project, known as Electric Vehicle\n"
      },
      {
        "title": "Selective Laser Melting (SLM), Fused Deposition Modeling (FDM), Multi Jet Fusion (MJF) and",
        "content": "Stereolithography (SLA).\nIn this project, we will be exploring the use of generative design tools to propose practical\nsolutions that can promote material and energy savings. We will be focusing on the Electric Vehicle\n"
      },
      {
        "title": "Methodology",
        "content": "Firstly, we had to understand the loadconstraintsofthe EVAMrocker.Forthis,weneededto\nmap out the component’s function within the mechanism of the system, whichhasbeendrawnoutin\nFigure 1.\nFigure 1\nIllustration of the EVAM Rocker Mechanism\n"
      },
      {
        "title": "Content",
        "content": "The purpose of this mechanism is to stabilize the control of thevehicleandincreasecomfort\nin the vehicle (Universal Technical Institute, 2021). This can be illustrated in the stages of motion\nwhen the vehicle travels over a bump.\nIn the first stage when the wheel first hits thebump,thereisanupwardforceonthewheelof\nthe car. This pushes the suspension rod upwards at a 55 degree angle to the vertical. Through the\nrocker, the force is translated to a downward force onto the coil spring. This first stage is illustrated\nusing the green arrows.\nThe second stage is when the wheel fully travels over the bump. The suspension coil spring\nexerts a force back on the rocker. The rockertranslatestheforcebackontothesuspensionrod,which\npushesthewheelbackintotheground.Thismaximizesthecontactofthetireswiththeground,sothat\ntraction can be kept, and the driver will not lose control of the car. This is illustrated by the red\narrows.\nWithin thesetwostages,therockertranslatesthedirectionsoftheforces.Thisisillustratedby\nthe pink arrows, as the rocker rotates about the axis (pink dotted line).\nLoads and Constraints\nFigure 2\nIllustration of loads and constraints\nNext, we needed to interpret our force analysis from Figure 1 into the generative design\nsoftware that we would be using, which is Autodesk Fusion 360. A generative study was created as\nseen in Appendix A.\nThe generative study requires preserve geometries and obstacle geometries. The preserve\ngeometries are the structures that will hold pins that are attached to the suspension rod, coil spring,\nand main chassis.Meanwhiletheobstaclegeometriesareforthesepinstogothrough.Thecomponent\nis then sandwiched by two more obstacle geometries so that its width does not increase.\n"
      },
      {
        "title": "Content",
        "content": "The design of mechanical systems often requires the use ofconstraintstoachievethedesired\nmotion or behavior. In the case of the rocker and connector bearing support, a pin constraint and a\nfixed constraint were used according to the specifications provided in the design information. The\nfixed constraint was applied to the largestholeintherockertopreventanymovementwhileallowing\nit to act as a hinge, while the pin constraint was appliedtothebearingsupporttopreventdeformities\nin the radial and axial directions.\nThe pin constraint on the bearing support allowed for movement in response to the force\nbeing applied on the other hole by the pushrods, while preventing undesirable deformities. The\npushrods werefoundtobeexertingaloadof5000 Natanangleof55 degreesfromthevertical,which\nrequired careful consideration of the constraints to ensure proper function of the system.\nBy using textbook constraints and following the design information, the system was able to\nachieve the desired motion and withstandtheappliedforces.Theproperuseofconstraintsisessential\nin mechanical design to ensure safe and reliable operation of the system.\nMaterials\nIn order to generate design alternatives for evaluation, we decided tovarythematerialofthe\ngenerative design study. We narrowed down the materials down to Aluminium Al Si (10 Mg), Cobalt\nChrome, Inconel 718, Stainless Steel 17-4 PH, Titanium 6 Al-4 V, from the Fusion 360 Additive\n"
      },
      {
        "title": "Material Library. We chose Inconel 718 and Stainless Steel 17-4 PH from their other variants of",
        "content": "Inconels and Stainless Steels because both materials have better properties in general, in terms of\nyield and tensile strength, density and cost.\nWe are aiming to settle for materials that have the highest yield and tensile strength, lowest\ndensity in order to minimize its weight contribution to the vehicle, and lowest cost for better\nmanufacturing. Although Inconel 718 has lower ultimate tensile strength than its other 2 variants\n(Inconel 625 and Inconel 718 plus), it also has the lowest density and highest yield strength, whilst\nbeing cheaper. The Stainless Steel that we picked (Stainless Steel 17-4 PH)haslowerdensity,higher\nyield, and higher tensile strength as compared to its alternative (Stainless Steel AISI 304) in the\nFusion 360 Additive Material Library (Ansys Granta, 2019, 2). Since wearedesigningarockerfora\nformula 1 themed electric vehicle, although Stainless Steel17-4 PHismorecostly,thebenefitsofthe\nmaterial being light and strong exceeded the drawback of the cost of the material.\nFrom there, we performed a Life Cycle Assessment (LCA) on these materials to assess and\nunderstand the environmental impacts in each stage of the material’s life cycle. This is important in\norder to be inline withtheproject’sgoalofsustainabledesign.Inthiscase,wefocusedonthemassof\ncarbon dioxide (CO2) produced during material extraction, component fabrication, and component\nrecycling.Thisisbecausethebenefitofanelectricvehicleisthatitdoesnotrelease CO2 whileinuse.\nTo simplify the analysis, we had to make some assumptions:\n(1) No CO2 is produced in use andduringdistributionofcomponents,asthecomponentswillbe\nmanufactured in-house.\n(2) Data will be collected from Casting CO2, due to lack of information about CO2 producedin\nadditive manufacturing\n"
      },
      {
        "title": "Content",
        "content": "The LCA can hence be summarized in Figure 13, with additional LCA charts and graphs in\nAppendix A.\nTable 1\n"
      },
      {
        "title": "Material Stage of LCA CO2 per kg (kg/kg) Total CO2 (kg/kg)",
        "content": "Extraction 42.2\n"
      },
      {
        "title": "Aluminium Al Si",
        "content": "Manufacturing 1.16 61.06\n(10 Mg)\nRecycling 17.7\nExtraction 12.5\n"
      },
      {
        "title": "Cobalt Chrome Manufacturing 0.909 16.79",
        "content": "Recycling 3.38\nExtraction 18.3\nInconel 718 Manufacturing 0.996 23.18\nRecycling 3.88\nExtraction 9.58\n"
      },
      {
        "title": "Stainless Steel 17-4",
        "content": "Manufacturing 0.894 12.49\nRecycling 2.02\nExtraction 42.2\nTitanium 6 Al-4 V Manufacturing 1.16 50.53\nRecycling 7.17\nFrom this, wecanconcludethat Stainless Steel17-4 PHhastheleastenvironmentalimpactin\nregards to CO2 produced. However, the different materials used in formgenerationwillusedifferent\nvolumesofmaterial.Hence,weneedtofactorinthevolumeofmaterialusedinordertodeterminethe\nactual total CO2 produced.\nWe also researched material cost for our evaluation to optimize the design from a business\nperspective. The cost of the Aluminium A1 SI (10 Mg), Cobalt Chrome, Inconel 718, Stainless Steel\n17-4 PH, Titanium 6 A1-4 V, and the original Aluminum 7075-T6 are (in SGD per m3) is\n8.08 e3-9.46 e3, 3.88 e5-5.26 e5, 1.47 e5-1.69 e5, 2.81 e4-3.22 e4, 1.2 e5-1.43 e5, and 1.66 e4 - 2 e4\nrespectively (Ansys Granta, 2019, 2). The product of the prices per unit volume and the volume of\nmaterial used will then be used in our evaluation.\n"
      },
      {
        "title": "Additive Manufacturing vs Die Casting",
        "content": "While additive manufacturing is of the future, we took into consideration die casting aswell\nas it was one of the manufacturing methods offered in Fusion360. Using the default material,\naluminum,wegeneratedtwomodelsusingthesetwofabricationmethods,asseenin Appendix B.The\n"
      },
      {
        "title": "Content",
        "content": "results show that the volume of material used in die casting is much greater than in additive\nmanufacturing, due to the precision that it offers ascomparedtodiecasting.Thismeansthatadditive\nmanufacturing will produce less CO2 than die casting as well.\nControl\nTo aid in our evaluation, we set the control to be the original design fabricated using\nAluminum 7075-T6, using milling. A simulation was conducted and the results are in Table 2.\nTable 2\nOriginal rocker design specifications\n"
      },
      {
        "title": "Min Safety Factor 3.459 Mass of component 1323",
        "content": "(kg)\n"
      },
      {
        "title": "Max Displacement 0.03264",
        "content": "(mm)\nStrain 4.843 e-04\nVolume of Material 1.686 e5\n(mm3)\n"
      },
      {
        "title": "Generated Designs",
        "content": "Thus,wegenerated10 modelswith5 differentmaterialsand2 differentpresettedsafetyfactor\n. To aid our evaluation, we extracted simulation data regarding the (A) Safety Factor;(B)Stress;(C)\nDisplacement; (D) Strain. Additionally, fabrication costs such as (E) Volume of material, and\ncomponent specifics such as(F)Mass,wereextracted.Figure5 to9 showstheoutcomesofthemodel\ngeneration, while Appendix C presents the heat maps of the additional data.\n"
      },
      {
        "title": "Aluminium Al Si (10 Mg)",
        "content": "Figure 3 and 4\n"
      },
      {
        "title": "Generated Structural Component EVAM Rocker using Aluminium Al Si (10 Mg)",
        "content": "*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 4\n"
      },
      {
        "title": "Content",
        "content": "A. Minimum Safety Factor: 1.305 A. Minimum Safety Factor: 3.86\nB. Max Stress: 184 MPa B. Max Stress: 62.18 Mpa\nC. Displacement: 0.13 mm C. Displacement: 0.05767 mm\nD. Strain: 0.003632 D. Strain: 0.001543\nE. Volume of Material: 73.597 cm3 E. Volume of Material:116.959 cm3\nF. Mass of component: 196.504 g F. Mass of component: 312.218 g\n"
      },
      {
        "title": "Cobalt Chrome",
        "content": "Figure 5 and 6\n"
      },
      {
        "title": "Generated Structural Component EVAM Rocker using Cobalt Chrome",
        "content": "*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Safety Factor: 4.267 A. Safety Factor: 4.633\nB. Stress: 137.3 MPa B. Stress: 126.5 MPa\nC. Displacement: 0.05363 mm C. Displacement: 0.05359 mm\nD. Strain: 9.804 e-04 D. Strain: 0.00101\nE. Volume of Material: 68.669 cm3 E. Volume of Material:68.648 cm3\nF. Mass of component: 569.269 g F. Mass of component: 569.093\nInconel 718\nFigure 7 and 8\n"
      },
      {
        "title": "Generated Structural Component EVAM Rocker using Inconel 718",
        "content": "*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Safety Factor: 6.204 A. Safety Factor: 4.906\nB. Max Stress: 124.4 Mpa B. Max Stress: 157.4 MPa\n"
      },
      {
        "title": "Content",
        "content": "C. Displacement: 0.06011 mm C. Displacement: 0.06146 mm\nD. Strain: 9.97 e-04 D. Strain: 0.001386\nE. Volume of Material: 68.518 cm3 E. Volume of Material:68.807 cm3\nF. Mass of component: 554.035 g F. Mass of component: 556.374 g\n"
      },
      {
        "title": "Stainless Steel 17-4 PH",
        "content": "Figure 9 and 10\n"
      },
      {
        "title": "Generated Structural Component EVAM Rocker using Stainless Steel 17-4 PH",
        "content": "*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Safety Factor: 4.904 A. Safety Factor: 3.674\nB. Max Stress: 122.4 MPa B. Max Stress: 163.3 MPa\nC. Displacement: 0.05241 mm C. Displacement: 0.05239 mm\nD. Strain: 9.315 e-04 D. Strain: 0.001249\nE. Volume of Material: 68.636 cm3 E. Volume of Material:68.636 cm3\nF. Mass of component: 535.36 g F. Mass of component: 535.36 g\nTitanium 6 Al-4 V\nFigure 11 and 12\n"
      },
      {
        "title": "Generated Structural Component EVAM Rocker using Titanium 6 Al-4 V",
        "content": "*generative design preset safety factor set to 2.5 *generative design preset safety factor set to 2\nA. Minimum Safety Factor: 7.667 A. Minimum Safety Factor: 4.358\nB. Max Stress: 115.1 MPa B. Max Stress: 202.5 MPa\nC. Displacement: 0.09401 mm C. Displacement: 0.1566 mm\n"
      },
      {
        "title": "Content",
        "content": "D. Strain: 0.001387 D. Strain: 0.002449\nE. Volume of Material: 68.645 cm3 E. Volume of Material:68.573 cm3\nF. Mass of component: 304.097 g F. Mass of component: 303.773 g\n"
      },
      {
        "title": "Life Cycle Assessment",
        "content": "With the mass of component generated, the total mass of CO2 can be calculated using\nequation (1):\n𝑡𝑜𝑡𝑎𝑙 𝐶𝑂 𝑝𝑟𝑜𝑑𝑢𝑐𝑒𝑑 = 𝑚𝑎𝑠𝑠 𝑜𝑓 𝑐𝑜𝑚𝑝𝑜𝑛𝑒𝑛𝑡 × 𝐶𝑂 𝑝𝑒𝑟 𝑘𝑔 (1)\nThe calculated data is computed in Table 2 and represented in Figure 8.\nTable 3\nTotal mass of CO2 produced based on mass of rocker component\n"
      },
      {
        "title": "Material Stage of LCA CO2 produced (kg) Total CO2 (kg)",
        "content": "Extraction 13.17\n"
      },
      {
        "title": "Aluminium Al Si",
        "content": "Manufacturing 0.36 19.05\n(10 Mg)\nRecycling 5.52\nExtraction 7.11\n"
      },
      {
        "title": "Cobalt Chrome Manufacturing 0.52 9.55",
        "content": "Recycling 1.92\nExtraction 10.17\nInconel 718 Manufacturing 0.55 12.89\nRecycling 2.16\nExtraction 5.13\n"
      },
      {
        "title": "Stainless Steel 17-4",
        "content": "Manufacturing 0.48 6.69\nRecycling 1.08\nExtraction 12.79\nTitanium 6 Al-4 V Manufacturing 0.35 15.31\nRecycling 2.17\n"
      },
      {
        "title": "Content",
        "content": "Figure 13\n"
      },
      {
        "title": "Evaluation",
        "content": "The generated results can be summarized in Table 4 below:\nTable 4\nSummarized data from generated models with preset safety factor to 2.5 of different materials\n"
      },
      {
        "title": "Al Si Chrome 718 Steel 17-4 6 Al-4 V 7075-T6",
        "content": "(10 Mg) PH\n(Original\ndesign)\nPreset 2.5 2.5 2.5 2.5 2.5 NA\nsafety\nfactor\n"
      },
      {
        "title": "Min Safety 1.305 4.267 6.204 4.904 7.667 3.459",
        "content": "Factor\n"
      },
      {
        "title": "Max Stress 184 137.3 124.4 122.4 115.1 49.85",
        "content": "(MPa)\nMax 0.13 0.05363 0.06011 0.05241 0.09401 0.03264\nDisplaceme\nnt (mm)\n"
      },
      {
        "title": "Content",
        "content": "Strain 0.003632 9.804 e-04 9.97 e-04 9.315 e-04 0.001387 4.843 e-04\nYield 240 586 772 600 882.528 145\nStrength\n(MPa)\nVolume of 73.597 68.669 68.518 68.636 68.645 1.686 e5\nMaterial\n(cm3)\nMaterial 594,663.76 26,643,572 10,072,146 1,928,671.6 8,237,400 2,798,760,\nCost (SGD) 000\nMass (g) 196.504 569.296 554.035 535.36 304.097 1323\nCO2 11.97 9.55 12.84 6.68 15.36 23.63\nProduced\n(kg)\n* Improvement in green and worsened in red, from the original design control\nAccording to the results from fusion 360’s simulation, Aluminium’s design is marginal and\noutside factors could cause it to bend or break. Inconel and Titanium’s design is over engineered\nwhich also means it is too strong for the conditions the design is under, which signals wasting\nmaterials and cost.\nFigure 14 and 15\nExample of Fusion360’s warning after running static stress simulations\n"
      },
      {
        "title": "Further Evaluation",
        "content": "We wanttoexplorehowthegenerateddesignwouldchangeifwevarythepresetsafetyfactor\naccording to the previous results (table 3). We predicted that if wepresetthesafetyfactortoahigher\nvalue, the generated design would turn out to be over engineered and therefore having additional\nunnecessary mass and materials, and if we preset the safety factor to a lower value, the generated\ndesign would turn out to be fragile and would break under external forces.\nTherefore to further evaluate our designs, we tried to find the optimal preset safety factor\nvalue for each material, and generated a new design under changed preset safety factor as shown in\ntable 4.\nThe generated results can be summarized in Table 5 below:\n"
      },
      {
        "title": "Content",
        "content": "Table 5\nSummarized data from generated models with revised preset safety factor of different materials\n"
      },
      {
        "title": "Al Si Chrome 718 Steel 17-4 6 Al-4 V 7075-T6",
        "content": "(10 Mg) PH\n(Original\ndesign)\nPreset 4 2 2 2 1 NA\nsafety\nfactor\n"
      },
      {
        "title": "Min Safety 3.86 4.633 4.906 3.674 4.358 3.459",
        "content": "Factor\n"
      },
      {
        "title": "Max Stress 62.18 126.5 157.4 163.3 202.5 49.85",
        "content": "(MPa)\nMax 0.05767 0.05359 0.06146 0.05239 0.1566 0.03264\nDisplaceme\nnt (mm)\nStrain 0.001543 0.00101 0.001386 0.001249 0.002449 4.843 e-04\nYield 240 586 772 600 882.528 145\nStrength\n(MPa)\nVolume of 116.959 68.648 68.807 68.636 68.573 1.686 e5\nMaterial\n(cm3)\nMaterial 680,012.8 26,635,424 10,114,629 1,928,671.6 8,249,880 2,798,760,\nCost (SGD) 000\nMass (g) 312.218 569.093 556.374 535.36 303.773 1323\nCO2 19.05 9.55 12.89 6.69 15.31 23.63\nProduced\n(kg)\n* Improvement in green and worsened in red, from the original design control\nFrom the second run of generation and simulation, only Aluminium’s simulation appears a\nwarning from fusion 360 that the design is marginal; it will break or bend under external\nconditions. Cobalt’s minimum safety factorhasincreased(worsen)fromthefirstrunofsimulation\nwith preset safety factor of 2.5. Inconel, Stainless Steel , and titanium’s simulation resultisbetter\nthan their first designs (no warning shown from fusion 360). Although each of their masses did\nincrease or remain the same, it is not a huge change (the biggest changeofmassfromtable1 and\ntable 2 is 2.34 g).\n"
      },
      {
        "title": "Content",
        "content": "Therefore we willbeusing Inconel718,Stainless Steel17-4 PH,Aluminium Al Si(10 Mg)and\nTitanium 6 AI-4 V from table 4 and Cobalt Chrome from table 3 in the final comparison.\nTable 6\nFinalized table of the optimal preset safety factor of each material for final comparison.\n"
      },
      {
        "title": "Al Si Chrome 718 Steel 17-4 6 Al-4 V 7075-T6",
        "content": "(10 Mg) PH\n(Original\ndesign)\nPreset 4 2.5 2 2 1 NA\nsafety\nfactor\n"
      },
      {
        "title": "Min Safety 3.86 4.267 4.906 3.674 4.358 3.459",
        "content": "Factor\n"
      },
      {
        "title": "Max Stress 62.18 137.3 157.4 163.3 202.5 49.85",
        "content": "(MPa)\nMax 0.05767 0.05363 0.06146 0.05239 0.1566 0.03264\nDisplaceme\nnt (mm)\nStrain 0.001543 9.804 e-04 0.001386 0.001249 0.002449 4.843 e-04\nYield 240 586 772 600 882.528 145\nStrength\n(MPa)\nVolume of 116.959 68.669 68.807 68.636 68.573 1.686 e5\nMaterial\n(cm3)\nMaterial 680,012.8 26,643,572 10,114,629 1,928,671.6 8,249,880 2,798,760,\nCost (SGD) 000\nMass (g) 312.218 569.296 556.374 535.36 303.773 1323\nCO2 19.05 9.55 12.89 6.69 15.31 23.63\nProduced\n(kg)\n* Improvement in green and worsened in red, from the original design control\nPerspectives\nBecause there are manyvariablesandfactorstotakeintoaccountforouranalysis,wedivided\nour analysis into three perspectives (not ordered by importance): Engineering, Business and\nEnvironment.\n"
      },
      {
        "title": "Engineering Perspective",
        "content": "Engineering is concerned with designing the strongest structure, which we will be taking in\nconsideration the material’s yield strength as well as each material’s generated design’s maximum\nstress data from the static stress simulation done on Fusion360. Material’syieldstrengthtellsushow\nmuch load the material can take before permanent plastic deformation. And maximum stress from\nFusion360’s simulation workspace tells us what isthemaximumstressexperiencedbythedesignata\ncertain area of the design. Therefore if the value ishigher,itwouldmeanthatahighamountofstress\nis focused on one point as all designs face the same loadsandconstraintswhileperformingthestatic\nstress simulation. Which also means that the design is not good at distributing thestressexperienced\nby it.\nTo judge which material design isthestrongest,wewouldwantthatdesign’smaterialtohave\nthe highest yield strength, and the lowest max stress value from Fusion360 as this will give us the\ndesign that is the strongest yet having a good stress distribution. Therefore we havecameupwithan\nequation 𝑌𝑖𝑒𝑙𝑑 𝑆𝑡𝑟𝑒𝑛𝑔𝑡ℎ − 𝑀𝑎𝑥 𝑆𝑡𝑟𝑒𝑠𝑠 in order to judge which design is the best from an\nengineering perspective.\nTable 7\nTable of the results of𝑌𝑖𝑒𝑙𝑑 𝑆𝑡𝑟𝑒𝑛𝑔𝑡ℎ − 𝑀𝑎𝑥 𝑆𝑡𝑟𝑒𝑠𝑠of each material.\n"
      },
      {
        "title": "Al Si Chrome 718 Steel 17-4 6 Al-4 V 7075-T6",
        "content": "(10 Mg) PH\n(Original\ndesign)\nYield 143.71 448.7 614.6 436.7 680.03 95.15\nStrength -\n"
      },
      {
        "title": "Max Stress",
        "content": "(MPa)\nFrom the results in Table 7, Titanium 6 A1-4 V is the best choice of material from an\nengineering lens, as it has the highest high yield strength of 882.528 MPa. Though the lowest\nmaximum stress is 49.85 MPa from Aluminum 7075-T6, it also has the lowest yield strength of 145\nMPa, hence why it will not be the best option of material.\n"
      },
      {
        "title": "Business Perspective",
        "content": "From a business pointofview,wewanttoreducetheoverallcostofthematerial.Thiswillbe\nderived by taking the minimum cost per volume (SGD / m3), multiplied by the volume that was\ngenerated in Fusion360. From the data in Table 6, Aluminum A1 SI (10 Mg) has the lowest overall\nminimum cost at SGD 680,012.80. The next lowest minimum material cost would be Stainless Steel\n17-4 PH which is at SGD 1,928,671.60, a largedifferenceof SGD$1,248,658.20.Withthat,interms\nof minimizing cost, Aluminum A1 SI (10 Mg) is the best option of material.\n"
      },
      {
        "title": "Environment Perspective",
        "content": "We want to reduce the overall CO2 produced by the materials, since increasedproductionof\nCO2 contributes more harm to the environment and people’s health. In Table 6, Stainless Steel 17-4\n"
      },
      {
        "title": "Content",
        "content": "PH produces the least amount of CO2 at 6.69 kg/kg, therefore it will be the material chosen in\nconsideration of the environment.\nRankings\nTo simplify our analysis, we used a ranking system to determine the best material in\nconsideration of all 3 perspectives, asseenin Table8.Weassignedpointstoour6 differentmaterials,\nwith 0 being the lowest and 5 being the highest.\nTable 8\nRanking of materials from engineering, business and environmental perspectives\n"
      },
      {
        "title": "Al Si Chrome 718 Steel 17-4 6 Al-4 V 7075-T6",
        "content": "(10 Mg) PH (Original\ndesign)\nEngineering 1 3 4 2 5 0\nBusiness 5 1 2 4 3 0\nEnvironment 1 4 3 5 2 0\n"
      },
      {
        "title": "Total Points 7 8 9 11 10 0",
        "content": "Recommendations\nWith the rankings presented in Table 8, the best overall option of material will be Stainless\nSteel 17-4 PH, which ranked up the most points. The second best option will be Titanium 6 A1-4 V\nwhich ranked 10 points. Next in line will be Inconel 718 whichranked9 points,then Cobalt Chrome\nwith 8 points, and Aluminum A1 SI (10 Mg) which is at 7 points. Lastly, there is Aluminum7075-T6\nwhich performed the worst in all perspectives given that it has accumulated 0 points total.\n"
      },
      {
        "title": "Though Stainless Steel 17-4 PH was not ranked the highest from an engineering point of",
        "content": "view, in terms of sustainable development, it is the best. Itproducestheleastmassof CO2 emissions\nwhich is incredibly important in the modern world where there are serious implications of pursuing\nharmful practices that contribute to climate change. In developing for the long term where future\ngenerations are taken into consideration, it is the best decision to pursue a practice thatbenefitsboth\nthe environment and business. In this case, because Stainless Steel 17-4 PHisthesecondcheapestin\nterms of cost, it will be greatforthebusinessastheycankeepmanufacturingwiththematerialwhilst\nbeing considerate to the environment and health of people.\nIf we were to proceed with the best performing material in terms of strength, then Titanium\n6 A1-4 V would be the option to go for. However, this will not be a sustainable choice as it is ranked\nthe third lowest in terms of the environment’s perspective. It produces thethirdhighestmassof CO2\nat 15.31 kg, which is a difference of 8.72 kg of CO2 from Stainless Steel 17-4 PH.Andthoughitwill\n"
      },
      {
        "title": "Content",
        "content": "be tough to break and effective product wise, this will only be beneficial short term, and its\nunsustainable practices cannot be carried forward for the long term and future generations.\nSummary of Lessons Learnt\n"
      },
      {
        "title": "1. The use of generative design can help identify new design possibilities that may not have",
        "content": "been considered before.\n"
      },
      {
        "title": "2. In generating designs, we must use preserve and obstacle geometries in order to output",
        "content": "desired outcome.\n"
      },
      {
        "title": "3. Failure to properly define project requirements and constraintscouldresultindesignsthatdo",
        "content": "not meet the necessary specifications. Constraints are needed to achieve the motion or\nbehavior needed.\n"
      },
      {
        "title": "4. Careful consideration should be given to the selection of materials based on their",
        "content": "performance, cost, and sustainability.\n"
      },
      {
        "title": "5. In analyzing the sustainability factor of materials, a life cycle assessment is an appropriate",
        "content": "method in identifying how much CO2 is being produced.\n"
      },
      {
        "title": "6. Testing and evaluation of the generative design outcomes is crucial to ensure that the design",
        "content": "meets all project requirements and constraints.\n"
      },
      {
        "title": "7. An evaluation of the original design mustbeconductedinordertohaveabenchmarkthatthe",
        "content": "generated designs can compare to.\n"
      },
      {
        "title": "8. Continuous improvement and iterationarenecessaryforrefiningthedesignandimprovingits",
        "content": "performance over time.\n"
      },
      {
        "title": "9. Observing safety factors and stress levels is crucial to determine the design's real-life",
        "content": "performance. A design with a low safety factor or high stress may not be reliable, requiring\nmodifications. High values of safety factors can lead to over-engineering.\n"
      },
      {
        "title": "10. There are alwaysmultipleperspectivesinevaluatingtherightmaterialformanufacturing,and",
        "content": "we should never rule outtheotherperspectivesinfavorofone.Andjustbecauseamaterialis\nthe best performing, doesn’t always mean thatit’sthematerialthatshouldbepicked,because\nsustainability has become an important topic for consideration and we should start learning\nhow to build with the future in mind.\nReferences\n"
      },
      {
        "title": "Ansys Granta. (2019).Nickel-chromium alloy, INCONEL.CES 2019 Edupack.",
        "content": "https://www.ansys.com/products/materials\nPowell, D. (2023, January 10).Electric car statistics- EV Data [Update: Jan 23]. heycar. Retrieved\nFebruary 22, 2023, from https://heycar.co.uk/blog/electric-cars-statistics-and-projections\n"
      },
      {
        "title": "Universal Technical Institute. Retrieved February 22, 2023, from",
        "content": "https://www.uti.edu/blog/automotive/car-suspension\n"
      }
    ],
    "metadata": {
      "title": "Generative Design of Electric Vehicle Additive Manufacturing (EVAM) Suspension Rocker",
      "category": "resume",
      "file_name": "EVAM_rocker",
      "relative_path": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/EVAM_rocker.pdf",
      "page_count": 18,
      "project_name": null,
      "file_size": 1032275,
      "last_modified": 1749024948.2689238
    },
    "word_count": 3715,
    "page_count": 18
  },
  {
    "id": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f_byteus",
    "source_file": "/Users/weimingchin/Desktop/weiming_chatbot/data/raw/notion_export/Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/byteus.pdf",
    "type": "pdf",
    "title": "Aditya Kumar 1006300",
    "category": "general_document",
    "raw_content": "\n--- Page 1 ---\nTEAM\nBYTEUS\nAditya Kumar 1006300\nMahima Sharma 1006106\nCaitlin Chiang 1006537\nChin Wei Ming 1006264\n\n--- Page 2 ---\nTeam ByteUs\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nCrawling\nCleaning Parser\nSUTD Docs\nFormatting\nWeb\nChunks Embeddings\nRAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n\n--- Page 3 ---\nTeam ByteUs\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nCrawling\nCleaning\nSUTD Docs\nFormatting\nWeb\nInitial Approach\nTried saving SUTD pages as PDFs\nChange to LangChain\nRecursiveWebLoader\nCrawled pages with JavaScript rendering\nTarget only SUTD Site\nFocus Solely on SUTD Website\n\n--- Page 4 ---\nTeam ByteUs\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nCrawling\nCleaning Parser\nSUTD Docs\nFormatting\nWeb\nChunks Embeddings\nRAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n\n--- Page 5 ---\nTeam ByteUs\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nOne Chunk per page\nParser\nLarge Chunks that exceeds context window\nDocument-based Chunking\nHierarchical chunking using markdown headers\nChunks\nGraph-Based URL Tree\ncombining parent-child sections with similar\ntopics\n\n--- Page 6 ---\nTeam ByteUs\nRAG PIPELINE\nGraph-Based URL Tree\ncombining parent-child sections with similar\ntopics\n\n--- Page 7 ---\nTeam ByteUs\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nOne Chunk per page\nParser\nLarge Chunks that exceeds context window\nDocument-based Chunking\nSemantic Chunking\nHierarchical chunking using markdown headers\nChunks\nChunking based on semantic\nsimilarity.\nchunk_size = 1000\nGraph-Based URL Tree chunk_overlap = 100\nmin_chunk_size = 100\ncombining parent-child sections with similar\ntopics\n\n--- Page 8 ---\nTeam ByteUs\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nCrawling\nCleaning Parser\nSUTD Docs\nFormatting\nWeb\nChunks Embeddings\nRAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n\n--- Page 9 ---\nTeam ByteUs\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nFAISS (Facebook AI Similarity Search)\nA highly efficient vector store with fast query performance and\naccurate similarity matching.\nEmbeddings\nVector Store\n\n--- Page 10 ---\nTeam ByteUs\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nCrawling\nCleaning Parser\nSUTD Docs\nFormatting\nWeb\nChunks Embeddings\nRAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n\n--- Page 11 ---\nTeam ByteUs\nFINETUNING LLMS\nModel Used: gemini-0.2-flash\nGenerate Generate Generate\nSynthetic\nTopics Questions Answers\nQuestion-Answer\n(QA) Pair\nQuery and Answer RAG\nUser\nFinetune Llama 3.2 1B model\nParameter Efficient Finetuning (PEFT)\n\n--- Page 12 ---\nTeam ByteUs\nEVALUATION\n- BLEU SCORE & COSINE SIMILARITY\n\n--- Page 13 ---\nTeam ByteUs\nLLM-AS-A-JUDGE\n- QA EVALUATION\nModel Used: gemini-0.2-flash\n\n--- Page 14 ---\nTeam ByteUs\nLLM-AS-A-JUDGE\n- BATTLES\nModel Used: gemini-0.2-flash\n\n--- Page 15 ---\nTeam ByteUs\nDEMO\n",
    "cleaned_content": "[PAGE BREAK]\nTEAM\nBYTEUS\nAditya Kumar 1006300\nMahima Sharma 1006106\nCaitlin Chiang 1006537\nChin Wei Ming 1006264\n[PAGE BREAK]\nTeam Byte Us\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nCrawling\nCleaning Parser\nSUTD Docs\nFormatting\nChunks Embeddings\nRAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n[PAGE BREAK]\nTeam Byte Us\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nCrawling\nCleaning\nSUTD Docs\nFormatting\nInitial Approach\nTried saving SUTD pages as PDFs\nChange to Lang Chain\nRecursive Web Loader\nCrawled pages with Java Script rendering\nTarget only SUTD Site\nFocus Solely on SUTD Website\n[PAGE BREAK]\nTeam Byte Us\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nCrawling\nCleaning Parser\nSUTD Docs\nFormatting\nChunks Embeddings\nRAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n[PAGE BREAK]\nTeam Byte Us\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nOne Chunk per page\nParser\nLarge Chunks that exceeds context window\nDocument-based Chunking\nHierarchical chunking using markdown headers\nChunks\nGraph-Based URL Tree\ncombining parent-child sections with similar\ntopics\n[PAGE BREAK]\nTeam Byte Us\nRAG PIPELINE\nGraph-Based URL Tree\ncombining parent-child sections with similar\ntopics\n[PAGE BREAK]\nTeam Byte Us\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nOne Chunk per page\nParser\nLarge Chunks that exceeds context window\nDocument-based Chunking\nSemantic Chunking\nHierarchical chunking using markdown headers\nChunks\nChunking based on semantic\nsimilarity.\nchunk_size = 1000\nGraph-Based URL Tree chunk_overlap = 100\nmin_chunk_size = 100\ncombining parent-child sections with similar\ntopics\n[PAGE BREAK]\nTeam Byte Us\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nCrawling\nCleaning Parser\nSUTD Docs\nFormatting\nChunks Embeddings\nRAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n[PAGE BREAK]\nTeam Byte Us\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nFAISS (Facebook AI Similarity Search)\nA highly efficient vector store with fast query performance and\naccurate similarity matching.\nEmbeddings\nVector Store\n[PAGE BREAK]\nTeam Byte Us\nRequirements of the Chatbot\nRAG PIPELINE\nDirect Answer\nSource of the Answer\nFurther Links\nCrawling\nCleaning Parser\nSUTD Docs\nFormatting\nChunks Embeddings\nRAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n[PAGE BREAK]\nTeam Byte Us\nFINETUNING LLMS\nModel Used: gemini-0.2-flash\nGenerate Generate Generate\nSynthetic\nTopics Questions Answers\nQuestion-Answer\n(QA) Pair\nQuery and Answer RAG\nUser\nFinetune Llama 3.2 1 B model\nParameter Efficient Finetuning (PEFT)\n[PAGE BREAK]\nTeam Byte Us\nEVALUATION\n- BLEU SCORE & COSINE SIMILARITY\n[PAGE BREAK]\nTeam Byte Us\nLLM-AS-A-JUDGE\n- QA EVALUATION\nModel Used: gemini-0.2-flash\n[PAGE BREAK]\nTeam Byte Us\nLLM-AS-A-JUDGE\n- BATTLES\nModel Used: gemini-0.2-flash\n[PAGE BREAK]\nTeam Byte Us\nDEMO",
    "sections": [
      {
        "title": "Page 2",
        "content": "TEAM\nBYTEUS\nAditya Kumar 1006300\nMahima Sharma 1006106\nCaitlin Chiang 1006537\nChin Wei Ming 1006264",
        "page_number": 2
      },
      {
        "title": "Team Byte Us",
        "content": "Requirements of the Chatbot\n"
      },
      {
        "title": "Direct Answer",
        "content": "Source of the Answer\n"
      },
      {
        "title": "Further Links",
        "content": "Crawling\n"
      },
      {
        "title": "Cleaning Parser",
        "content": "SUTD Docs\nFormatting\n"
      },
      {
        "title": "Chunks Embeddings",
        "content": "RAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n"
      },
      {
        "title": "Team Byte Us",
        "content": "Requirements of the Chatbot\n"
      },
      {
        "title": "Direct Answer",
        "content": "Source of the Answer\n"
      },
      {
        "title": "Further Links",
        "content": "Crawling\nCleaning\nSUTD Docs\nFormatting\n"
      },
      {
        "title": "Initial Approach",
        "content": "Tried saving SUTD pages as PDFs\nChange to Lang Chain\n"
      },
      {
        "title": "Recursive Web Loader",
        "content": "Crawled pages with Java Script rendering\nTarget only SUTD Site\n"
      },
      {
        "title": "Team Byte Us",
        "content": "Requirements of the Chatbot\n"
      },
      {
        "title": "Direct Answer",
        "content": "Source of the Answer\n"
      },
      {
        "title": "Further Links",
        "content": "Crawling\n"
      },
      {
        "title": "Cleaning Parser",
        "content": "SUTD Docs\nFormatting\n"
      },
      {
        "title": "Chunks Embeddings",
        "content": "RAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n"
      },
      {
        "title": "Team Byte Us",
        "content": "Requirements of the Chatbot\n"
      },
      {
        "title": "Direct Answer",
        "content": "Source of the Answer\n"
      },
      {
        "title": "One Chunk per page",
        "content": "Parser\n"
      },
      {
        "title": "Large Chunks that exceeds context window",
        "content": "Document-based Chunking\nHierarchical chunking using markdown headers\nChunks\nGraph-Based URL Tree\ncombining parent-child sections with similar\ntopics\n"
      },
      {
        "title": "RAG PIPELINE",
        "content": "Graph-Based URL Tree\ncombining parent-child sections with similar\ntopics\n"
      },
      {
        "title": "Team Byte Us",
        "content": "Requirements of the Chatbot\n"
      },
      {
        "title": "Direct Answer",
        "content": "Source of the Answer\n"
      },
      {
        "title": "One Chunk per page",
        "content": "Parser\n"
      },
      {
        "title": "Large Chunks that exceeds context window",
        "content": "Document-based Chunking\n"
      },
      {
        "title": "Semantic Chunking",
        "content": "Hierarchical chunking using markdown headers\nChunks\nChunking based on semantic\nsimilarity.\nchunk_size = 1000\nGraph-Based URL Tree chunk_overlap = 100\nmin_chunk_size = 100\ncombining parent-child sections with similar\ntopics\n"
      },
      {
        "title": "Team Byte Us",
        "content": "Requirements of the Chatbot\n"
      },
      {
        "title": "Direct Answer",
        "content": "Source of the Answer\n"
      },
      {
        "title": "Further Links",
        "content": "Crawling\n"
      },
      {
        "title": "Cleaning Parser",
        "content": "SUTD Docs\nFormatting\n"
      },
      {
        "title": "Chunks Embeddings",
        "content": "RAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n"
      },
      {
        "title": "Team Byte Us",
        "content": "Requirements of the Chatbot\n"
      },
      {
        "title": "Direct Answer",
        "content": "Source of the Answer\n"
      },
      {
        "title": "Further Links",
        "content": "FAISS (Facebook AI Similarity Search)\nA highly efficient vector store with fast query performance and\naccurate similarity matching.\nEmbeddings\n"
      },
      {
        "title": "Team Byte Us",
        "content": "Requirements of the Chatbot\n"
      },
      {
        "title": "Direct Answer",
        "content": "Source of the Answer\n"
      },
      {
        "title": "Further Links",
        "content": "Crawling\n"
      },
      {
        "title": "Cleaning Parser",
        "content": "SUTD Docs\nFormatting\n"
      },
      {
        "title": "Chunks Embeddings",
        "content": "RAG Query and Answer\nUser\nLLM Re Ranker Top 10 most relevant docs Vector Store\n"
      },
      {
        "title": "Generate Generate Generate",
        "content": "Synthetic\n"
      },
      {
        "title": "Topics Questions Answers",
        "content": "Question-Answer\n(QA) Pair\nQuery and Answer RAG\nUser\n"
      },
      {
        "title": "EVALUATION",
        "content": "- BLEU SCORE & COSINE SIMILARITY\n"
      },
      {
        "title": "Team Byte Us",
        "content": "LLM-AS-A-JUDGE\n- QA EVALUATION\n"
      },
      {
        "title": "Team Byte Us",
        "content": "LLM-AS-A-JUDGE\n- BATTLES\n"
      },
      {
        "title": "Page 16",
        "content": "Team Byte Us\nDEMO",
        "page_number": 16
      }
    ],
    "metadata": {
      "title": "Aditya Kumar 1006300",
      "category": "general_document",
      "file_name": "byteus",
      "relative_path": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/byteus.pdf",
      "page_count": 16,
      "project_name": null,
      "file_size": 1108581,
      "last_modified": 1749024949.5785036
    },
    "word_count": 481,
    "page_count": 16
  },
  {
    "id": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f_NLP_final_project_report",
    "source_file": "/Users/weimingchin/Desktop/weiming_chatbot/data/raw/notion_export/Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/NLP_final_project_report.pdf",
    "type": "pdf",
    "title": "Sentiment Analysis Design Challenge",
    "category": "project_report",
    "raw_content": "\n--- Page 1 ---\nSentiment Analysis Design Challenge\nAUTHORS STUDENT ID\nAditya Kumar 1006300\nChin Wei Ming 1006264\nCaitlin Daphne Tan Chiang 1006537\nSingapore University of Technology and Design\n50.040: Natural Language Processing\nProf. Lu Wei\nDecember 13, 2024\n\n--- Page 2 ---\nOverview\nFor the design challenge portion of the project, the team decided to explore a range of models, to find\nout which will produce the highest in the evaluation metrics.\nBefore the experimentations, we had to set the records of the original base metrics of both the TextCNN\nand BiRNN models included in the final_project notebook. With the test set provided to the cohort, we\nutilized this to check our models:\nModel Loss Accuracy Precision Recall F1 Score\nBiRNN 0.3195 0.8648 0.8385 0.9034 0.8697\nTextCNN 1.1734 0.5494 0.5261 0.9888 0.6867\nTable 1: Results of BiRNN in Comparison with the TextCNN Model\nWith these metrics established, the team wanted to explore what could happen if we leverage the\nstrengths of both of these models to then produce an even higher accuracy.\nModels\nEnsemble Model\nThe team decided to analyze if we can leverage the strengths of both models and create an Ensemble\nmodel. We wanted to further break this down explore three types of Ensemble methods:\n(a) A base ensemble that simply combines predictions from sub-models (TextCNN and BiRNN) and\nuses static aggregation methods to produce the final output.\n(b) An ensemble model with added attention mechanism that assigns weights to the sub-model\noutputs. The attention layer provides interpretability by revealing the importance of each\nsub-model's contribution to the final decision.\n(c) The same ensemble attention model as (b) but instead with custom embeddings into the\nframework. This is different from the other models that used the glove embeddings. These\nembeddings, such as positional or domain-specific features, enrich input data representations,\nenabling the ensemble to better align with task-specific characteristics. The attention\nmechanism leverages these enriched embeddings to assign more context-aware weights to the\nsub-models, resulting in more accurate predictions.\nWe have conducted experiments for all three. But the best outcome in the training set came from the\nensemble attention model with custom embeddings. With that, we have decided to focus on testing\nmore with this model.\nThe overall architecture is as follows:\n\n--- Page 3 ---\n1. Custom Embedding Layer: Generates dense vector representations for the input tokens using\na pre-trained Word2Vec model or a similar approach.\n2. Attention Mechanism: Dynamically assigns weights to sub-model outputs by evaluating their\nimportance for the current input.\n3. Weighted Aggregation Layer: Uses the attention weights to combine sub-model outputs into a\nsingle representation.\n4. Linear Layer: Transforms the aggregated representation into logits for the classification task.\n5. Activation Layer: Converts logits into probabilities using softmax or sigmoid for final\npredictions.\nAs the custom embeddings also needed a slightly different version of the data pre-processing pipeline\nas shown in the original final_project notebook, we made the following changes:\nThe pipeline starts by reading and organizing the IMDB dataset, each containing subfolders for positive\nand negative sentiment labels. Text data from these files is read, stripped of whitespace, and stored\nalongside corresponding sentiment labels. To enable custom embeddings, all texts from both the\ntraining and testing datasets undergo tokenization using a basic preprocessing function to generate a\nunified tokenized corpus.\nFor further dataset preparation, the training and testing data are tokenized into word-level sequences\nusing a utility function, and a vocabulary is constructed with a specified minimum word frequency,\nreserving a special token <pad> for padding. Sentences are then transformed into sequences of\nnumerical indices corresponding to the vocabulary, truncated or padded to a fixed length. Labels are\nconverted into binary format for sentiment classification (positive as 1, negative as 0).\nFinally, the data is wrapped in a custom dataset class and divided into batches using PyTorch\nDataLoader, ensuring efficient sampling and shuffling. This comprehensive preprocessing pipeline\nfacilitates the integration of custom embeddings, which are built separately using a Word2Vec model to\ncreate dense vector representations for the vocabulary. Words not found in the embedding model are\ninitialized with random vectors, ensuring a complete embedding matrix for use in the Attention Model.\nTo train the model, we used the following constant configurations:\nweight_decay: 0.00001,\nnum_epochs: 5,\nembedding_dim: 300,\nnum_layers: 2,\nkernel_sizes: [3, 4, 5],\nnum_channels: [100, 100, 100],\nhidden_size: 128,\nvocabulary_size: len(vocab)\nFor experimentation purposes in finding the best configuration for this model in particular, we have\ndecided to hyperparameter tune the following:\nLearning Rate: [0.001, 0.01]\n\n--- Page 4 ---\nBatch Size: [64, 128]\nHidden Dimension: [128, 256]\nLearn Batc Hidd Train Train Test Test Precisi Recall F1 Best Epoc\ning h en Loss Acc Loss Acc on Score Test F1 h\nRate Size Dim Best\n0.001 64 128 0.1821 0.9360 0.3712 0.8647 0.9015 0.8190 0.8582 0.8582 5\n0.001 64 256 0.0634 0.9803 0.4174 0.8694 0.8973 0.8344 0.8647 0.8817 3\n0.001 128 128 0.1345 0.9550 0.3743 0.8570 0.9135 0.7886 0.8465 0.8639 4\n0.001 128 256 0.0870 0.9715 0.3609 0.8827 0.8664 0.9050 0.8853 0.8885 3\n0.01 64 128 0.1889 0.9267 0.3467 0.8543 0.8749 0.8267 0.8502 0.8614 3\n0.01 64 256 0.6832 0.5568 0.7023 0.5176 0.5317 0.2945 0.3790 0.6500 2\nTable 2: Results of Hyperparameter Tuning of Ensemble Attention Model with Custom Embeddings\nThe best result turned out to be the model with the learning rate of 0.001, batch size 128, and hidden\ndimensions of 256. This produced the best F1 score of 0.8885 at epoch 3.\nFigure 1: Result Graphs of the Best Configured Variant of the Ensemble Attention Model\nWith the best model and configuration, it was then further tested with the test set:\nLoss Accuracy Precision Recall F1 Score\n0.2002 0.9267 0.9190 0.9358 0.9273\nTable 3: Test Results of the Best Configured Variant of the Ensemble Attention Model\nThe results show that the model performed better than the original TextCNN and BiRNN models, but we\ndecided that we can explore even better models.\n\n--- Page 5 ---\nRoberta-BiLSTM Model\nThe second idea is to use RoBERTa, to generate deep contextual embeddings by capturing complex\nlanguage patterns. By adding a Bidirectional LSTM (BiLSTM) layer on top of RoBERTa, the model\nprocesses these embeddings sequentially in both directions, enabling it to capture a more\ncomprehensive contextual information across the entire text. To obtain the sentiment prediction, a linear\nlayer is applied to the BiLSTM outputs, mapping the learned representations to the desired sentiment\nclasses.\nThe overall architecture is as follows:\n1. RoBERTa Encoder: The idea was to use RoBERTa as an encoder to generate deep contextual\nembeddings for the input tokens. The encoder takes in the tokenized input IDs and attention\nmasks and processes them through the roberta-base model. This produces a\nlast_hidden_state tensor with rich representations of the input text.\n2. Bidirectional LSTM (BiLSTM) Layer: The output from the encoder was passed to a BiLSTM\nlayer to capture sequential dependencies and contextual information from both directions of the\ntext.\n3. Fully Connected (Linear) Layer: After the BiLSTM layer, a dropout layer is applied to prevent\noverfitting by randomly zeroing some of the features. The output is then averaged across the\nsequence length to create a single vector for each input. This vector is fed into the fully\nconnected layer, which produces logits corresponding to the two sentiment classes (positive and\nnegative).\nThe pre-processing data pipeline has also been changed from the original one provided. The raw movie\nreviews are first collected from the ACL IMDB dataset’s directory structure, separating them into\ntraining and test sets based on their parent folders. Each review is then passed through a\npreprocessing pipeline that converts the text to lowercase, removes URLs and non-alphabetic\ncharacters, and eliminates English stopwords. The remaining terms are lemmatized to reduce them to\ntheir base forms, ensuring a more normalized input representation. Subsequently, these cleaned\nreviews are tokenized with the RoBERTa tokenizer, which encodes the text into subword units, handles\npadding, and truncated sequences to a predefined length. Finally, the binary sentiment labels are\nmapped to 0 for negative and 1 for positive.\nTo train the model, we used the following constant configurations:\nLearning Rate: 0.00001\nBatch Size: 64\nHidden Dimension: 128\nDropout Rate: 0.1\nNumber of LSTM Layers: 1\nNumber of Epochs: 5\nFor experimentation purposes in finding the best configuration for this model in particular, we have\ndecided to hyperparameter tune the following:\n\n--- Page 6 ---\nLearning Rate: [0.001, 0.01]\nBatch Size: [64, 128]\nHidden Dimension: [128, 256]\nLearni Batch Hidde Train Train Test Test Precis Recall F1 Best\nng Size n Dim Loss Acc Loss Acc ion Score Test\nRate F1\n0.0001 64 128 0.6938 0.4973 0.6944 0.5000 0.2500 0.5000 0.3333 0.3333\n0.0001 64 256 0.6935 0.4975 0.6932 0.5000 0.2500 0.5000 0.3333 0.3345\n0.0001 128 128 0.1001 0.9658 0.4156 0.8705 0.8756 0.8705 0.8701 0.8831\n0.0001 128 256 0.0901 0.9679 0.3713 0.8760 0.8763 0.8760 0.8760 0.8854\n0.0000 64 128 0.1275 0.9536 0.2883 0.8975 0.8979 0.8975 0.8975 0.9042\n1\n0.0000 64 256 0.1288 0.9517 0.2941 0.9019 0.9023 0.9019 0.9019 0.9019\n1\n0.0000 128 128 0.1631 0.9374 0.2656 0.9022 0.9023 0.9022 0.9022 0.9022\n1\n0.0000 128 256 0.1621 0.9375 0.2649 0.8994 0.9009 0.8994 0.8993 0.9015\n1\n0.0000 64 128 0.2804 0.8853 0.2754 0.8861 0.8863 0.8861 0.8861 0.8861\n01\n0.0000 64 256 0.2775 0.8860 0.2709 0.8869 0.8873 0.8869 0.8869 0.8869\n01\n0.0000 128 128 0.2994 0.8761 0.2844 0.8822 0.8822 0.8822 0.8822 0.8822\n01\n0.0000 128 256 0.2949 0.8762 0.2809 0.8829 0.8831 0.8829 0.8829 0.8829\n01\nTable 4: Results of Hyperparameter Tuning of RoBERTa-BiLSTM Model\n\n--- Page 7 ---\nFigure 2: Result Graphs of the Best Configured Variant of the RoBERTa-BiLSTM Model\nWith the best model and configuration, it was then further tested with the test set:\nLoss Accuracy Precision Recall F1 Score\n0.1750 0.9392 0.9368 0.9418 0.9393\nTable 5: Test Results of the Best Configured Variant of the RoBERTa-BiLSTM Model\nThe results show that the model performed better than the Ensemble we have built previously. But\nbefore settling on a model, we wanted to explore the possibility of an even better one.\nRoBERTa-large Model\nThe team wanted to explore the leverage that Large Language Models have in sentiment analysis, and\ntherefore pursued transformer-based architectures, specifically focusing on RoBERTa. While our\nprevious RoBERTa-BiLSTM implementation showed promise, we hypothesized that the additional\nBiLSTM layer might be unnecessary given RoBERTa's capability to capture complex contextual\nrelationships.\nGiven our computational resource constraints, we adopted Low-Rank Adaptation (LoRA) to fine-tune\nRoBERTa-large. While full fine-tuning would typically yield optimal results, LoRA offered an efficient\nalternative by updating only a small set of task-specific parameters while keeping the base model\nfrozen. This approach significantly reduced the memory footprint during training while still allowing us to\nleverage RoBERTa-large's full capabilities. Through LoRA, we maintained strong model performance\nwhile achieving greater computational efficiency compared to traditional fine-tuning methods.\n\n--- Page 8 ---\nThe overall architecture is as follows:\nBase Model Architecture (RoBERTa-large)\nRoBERTa comes in three versions: base, large, and small, which differ in size and number of\nparameters. These versions allow flexibility in choosing a model based on computational resources and\ntask complexity. Below is a table detailing the parameters for each version:\nModel Parameters Layers Attention Hidden Size Vocabulary Maximum\n(RoBERTa) Heads Size Sequence\nLength\nLarge 355M 24 16 1024 50,265 512\nBase 125M 12 12 768 50,265 512\nSmall 84M 6 8 768 50,265 512\nTable 6: Parameters of the Various Versions of the RoBERTa Model\nRoBERTa-large was chosen as our base model architecture due to its superior capabilities in handling\ncomplex sentiment analysis tasks. Movie reviews often contain complex and mixed opinions that\nrequire sophisticated language understanding. With 355 million parameters (compared to 125M in base\nand 84M in small versions), RoBERTa-large provides enhanced capability to capture nuanced\nexpressions and contextual relationships in text.\nThe model's architecture offers several key advantages for sentiment analysis. Its deeper layer\nstructure (24 transformer layers versus 12 in base and 6 in small) enables more sophisticated\nprocessing of language hierarchies and better captures complex sentiment relationships across text.\nThe larger hidden size of 1024 dimensions (compared to 768 in smaller versions) provides richer\nrepresentation of sentiment features and increased capacity to store complex emotional patterns.\nAdditionally, with 16 attention heads (versus 12 in base and 8 in small), the model can better process\nmultiple aspects of sentiment simultaneously, making it particularly effective at understanding reviews\nwith mixed or nuanced opinions.\nLoRA (Low-Rank Adaption) Configuration\nLoRA (Low-Rank Adaptation) is an efficient fine-tuning technique that significantly reduces the number\nof trainable parameters in our model. Instead of fine-tuning the entire RoBERTa-large model with its\n355M parameters, LoRA selectively updates specific matrices, effectively reducing both memory usage\nand training time while maintaining strong performance on sentiment analysis tasks. This approach is\nparticularly valuable when working with RoBERTa-large, where traditional full fine-tuning would be\ncomputationally expensive.\nDue to limitation to our computational resource, we could only execute a single training run, below are\nthe LoRA configuration that we trained the model with:\n1. Rank Parameter (r=128) : The rank parameter defines how many dimensions we use in our\nlow-rank matrices. We choose 128 to be our rank parameter to reduce the trainable parameters\nfrom 355M to a manageable size. It provides sufficient complexity to capture sentiment patterns\nwhile maintaining computational efficiency. A higher rank would increase the computational\n\n--- Page 9 ---\ndemands, while a lower rank might not capture the complexity of sentiment expressions\neffectively.\n2. Alpha Parameter (512) : This scaling factor controls the magnitude of LoRA updates during\ntraining. We selected a value of 512 to enable sufficiently big updates to the model’s behaviour.\nHigher values could lead to training instability, while lower values might result in insufficient\nlearning of task-specific patterns.\n3. Dropout Rate (0.05) : We implemented a dropout rate of 0.05 for regularization. This relatively\nlow value maintains model stability while providing sufficient regularization to prevent overfitting,\nensuring the model generalizes well to new movie reviews.\n4. Target Modules ([\"query\", \"key\", \"value\"]) : Our configuration focuses on adapting the\nattention mechanism components - specifically the query, key, and value matrices.\n5. Task Type (SEQ_CLS) : The sequence classification setting aligns with our binary sentiment\nanalysis task of classifying movie reviews as positive or negative.\nThe pre-processing of data is also different from the original one presented in the final_project\nnotebook.\nThe initial data preprocessing pipeline, similar to our base model, applies several fundamental\ntechniques to both training and testing data. First, the text undergoes initial cleaning to standardize\nformat and remove unnecessary whitespace. Next, sentences are broken down into individual words\nthrough tokenization. Common words that don't carry significant meaning are then removed through\nstop word filtering. Finally, lemmatization converts all words to their base forms to reduce vocabulary\ncomplexity.\nAfter these core preprocessing steps, we leverage RoBERTa tokenizers to convert the processed text\ninto numerical embeddings. These embeddings are crucial as they capture both word relationships and\ncontextual information within sentences. RoBERTa's tokenization is particularly well-suited for\nsentiment analysis of movie reviews due to its ability to effectively handle informal language patterns\ncommonly found in user-generated content. It excels at processing emotional expressions and\nintensifiers (such as \"very\", \"really\", \"absolutely\") that are crucial for sentiment understanding. The\ntokenizer also maintains important contextual relationships between words, allowing it to capture subtle\nsentiment indicators that depend on the surrounding text context.\nTo train the model, we used the following constant configurations:\ntraining_batch_size: 16\nevaluation_batch_size: 32\ngradient_accumulation: 4\nlearning_rate: 1e-5\nwarmup_ratio: 0.1\nweight_decay: 0.1\nThe following is the learning process:\n1. Loss and Optimization: The model uses a cross-entropy loss function specifically designed for\nbinary classification tasks.\n\n--- Page 10 ---\n2. Linear learning rate scheduler with warmup: Implements a gradual increase in learning rate\nduring initial training.\n3. Gradient clipping at 1.0: Sets maximum threshold for gradient values to prevent exploding\ngradients.\nFor experimentation purposes in finding the best configuration for this model in particular, we have\ndecided to hyperparameter tune the following:\nThis model is complex and is computationally heavy to train. Therefore, we decided to not perform\nhyperparameter tuning for the model. The following were the results:\nTrain Loss Train Acc Test Loss Test Acc Precision Recall\n0.1799 0.9344 0.1781 0.9411 0.9362 0.9323\nTable 7: Results of Lora-RoBERTa Model with the IMDB Dataset\nFigure 3: Result Graph of Training vs. Evaluation Loss for Lora-RoBERTa Model\nFigure 4: Result Graph of Training vs. Evaluation Accuracy for Lora-RoBERTa Model\n\n--- Page 11 ---\nFigure 5: Result Graph of Evaluation Metrics for Lora-RoBERTa Model\nTo ensure that the model performs just as well with the test set, we have tested it on there as well:\nLoss Accuracy Precision Recall F1 Score\n0.1634 0.9408 0.9425 0.9388 0.9407\nTable 8: Test Results of the Lora-RoBERTa Model\nWith that, the team has decided to choose Lora-RoBERTa as our final model due to the high metrics it\ndisplays in both training and test.\nConclusion\nThe team has showcased a progression starting from the very base BiRNN and TextCNN models, and\nexplored various models that kept improving. We have displayed constant improvement in all the\nmodels that we have experimented with, which demonstrates the effectiveness of iterative\nexperimentation and refinement in achieving superior performance. By systematically evaluating and\ncomparing models, such as BiRNN, TextCNN, and other advanced architectures, we highlighted the\nincremental gains achieved through each upgrade. The progression culminated in the Lora-RoBERTa\nmodel, which excelled in sentiment analysis tasks due to its robust pretraining and fine-tuning\ncapabilities, making it the optimal choice for this application.\n",
    "cleaned_content": "[PAGE BREAK]\nSentiment Analysis Design Challenge\nAUTHORS STUDENT ID\nAditya Kumar 1006300\nChin Wei Ming 1006264\nCaitlin Daphne Tan Chiang 1006537\nSingapore University of Technology and Design\n50.040: Natural Language Processing\nProf. Lu Wei\nDecember 13, 2024\n[PAGE BREAK]\nOverview\nFor the design challenge portion of the project, the team decided to explore a range of models, to find\nout which will produce the highest in the evaluation metrics.\nBefore the experimentations, we had to set the records of the original base metrics of both the Text CNN\nand Bi RNN models included in the final_project notebook. With the test set provided to the cohort, we\nutilized this to check our models:\nModel Loss Accuracy Precision Recall F1 Score\nBi RNN 0.3195 0.8648 0.8385 0.9034 0.8697\nText CNN 1.1734 0.5494 0.5261 0.9888 0.6867\nTable 1: Results of Bi RNN in Comparison with the Text CNN Model\nWith these metrics established, the team wanted to explore what could happen if we leverage the\nstrengths of both of these models to then produce an even higher accuracy.\nModels\nEnsemble Model\nThe team decided to analyze if we can leverage the strengths of both models and create an Ensemble\nmodel. We wanted to further break this down explore three types of Ensemble methods:\n(a) A base ensemble that simply combines predictions from sub-models (Text CNN and Bi RNN) and\nuses static aggregation methods to produce the final output.\n(b) An ensemble model with added attention mechanism that assigns weights to the sub-model\noutputs. The attention layer provides interpretability by revealing the importance of each\nsub-model's contribution to the final decision.\n(c) The same ensemble attention model as (b) but instead with custom embeddings into the\nframework. This is different from the other models that used the glove embeddings. These\nembeddings, such as positional or domain-specific features, enrich input data representations,\nenabling the ensemble to better align with task-specific characteristics. The attention\nmechanism leverages these enriched embeddings to assign more context-aware weights to the\nsub-models, resulting in more accurate predictions.\nWe have conducted experiments for all three. But the best outcome in the training set came from the\nensemble attention model with custom embeddings. With that, we have decided to focus on testing\nmore with this model.\nThe overall architecture is as follows:\n[PAGE BREAK]\n1. Custom Embedding Layer: Generates dense vector representations for the input tokens using\na pre-trained Word2 Vec model or a similar approach.\n2. Attention Mechanism: Dynamically assigns weights to sub-model outputs by evaluating their\nimportance for the current input.\n3. Weighted Aggregation Layer: Uses the attention weights to combine sub-model outputs into a\nsingle representation.\n4. Linear Layer: Transforms the aggregated representation into logits for the classification task.\n5. Activation Layer: Converts logits into probabilities using softmax or sigmoid for final\npredictions.\nAs the custom embeddings also needed a slightly different version of the data pre-processing pipeline\nas shown in the original final_project notebook, we made the following changes:\nThe pipeline starts by reading and organizing the IMDB dataset, each containing subfolders for positive\nand negative sentiment labels. Text data from these files is read, stripped of whitespace, and stored\nalongside corresponding sentiment labels. To enable custom embeddings, all texts from both the\ntraining and testing datasets undergo tokenization using a basic preprocessing function to generate a\nunified tokenized corpus.\nFor further dataset preparation, the training and testing data are tokenized into word-level sequences\nusing a utility function, and a vocabulary is constructed with a specified minimum word frequency,\nreserving a special token <pad> for padding. Sentences are then transformed into sequences of\nnumerical indices corresponding to the vocabulary, truncated or padded to a fixed length. Labels are\nconverted into binary format for sentiment classification (positive as 1, negative as 0).\nFinally, the data is wrapped in a custom dataset class and divided into batches using Py Torch\nData Loader, ensuring efficient sampling and shuffling. This comprehensive preprocessing pipeline\nfacilitates the integration of custom embeddings, which are built separately using a Word2 Vec model to\ncreate dense vector representations for the vocabulary. Words not found in the embedding model are\ninitialized with random vectors, ensuring a complete embedding matrix for use in the Attention Model.\nTo train the model, we used the following constant configurations:\nweight_decay: 0.00001,\nnum_epochs: 5,\nembedding_dim: 300,\nnum_layers: 2,\nkernel_sizes: [3, 4, 5],\nnum_channels: [100, 100, 100],\nhidden_size: 128,\nvocabulary_size: len(vocab)\nFor experimentation purposes in finding the best configuration for this model in particular, we have\ndecided to hyperparameter tune the following:\nLearning Rate: [0.001, 0.01]\n[PAGE BREAK]\nBatch Size: [64, 128]\nHidden Dimension: [128, 256]\nLearn Batc Hidd Train Train Test Test Precisi Recall F1 Best Epoc\ning h en Loss Acc Loss Acc on Score Test F1 h\nRate Size Dim Best\n0.001 64 128 0.1821 0.9360 0.3712 0.8647 0.9015 0.8190 0.8582 0.8582 5\n0.001 64 256 0.0634 0.9803 0.4174 0.8694 0.8973 0.8344 0.8647 0.8817 3\n0.001 128 128 0.1345 0.9550 0.3743 0.8570 0.9135 0.7886 0.8465 0.8639 4\n0.001 128 256 0.0870 0.9715 0.3609 0.8827 0.8664 0.9050 0.8853 0.8885 3\n0.01 64 128 0.1889 0.9267 0.3467 0.8543 0.8749 0.8267 0.8502 0.8614 3\n0.01 64 256 0.6832 0.5568 0.7023 0.5176 0.5317 0.2945 0.3790 0.6500 2\nTable 2: Results of Hyperparameter Tuning of Ensemble Attention Model with Custom Embeddings\nThe best result turned out to be the model with the learning rate of 0.001, batch size 128, and hidden\ndimensions of 256. This produced the best F1 score of 0.8885 at epoch 3.\nFigure 1: Result Graphs of the Best Configured Variant of the Ensemble Attention Model\nWith the best model and configuration, it was then further tested with the test set:\nLoss Accuracy Precision Recall F1 Score\n0.2002 0.9267 0.9190 0.9358 0.9273\nTable 3: Test Results of the Best Configured Variant of the Ensemble Attention Model\nThe results show that the model performed better than the original Text CNN and Bi RNN models, but we\ndecided that we can explore even better models.\n[PAGE BREAK]\nRoberta-Bi LSTM Model\nThe second idea is to use Ro BERTa, to generate deep contextual embeddings by capturing complex\nlanguage patterns. By adding a Bidirectional LSTM (Bi LSTM) layer on top of Ro BERTa, the model\nprocesses these embeddings sequentially in both directions, enabling it to capture a more\ncomprehensive contextual information across the entire text. To obtain the sentiment prediction, a linear\nlayer is applied to the Bi LSTM outputs, mapping the learned representations to the desired sentiment\nclasses.\nThe overall architecture is as follows:\n1. Ro BERTa Encoder: The idea was to use Ro BERTa as an encoder to generate deep contextual\nembeddings for the input tokens. The encoder takes in the tokenized input IDs and attention\nmasks and processes them through the roberta-base model. This produces a\nlast_hidden_state tensor with rich representations of the input text.\n2. Bidirectional LSTM (Bi LSTM) Layer: The output from the encoder was passed to a Bi LSTM\nlayer to capture sequential dependencies and contextual information from both directions of the\ntext.\n3. Fully Connected (Linear) Layer: After the Bi LSTM layer, a dropout layer is applied to prevent\noverfitting by randomly zeroing some of the features. The output is then averaged across the\nsequence length to create a single vector for each input. This vector is fed into the fully\nconnected layer, which produces logits corresponding to the two sentiment classes (positive and\nnegative).\nThe pre-processing data pipeline has also been changed from the original one provided. The raw movie\nreviews are first collected from the ACL IMDB dataset’s directory structure, separating them into\ntraining and test sets based on their parent folders. Each review is then passed through a\npreprocessing pipeline that converts the text to lowercase, removes URLs and non-alphabetic\ncharacters, and eliminates English stopwords. The remaining terms are lemmatized to reduce them to\ntheir base forms, ensuring a more normalized input representation. Subsequently, these cleaned\nreviews are tokenized with the Ro BERTa tokenizer, which encodes the text into subword units, handles\npadding, and truncated sequences to a predefined length. Finally, the binary sentiment labels are\nmapped to 0 for negative and 1 for positive.\nTo train the model, we used the following constant configurations:\nLearning Rate: 0.00001\nBatch Size: 64\nHidden Dimension: 128\nDropout Rate: 0.1\nNumber of LSTM Layers: 1\nNumber of Epochs: 5\nFor experimentation purposes in finding the best configuration for this model in particular, we have\ndecided to hyperparameter tune the following:\n[PAGE BREAK]\nLearning Rate: [0.001, 0.01]\nBatch Size: [64, 128]\nHidden Dimension: [128, 256]\nLearni Batch Hidde Train Train Test Test Precis Recall F1 Best\nng Size n Dim Loss Acc Loss Acc ion Score Test\nRate F1\n0.0001 64 128 0.6938 0.4973 0.6944 0.5000 0.2500 0.5000 0.3333 0.3333\n0.0001 64 256 0.6935 0.4975 0.6932 0.5000 0.2500 0.5000 0.3333 0.3345\n0.0001 128 128 0.1001 0.9658 0.4156 0.8705 0.8756 0.8705 0.8701 0.8831\n0.0001 128 256 0.0901 0.9679 0.3713 0.8760 0.8763 0.8760 0.8760 0.8854\n0.0000 64 128 0.1275 0.9536 0.2883 0.8975 0.8979 0.8975 0.8975 0.9042\n0.0000 64 256 0.1288 0.9517 0.2941 0.9019 0.9023 0.9019 0.9019 0.9019\n0.0000 128 128 0.1631 0.9374 0.2656 0.9022 0.9023 0.9022 0.9022 0.9022\n0.0000 128 256 0.1621 0.9375 0.2649 0.8994 0.9009 0.8994 0.8993 0.9015\n0.0000 64 128 0.2804 0.8853 0.2754 0.8861 0.8863 0.8861 0.8861 0.8861\n0.0000 64 256 0.2775 0.8860 0.2709 0.8869 0.8873 0.8869 0.8869 0.8869\n0.0000 128 128 0.2994 0.8761 0.2844 0.8822 0.8822 0.8822 0.8822 0.8822\n0.0000 128 256 0.2949 0.8762 0.2809 0.8829 0.8831 0.8829 0.8829 0.8829\nTable 4: Results of Hyperparameter Tuning of Ro BERTa-Bi LSTM Model\n[PAGE BREAK]\nFigure 2: Result Graphs of the Best Configured Variant of the Ro BERTa-Bi LSTM Model\nWith the best model and configuration, it was then further tested with the test set:\nLoss Accuracy Precision Recall F1 Score\n0.1750 0.9392 0.9368 0.9418 0.9393\nTable 5: Test Results of the Best Configured Variant of the Ro BERTa-Bi LSTM Model\nThe results show that the model performed better than the Ensemble we have built previously. But\nbefore settling on a model, we wanted to explore the possibility of an even better one.\nRo BERTa-large Model\nThe team wanted to explore the leverage that Large Language Models have in sentiment analysis, and\ntherefore pursued transformer-based architectures, specifically focusing on Ro BERTa. While our\nprevious Ro BERTa-Bi LSTM implementation showed promise, we hypothesized that the additional\nBi LSTM layer might be unnecessary given Ro BERTa's capability to capture complex contextual\nrelationships.\nGiven our computational resource constraints, we adopted Low-Rank Adaptation (Lo RA) to fine-tune\nRo BERTa-large. While full fine-tuning would typically yield optimal results, Lo RA offered an efficient\nalternative by updating only a small set of task-specific parameters while keeping the base model\nfrozen. This approach significantly reduced the memory footprint during training while still allowing us to\nleverage Ro BERTa-large's full capabilities. Through Lo RA, we maintained strong model performance\nwhile achieving greater computational efficiency compared to traditional fine-tuning methods.\n[PAGE BREAK]\nThe overall architecture is as follows:\nBase Model Architecture (Ro BERTa-large)\nRo BERTa comes in three versions: base, large, and small, which differ in size and number of\nparameters. These versions allow flexibility in choosing a model based on computational resources and\ntask complexity. Below is a table detailing the parameters for each version:\nModel Parameters Layers Attention Hidden Size Vocabulary Maximum\n(Ro BERTa) Heads Size Sequence\nLength\nLarge 355 M 24 16 1024 50,265 512\nBase 125 M 12 12 768 50,265 512\nSmall 84 M 6 8 768 50,265 512\nTable 6: Parameters of the Various Versions of the Ro BERTa Model\nRo BERTa-large was chosen as our base model architecture due to its superior capabilities in handling\ncomplex sentiment analysis tasks. Movie reviews often contain complex and mixed opinions that\nrequire sophisticated language understanding. With 355 million parameters (compared to 125 M in base\nand 84 M in small versions), Ro BERTa-large provides enhanced capability to capture nuanced\nexpressions and contextual relationships in text.\nThe model's architecture offers several key advantages for sentiment analysis. Its deeper layer\nstructure (24 transformer layers versus 12 in base and 6 in small) enables more sophisticated\nprocessing of language hierarchies and better captures complex sentiment relationships across text.\nThe larger hidden size of 1024 dimensions (compared to 768 in smaller versions) provides richer\nrepresentation of sentiment features and increased capacity to store complex emotional patterns.\nAdditionally, with 16 attention heads (versus 12 in base and 8 in small), the model can better process\nmultiple aspects of sentiment simultaneously, making it particularly effective at understanding reviews\nwith mixed or nuanced opinions.\nLo RA (Low-Rank Adaption) Configuration\nLo RA (Low-Rank Adaptation) is an efficient fine-tuning technique that significantly reduces the number\nof trainable parameters in our model. Instead of fine-tuning the entire Ro BERTa-large model with its\n355 M parameters, Lo RA selectively updates specific matrices, effectively reducing both memory usage\nand training time while maintaining strong performance on sentiment analysis tasks. This approach is\nparticularly valuable when working with Ro BERTa-large, where traditional full fine-tuning would be\ncomputationally expensive.\nDue to limitation to our computational resource, we could only execute a single training run, below are\nthe Lo RA configuration that we trained the model with:\n1. Rank Parameter (r=128) : The rank parameter defines how many dimensions we use in our\nlow-rank matrices. We choose 128 to be our rank parameter to reduce the trainable parameters\nfrom 355 M to a manageable size. It provides sufficient complexity to capture sentiment patterns\nwhile maintaining computational efficiency. A higher rank would increase the computational\n[PAGE BREAK]\ndemands, while a lower rank might not capture the complexity of sentiment expressions\neffectively.\n2. Alpha Parameter (512) : This scaling factor controls the magnitude of Lo RA updates during\ntraining. We selected a value of 512 to enable sufficiently big updates to the model’s behaviour.\nHigher values could lead to training instability, while lower values might result in insufficient\nlearning of task-specific patterns.\n3. Dropout Rate (0.05) : We implemented a dropout rate of 0.05 for regularization. This relatively\nlow value maintains model stability while providing sufficient regularization to prevent overfitting,\nensuring the model generalizes well to new movie reviews.\n4. Target Modules ([\"query\", \"key\", \"value\"]) : Our configuration focuses on adapting the\nattention mechanism components - specifically the query, key, and value matrices.\n5. Task Type (SEQ_CLS) : The sequence classification setting aligns with our binary sentiment\nanalysis task of classifying movie reviews as positive or negative.\nThe pre-processing of data is also different from the original one presented in the final_project\nnotebook.\nThe initial data preprocessing pipeline, similar to our base model, applies several fundamental\ntechniques to both training and testing data. First, the text undergoes initial cleaning to standardize\nformat and remove unnecessary whitespace. Next, sentences are broken down into individual words\nthrough tokenization. Common words that don't carry significant meaning are then removed through\nstop word filtering. Finally, lemmatization converts all words to their base forms to reduce vocabulary\ncomplexity.\nAfter these core preprocessing steps, we leverage Ro BERTa tokenizers to convert the processed text\ninto numerical embeddings. These embeddings are crucial as they capture both word relationships and\ncontextual information within sentences. Ro BERTa's tokenization is particularly well-suited for\nsentiment analysis of movie reviews due to its ability to effectively handle informal language patterns\ncommonly found in user-generated content. It excels at processing emotional expressions and\nintensifiers (such as \"very\", \"really\", \"absolutely\") that are crucial for sentiment understanding. The\ntokenizer also maintains important contextual relationships between words, allowing it to capture subtle\nsentiment indicators that depend on the surrounding text context.\nTo train the model, we used the following constant configurations:\ntraining_batch_size: 16\nevaluation_batch_size: 32\ngradient_accumulation: 4\nlearning_rate: 1 e-5\nwarmup_ratio: 0.1\nweight_decay: 0.1\nThe following is the learning process:\n1. Loss and Optimization: The model uses a cross-entropy loss function specifically designed for\nbinary classification tasks.\n[PAGE BREAK]\n2. Linear learning rate scheduler with warmup: Implements a gradual increase in learning rate\nduring initial training.\n3. Gradient clipping at 1.0: Sets maximum threshold for gradient values to prevent exploding\ngradients.\nFor experimentation purposes in finding the best configuration for this model in particular, we have\ndecided to hyperparameter tune the following:\nThis model is complex and is computationally heavy to train. Therefore, we decided to not perform\nhyperparameter tuning for the model. The following were the results:\nTrain Loss Train Acc Test Loss Test Acc Precision Recall\n0.1799 0.9344 0.1781 0.9411 0.9362 0.9323\nTable 7: Results of Lora-Ro BERTa Model with the IMDB Dataset\nFigure 3: Result Graph of Training vs. Evaluation Loss for Lora-Ro BERTa Model\nFigure 4: Result Graph of Training vs. Evaluation Accuracy for Lora-Ro BERTa Model\n[PAGE BREAK]\nFigure 5: Result Graph of Evaluation Metrics for Lora-Ro BERTa Model\nTo ensure that the model performs just as well with the test set, we have tested it on there as well:\nLoss Accuracy Precision Recall F1 Score\n0.1634 0.9408 0.9425 0.9388 0.9407\nTable 8: Test Results of the Lora-Ro BERTa Model\nWith that, the team has decided to choose Lora-Ro BERTa as our final model due to the high metrics it\ndisplays in both training and test.\nConclusion\nThe team has showcased a progression starting from the very base Bi RNN and Text CNN models, and\nexplored various models that kept improving. We have displayed constant improvement in all the\nmodels that we have experimented with, which demonstrates the effectiveness of iterative\nexperimentation and refinement in achieving superior performance. By systematically evaluating and\ncomparing models, such as Bi RNN, Text CNN, and other advanced architectures, we highlighted the\nincremental gains achieved through each upgrade. The progression culminated in the Lora-Ro BERTa\nmodel, which excelled in sentiment analysis tasks due to its robust pretraining and fine-tuning\ncapabilities, making it the optimal choice for this application.",
    "sections": [
      {
        "title": "Singapore University of Technology and Design",
        "content": "50.040: Natural Language Processing\nProf. Lu Wei\nDecember 13, 2024\n"
      },
      {
        "title": "Content",
        "content": "Overview\nFor the design challenge portion of the project, the team decided to explore a range of models, to find\nout which will produce the highest in the evaluation metrics.\nBefore the experimentations, we had to set the records of the original base metrics of both the Text CNN\nand Bi RNN models included in the final_project notebook. With the test set provided to the cohort, we\nutilized this to check our models:\n"
      },
      {
        "title": "Model Loss Accuracy Precision Recall F1 Score",
        "content": "Bi RNN 0.3195 0.8648 0.8385 0.9034 0.8697\nText CNN 1.1734 0.5494 0.5261 0.9888 0.6867\nTable 1: Results of Bi RNN in Comparison with the Text CNN Model\nWith these metrics established, the team wanted to explore what could happen if we leverage the\nstrengths of both of these models to then produce an even higher accuracy.\nModels\n"
      },
      {
        "title": "Ensemble Model",
        "content": "The team decided to analyze if we can leverage the strengths of both models and create an Ensemble\nmodel. We wanted to further break this down explore three types of Ensemble methods:\n(a) A base ensemble that simply combines predictions from sub-models (Text CNN and Bi RNN) and\nuses static aggregation methods to produce the final output.\n(b) An ensemble model with added attention mechanism that assigns weights to the sub-model\noutputs. The attention layer provides interpretability by revealing the importance of each\nsub-model's contribution to the final decision.\n(c) The same ensemble attention model as (b) but instead with custom embeddings into the\nframework. This is different from the other models that used the glove embeddings. These\nembeddings, such as positional or domain-specific features, enrich input data representations,\nenabling the ensemble to better align with task-specific characteristics. The attention\nmechanism leverages these enriched embeddings to assign more context-aware weights to the\nsub-models, resulting in more accurate predictions.\nWe have conducted experiments for all three. But the best outcome in the training set came from the\nensemble attention model with custom embeddings. With that, we have decided to focus on testing\nmore with this model.\nThe overall architecture is as follows:\n"
      },
      {
        "title": "1. Custom Embedding Layer: Generates dense vector representations for the input tokens using",
        "content": "a pre-trained Word2 Vec model or a similar approach.\n"
      },
      {
        "title": "2. Attention Mechanism: Dynamically assigns weights to sub-model outputs by evaluating their",
        "content": "importance for the current input.\n"
      },
      {
        "title": "3. Weighted Aggregation Layer: Uses the attention weights to combine sub-model outputs into a",
        "content": "single representation.\n"
      },
      {
        "title": "5. Activation Layer: Converts logits into probabilities using softmax or sigmoid for final",
        "content": "predictions.\nAs the custom embeddings also needed a slightly different version of the data pre-processing pipeline\nas shown in the original final_project notebook, we made the following changes:\nThe pipeline starts by reading and organizing the IMDB dataset, each containing subfolders for positive\nand negative sentiment labels. Text data from these files is read, stripped of whitespace, and stored\nalongside corresponding sentiment labels. To enable custom embeddings, all texts from both the\ntraining and testing datasets undergo tokenization using a basic preprocessing function to generate a\nunified tokenized corpus.\nFor further dataset preparation, the training and testing data are tokenized into word-level sequences\nusing a utility function, and a vocabulary is constructed with a specified minimum word frequency,\nreserving a special token <pad> for padding. Sentences are then transformed into sequences of\nnumerical indices corresponding to the vocabulary, truncated or padded to a fixed length. Labels are\nconverted into binary format for sentiment classification (positive as 1, negative as 0).\nFinally, the data is wrapped in a custom dataset class and divided into batches using Py Torch\n"
      },
      {
        "title": "Data Loader, ensuring efficient sampling and shuffling. This comprehensive preprocessing pipeline",
        "content": "facilitates the integration of custom embeddings, which are built separately using a Word2 Vec model to\ncreate dense vector representations for the vocabulary. Words not found in the embedding model are\ninitialized with random vectors, ensuring a complete embedding matrix for use in the Attention Model.\nTo train the model, we used the following constant configurations:\nweight_decay: 0.00001,\nnum_epochs: 5,\nembedding_dim: 300,\nnum_layers: 2,\nkernel_sizes: [3, 4, 5],\nnum_channels: [100, 100, 100],\nhidden_size: 128,\nvocabulary_size: len(vocab)\nFor experimentation purposes in finding the best configuration for this model in particular, we have\ndecided to hyperparameter tune the following:\n"
      },
      {
        "title": "Learn Batc Hidd Train Train Test Test Precisi Recall F1 Best Epoc",
        "content": "ing h en Loss Acc Loss Acc on Score Test F1 h\n"
      },
      {
        "title": "Rate Size Dim Best",
        "content": "0.001 64 128 0.1821 0.9360 0.3712 0.8647 0.9015 0.8190 0.8582 0.8582 5\n0.001 64 256 0.0634 0.9803 0.4174 0.8694 0.8973 0.8344 0.8647 0.8817 3\n0.001 128 128 0.1345 0.9550 0.3743 0.8570 0.9135 0.7886 0.8465 0.8639 4\n0.001 128 256 0.0870 0.9715 0.3609 0.8827 0.8664 0.9050 0.8853 0.8885 3\n0.01 64 128 0.1889 0.9267 0.3467 0.8543 0.8749 0.8267 0.8502 0.8614 3\n0.01 64 256 0.6832 0.5568 0.7023 0.5176 0.5317 0.2945 0.3790 0.6500 2\nTable 2: Results of Hyperparameter Tuning of Ensemble Attention Model with Custom Embeddings\nThe best result turned out to be the model with the learning rate of 0.001, batch size 128, and hidden\ndimensions of 256. This produced the best F1 score of 0.8885 at epoch 3.\nFigure 1: Result Graphs of the Best Configured Variant of the Ensemble Attention Model\nWith the best model and configuration, it was then further tested with the test set:\n"
      },
      {
        "title": "Loss Accuracy Precision Recall F1 Score",
        "content": "0.2002 0.9267 0.9190 0.9358 0.9273\nTable 3: Test Results of the Best Configured Variant of the Ensemble Attention Model\nThe results show that the model performed better than the original Text CNN and Bi RNN models, but we\ndecided that we can explore even better models.\n"
      },
      {
        "title": "Content",
        "content": "Roberta-Bi LSTM Model\nThe second idea is to use Ro BERTa, to generate deep contextual embeddings by capturing complex\nlanguage patterns. By adding a Bidirectional LSTM (Bi LSTM) layer on top of Ro BERTa, the model\nprocesses these embeddings sequentially in both directions, enabling it to capture a more\ncomprehensive contextual information across the entire text. To obtain the sentiment prediction, a linear\nlayer is applied to the Bi LSTM outputs, mapping the learned representations to the desired sentiment\nclasses.\nThe overall architecture is as follows:\n"
      },
      {
        "title": "1. Ro BERTa Encoder: The idea was to use Ro BERTa as an encoder to generate deep contextual",
        "content": "embeddings for the input tokens. The encoder takes in the tokenized input IDs and attention\nmasks and processes them through the roberta-base model. This produces a\nlast_hidden_state tensor with rich representations of the input text.\n"
      },
      {
        "title": "2. Bidirectional LSTM (Bi LSTM) Layer: The output from the encoder was passed to a Bi LSTM",
        "content": "layer to capture sequential dependencies and contextual information from both directions of the\ntext.\n"
      },
      {
        "title": "3. Fully Connected (Linear) Layer: After the Bi LSTM layer, a dropout layer is applied to prevent",
        "content": "overfitting by randomly zeroing some of the features. The output is then averaged across the\nsequence length to create a single vector for each input. This vector is fed into the fully\nconnected layer, which produces logits corresponding to the two sentiment classes (positive and\nnegative).\nThe pre-processing data pipeline has also been changed from the original one provided. The raw movie\nreviews are first collected from the ACL IMDB dataset’s directory structure, separating them into\ntraining and test sets based on their parent folders. Each review is then passed through a\npreprocessing pipeline that converts the text to lowercase, removes URLs and non-alphabetic\ncharacters, and eliminates English stopwords. The remaining terms are lemmatized to reduce them to\ntheir base forms, ensuring a more normalized input representation. Subsequently, these cleaned\nreviews are tokenized with the Ro BERTa tokenizer, which encodes the text into subword units, handles\npadding, and truncated sequences to a predefined length. Finally, the binary sentiment labels are\nmapped to 0 for negative and 1 for positive.\nTo train the model, we used the following constant configurations:\n"
      },
      {
        "title": "Dropout Rate: 0.1",
        "content": "Number of LSTM Layers: 1\nNumber of Epochs: 5\nFor experimentation purposes in finding the best configuration for this model in particular, we have\ndecided to hyperparameter tune the following:\n"
      },
      {
        "title": "Learni Batch Hidde Train Train Test Test Precis Recall F1 Best",
        "content": "ng Size n Dim Loss Acc Loss Acc ion Score Test\nRate F1\n0.0001 64 128 0.6938 0.4973 0.6944 0.5000 0.2500 0.5000 0.3333 0.3333\n0.0001 64 256 0.6935 0.4975 0.6932 0.5000 0.2500 0.5000 0.3333 0.3345\n0.0001 128 128 0.1001 0.9658 0.4156 0.8705 0.8756 0.8705 0.8701 0.8831\n0.0001 128 256 0.0901 0.9679 0.3713 0.8760 0.8763 0.8760 0.8760 0.8854\n0.0000 64 128 0.1275 0.9536 0.2883 0.8975 0.8979 0.8975 0.8975 0.9042\n0.0000 64 256 0.1288 0.9517 0.2941 0.9019 0.9023 0.9019 0.9019 0.9019\n0.0000 128 128 0.1631 0.9374 0.2656 0.9022 0.9023 0.9022 0.9022 0.9022\n0.0000 128 256 0.1621 0.9375 0.2649 0.8994 0.9009 0.8994 0.8993 0.9015\n0.0000 64 128 0.2804 0.8853 0.2754 0.8861 0.8863 0.8861 0.8861 0.8861\n0.0000 64 256 0.2775 0.8860 0.2709 0.8869 0.8873 0.8869 0.8869 0.8869\n0.0000 128 128 0.2994 0.8761 0.2844 0.8822 0.8822 0.8822 0.8822 0.8822\n0.0000 128 256 0.2949 0.8762 0.2809 0.8829 0.8831 0.8829 0.8829 0.8829\nTable 4: Results of Hyperparameter Tuning of Ro BERTa-Bi LSTM Model\n"
      },
      {
        "title": "Content",
        "content": "Figure 2: Result Graphs of the Best Configured Variant of the Ro BERTa-Bi LSTM Model\nWith the best model and configuration, it was then further tested with the test set:\n"
      },
      {
        "title": "Loss Accuracy Precision Recall F1 Score",
        "content": "0.1750 0.9392 0.9368 0.9418 0.9393\nTable 5: Test Results of the Best Configured Variant of the Ro BERTa-Bi LSTM Model\nThe results show that the model performed better than the Ensemble we have built previously. But\nbefore settling on a model, we wanted to explore the possibility of an even better one.\nRo BERTa-large Model\nThe team wanted to explore the leverage that Large Language Models have in sentiment analysis, and\ntherefore pursued transformer-based architectures, specifically focusing on Ro BERTa. While our\nprevious Ro BERTa-Bi LSTM implementation showed promise, we hypothesized that the additional\nBi LSTM layer might be unnecessary given Ro BERTa's capability to capture complex contextual\nrelationships.\nGiven our computational resource constraints, we adopted Low-Rank Adaptation (Lo RA) to fine-tune\nRo BERTa-large. While full fine-tuning would typically yield optimal results, Lo RA offered an efficient\nalternative by updating only a small set of task-specific parameters while keeping the base model\nfrozen. This approach significantly reduced the memory footprint during training while still allowing us to\nleverage Ro BERTa-large's full capabilities. Through Lo RA, we maintained strong model performance\nwhile achieving greater computational efficiency compared to traditional fine-tuning methods.\n"
      },
      {
        "title": "Content",
        "content": "The overall architecture is as follows:\n"
      },
      {
        "title": "Base Model Architecture (Ro BERTa-large)",
        "content": "Ro BERTa comes in three versions: base, large, and small, which differ in size and number of\nparameters. These versions allow flexibility in choosing a model based on computational resources and\ntask complexity. Below is a table detailing the parameters for each version:\n"
      },
      {
        "title": "Model Parameters Layers Attention Hidden Size Vocabulary Maximum",
        "content": "(Ro BERTa) Heads Size Sequence\nLength\nLarge 355 M 24 16 1024 50,265 512\nBase 125 M 12 12 768 50,265 512\nSmall 84 M 6 8 768 50,265 512\nTable 6: Parameters of the Various Versions of the Ro BERTa Model\nRo BERTa-large was chosen as our base model architecture due to its superior capabilities in handling\ncomplex sentiment analysis tasks. Movie reviews often contain complex and mixed opinions that\nrequire sophisticated language understanding. With 355 million parameters (compared to 125 M in base\nand 84 M in small versions), Ro BERTa-large provides enhanced capability to capture nuanced\nexpressions and contextual relationships in text.\nThe model's architecture offers several key advantages for sentiment analysis. Its deeper layer\nstructure (24 transformer layers versus 12 in base and 6 in small) enables more sophisticated\nprocessing of language hierarchies and better captures complex sentiment relationships across text.\nThe larger hidden size of 1024 dimensions (compared to 768 in smaller versions) provides richer\nrepresentation of sentiment features and increased capacity to store complex emotional patterns.\nAdditionally, with 16 attention heads (versus 12 in base and 8 in small), the model can better process\nmultiple aspects of sentiment simultaneously, making it particularly effective at understanding reviews\nwith mixed or nuanced opinions.\nLo RA (Low-Rank Adaption) Configuration\nLo RA (Low-Rank Adaptation) is an efficient fine-tuning technique that significantly reduces the number\nof trainable parameters in our model. Instead of fine-tuning the entire Ro BERTa-large model with its\n355 M parameters, Lo RA selectively updates specific matrices, effectively reducing both memory usage\nand training time while maintaining strong performance on sentiment analysis tasks. This approach is\nparticularly valuable when working with Ro BERTa-large, where traditional full fine-tuning would be\ncomputationally expensive.\nDue to limitation to our computational resource, we could only execute a single training run, below are\nthe Lo RA configuration that we trained the model with:\n"
      },
      {
        "title": "1. Rank Parameter (r=128) : The rank parameter defines how many dimensions we use in our",
        "content": "low-rank matrices. We choose 128 to be our rank parameter to reduce the trainable parameters\nfrom 355 M to a manageable size. It provides sufficient complexity to capture sentiment patterns\nwhile maintaining computational efficiency. A higher rank would increase the computational\n"
      },
      {
        "title": "Content",
        "content": "demands, while a lower rank might not capture the complexity of sentiment expressions\neffectively.\n"
      },
      {
        "title": "2. Alpha Parameter (512) : This scaling factor controls the magnitude of Lo RA updates during",
        "content": "training. We selected a value of 512 to enable sufficiently big updates to the model’s behaviour.\nHigher values could lead to training instability, while lower values might result in insufficient\nlearning of task-specific patterns.\n"
      },
      {
        "title": "3. Dropout Rate (0.05) : We implemented a dropout rate of 0.05 for regularization. This relatively",
        "content": "low value maintains model stability while providing sufficient regularization to prevent overfitting,\nensuring the model generalizes well to new movie reviews.\n"
      },
      {
        "title": "4. Target Modules ([\"query\", \"key\", \"value\"]) : Our configuration focuses on adapting the",
        "content": "attention mechanism components - specifically the query, key, and value matrices.\n"
      },
      {
        "title": "5. Task Type (SEQ_CLS) : The sequence classification setting aligns with our binary sentiment",
        "content": "analysis task of classifying movie reviews as positive or negative.\nThe pre-processing of data is also different from the original one presented in the final_project\nnotebook.\nThe initial data preprocessing pipeline, similar to our base model, applies several fundamental\ntechniques to both training and testing data. First, the text undergoes initial cleaning to standardize\nformat and remove unnecessary whitespace. Next, sentences are broken down into individual words\nthrough tokenization. Common words that don't carry significant meaning are then removed through\nstop word filtering. Finally, lemmatization converts all words to their base forms to reduce vocabulary\ncomplexity.\nAfter these core preprocessing steps, we leverage Ro BERTa tokenizers to convert the processed text\ninto numerical embeddings. These embeddings are crucial as they capture both word relationships and\ncontextual information within sentences. Ro BERTa's tokenization is particularly well-suited for\nsentiment analysis of movie reviews due to its ability to effectively handle informal language patterns\ncommonly found in user-generated content. It excels at processing emotional expressions and\nintensifiers (such as \"very\", \"really\", \"absolutely\") that are crucial for sentiment understanding. The\ntokenizer also maintains important contextual relationships between words, allowing it to capture subtle\nsentiment indicators that depend on the surrounding text context.\nTo train the model, we used the following constant configurations:\ntraining_batch_size: 16\nevaluation_batch_size: 32\ngradient_accumulation: 4\nlearning_rate: 1 e-5\nwarmup_ratio: 0.1\nweight_decay: 0.1\nThe following is the learning process:\n"
      },
      {
        "title": "1. Loss and Optimization: The model uses a cross-entropy loss function specifically designed for",
        "content": "binary classification tasks.\n"
      },
      {
        "title": "2. Linear learning rate scheduler with warmup: Implements a gradual increase in learning rate",
        "content": "during initial training.\n"
      },
      {
        "title": "3. Gradient clipping at 1.0: Sets maximum threshold for gradient values to prevent exploding",
        "content": "gradients.\nFor experimentation purposes in finding the best configuration for this model in particular, we have\ndecided to hyperparameter tune the following:\nThis model is complex and is computationally heavy to train. Therefore, we decided to not perform\nhyperparameter tuning for the model. The following were the results:\n"
      },
      {
        "title": "Train Loss Train Acc Test Loss Test Acc Precision Recall",
        "content": "0.1799 0.9344 0.1781 0.9411 0.9362 0.9323\nTable 7: Results of Lora-Ro BERTa Model with the IMDB Dataset\nFigure 3: Result Graph of Training vs. Evaluation Loss for Lora-Ro BERTa Model\nFigure 4: Result Graph of Training vs. Evaluation Accuracy for Lora-Ro BERTa Model\n"
      },
      {
        "title": "Content",
        "content": "Figure 5: Result Graph of Evaluation Metrics for Lora-Ro BERTa Model\nTo ensure that the model performs just as well with the test set, we have tested it on there as well:\n"
      },
      {
        "title": "Loss Accuracy Precision Recall F1 Score",
        "content": "0.1634 0.9408 0.9425 0.9388 0.9407\n"
      },
      {
        "title": "Table 8: Test Results of the Lora-Ro BERTa Model",
        "content": "With that, the team has decided to choose Lora-Ro BERTa as our final model due to the high metrics it\ndisplays in both training and test.\n"
      },
      {
        "title": "Conclusion",
        "content": "The team has showcased a progression starting from the very base Bi RNN and Text CNN models, and\nexplored various models that kept improving. We have displayed constant improvement in all the\nmodels that we have experimented with, which demonstrates the effectiveness of iterative\nexperimentation and refinement in achieving superior performance. By systematically evaluating and\ncomparing models, such as Bi RNN, Text CNN, and other advanced architectures, we highlighted the\nincremental gains achieved through each upgrade. The progression culminated in the Lora-Ro BERTa\nmodel, which excelled in sentiment analysis tasks due to its robust pretraining and fine-tuning\ncapabilities, making it the optimal choice for this application.\n"
      }
    ],
    "metadata": {
      "title": "Sentiment Analysis Design Challenge",
      "category": "project_report",
      "file_name": "NLP_final_project_report",
      "relative_path": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/NLP_final_project_report.pdf",
      "page_count": 12,
      "project_name": null,
      "file_size": 463240,
      "last_modified": 1749024948.525621
    },
    "word_count": 2947,
    "page_count": 12
  },
  {
    "id": "mingresume",
    "source_file": "/Users/weimingchin/Desktop/weiming_chatbot/data/raw/mingresume.pdf",
    "type": "pdf",
    "title": "Chin Wei Ming",
    "category": "resume",
    "raw_content": "\n--- Page 1 ---\nChin Wei Ming\nM: 8032 5190 | E: weiming1902@gmail.com | LI: linkedin.com/in/weiming-chin | Portfolio: Project portfolio\nEDUCATION\n• Singapore University of Technology and Design Sept 21 to May 2025\n• Bachelor of Science (Design and Artificial Intelligence), Minor in Computer Science and Digital Humanities\n• Recipient of KKH-SUTD scholarship\nWORK EXPERIENCE\nThales Singapore Aug 23 to Dec 23\nDesign Innovation Engineer Intern\n• Collaborated with cross-functional teams including engineers, designers, and business stakeholders to translate insights from\nDesign Thinking workshops into actionable product requirements and solutions.\n• Developed and implemented dashboard prototypes using Figma, creating KPI visualisations that improved operational\nefficiency through real-time performance tracking.\n• Researched performances of AI technologies, resulting in strategic AI recommendations to internal teams.\nACADEMIC PROJECTS\nRetrieval-Augmented Generation (RAG) Chatbot | Machine Learning Operations (MLOps) Jan 24 to May 25\n• Built an end-to-end RAG pipeline with LangChain for a contextual SUTD website chatbot.\n• Web crawled through SUTD website pages, transformed into Markdown with enriched metadata.\n• Fine-tuned Llama 3.2 1B model using synthetic QA pairs generated by LLMs to enhance response quality.\n• Evaluated performance using LLM-as-a-judge techniques.\nRe-imagine Public Engagement with GenAI, Urban Redevelopment Board Sept 24 to Dec 24\n• Our team created a public engagement application leveraging Generative AI tools (image in-painting, prompt upscaling, AI\ncritique, from conceptualisation to execution.\n• Designed and developed the urban-planners facing analytical dashboard using Figma for rapid prototyping and React for front-\nend development.\n• Our application results in 3x faster completion rate and 68% higher user retention than conventional workflow.\nManpower Allocation Dashboard, Central Provident Fund Jan 24 to Apr 24\n• Led the design and prototyping of an analytical dashboard to optimise manpower allocation using historical trends.\n• Built and validated ML and mathematical models to forecast daily enquiry demand, reducing the need of manual estimation.\n• Researched and integrated suitable simulation system to support the project’s objective.\nSkin Lesion Classification, Deep Learning Jan 24 to Apr 24\n• Built Capsule Network model with 82.5% accuracy, trained and evaluated on HAM10000 skin lesion dataset.\n• Applied Grad-CAM for model explainability, enhancing the credibility of machine learning based skin lesion diagnostics.\n• Leveraged Generative Adversarial Network (GANs) technique to expand dataset, improving model generalisation.\n• Achieved accuracy comparable to human experts using deep learning, demonstrating a proof of concept for early-stage AI-\nassisted diagnostics.\nGuiding Hand, an app for woman facing unplanned pregnancy, Gebirah May 23 to Aug 23\n• End-to-end development of an AI-powered support app, integrating NLP chatbot, self-harm detector, and chat summarisation\nfor healthcare use.\n• Collaborated with founder of Gebirah, to align technical features with user needs and business goals.\n• Built automated testing scripts using Puppeteer (Javascript), ensuring reliability and functionally across the app.\nCO-CURRICULAR ACTIVITIES\nSUTD Football Club Jan 22 to Jan 25\nPresident | Team Captain\n• Led a 50-member organisation, managing a $3,500 budget to optimise resource allocation for key initiatives and team\ndevelopment.\n• Represented SUTD as team captain in the annual inter-varsity tournament for three consecutive years.\n• Organised large-scale events and marketing campaigns to promote football, collaborating stakeholders to drive engagement and\ncommunity participation.\nADDITIONAL INFORMATION\n• Programming Languages: Python, SQL, JavaScript\n• Machine Learning & Deep Learning: TensorFlow, PyTorch, Scikit-learn, NLP, Computer Vision, LangChain\n• Data Handling & Analysis: Pandas, Numpy, SQL, Matplotlib, Seaborn\n• Tools & Platforms: Git, Vast.ai, Figma, FastAPI\n• Fluent in English and Mandarin (spoken & written), and Malay (spoken)\n",
    "cleaned_content": "[PAGE BREAK]\nChin Wei Ming\nM: 8032 5190 | E: weiming1902@gmail.com | LI: linkedin.com/in/weiming-chin | Portfolio: Project portfolio\nEDUCATION\n• Singapore University of Technology and Design Sept 21 to May 2025\n• Bachelor of Science (Design and Artificial Intelligence), Minor in Computer Science and Digital Humanities\n• Recipient of KKH-SUTD scholarship\nWORK EXPERIENCE\nThales Singapore Aug 23 to Dec 23\nDesign Innovation Engineer Intern\n• Collaborated with cross-functional teams including engineers, designers, and business stakeholders to translate insights from\nDesign Thinking workshops into actionable product requirements and solutions.\n• Developed and implemented dashboard prototypes using Figma, creating KPI visualisations that improved operational\nefficiency through real-time performance tracking.\n• Researched performances of AI technologies, resulting in strategic AI recommendations to internal teams.\nACADEMIC PROJECTS\nRetrieval-Augmented Generation (RAG) Chatbot | Machine Learning Operations (MLOps) Jan 24 to May 25\n• Built an end-to-end RAG pipeline with Lang Chain for a contextual SUTD website chatbot.\n• Web crawled through SUTD website pages, transformed into Markdown with enriched metadata.\n• Fine-tuned Llama 3.2 1 B model using synthetic QA pairs generated by LLMs to enhance response quality.\n• Evaluated performance using LLM-as-a-judge techniques.\nRe-imagine Public Engagement with Gen AI, Urban Redevelopment Board Sept 24 to Dec 24\n• Our team created a public engagement application leveraging Generative AI tools (image in-painting, prompt upscaling, AI\ncritique, from conceptualisation to execution.\n• Designed and developed the urban-planners facing analytical dashboard using Figma for rapid prototyping and React for front-\nend development.\n• Our application results in 3 x faster completion rate and 68% higher user retention than conventional workflow.\nManpower Allocation Dashboard, Central Provident Fund Jan 24 to Apr 24\n• Led the design and prototyping of an analytical dashboard to optimise manpower allocation using historical trends.\n• Built and validated ML and mathematical models to forecast daily enquiry demand, reducing the need of manual estimation.\n• Researched and integrated suitable simulation system to support the project’s objective.\nSkin Lesion Classification, Deep Learning Jan 24 to Apr 24\n• Built Capsule Network model with 82.5% accuracy, trained and evaluated on HAM10000 skin lesion dataset.\n• Applied Grad-CAM for model explainability, enhancing the credibility of machine learning based skin lesion diagnostics.\n• Leveraged Generative Adversarial Network (GANs) technique to expand dataset, improving model generalisation.\n• Achieved accuracy comparable to human experts using deep learning, demonstrating a proof of concept for early-stage AI-\nassisted diagnostics.\nGuiding Hand, an app for woman facing unplanned pregnancy, Gebirah May 23 to Aug 23\n• End-to-end development of an AI-powered support app, integrating NLP chatbot, self-harm detector, and chat summarisation\nfor healthcare use.\n• Collaborated with founder of Gebirah, to align technical features with user needs and business goals.\n• Built automated testing scripts using Puppeteer (Javascript), ensuring reliability and functionally across the app.\nCO-CURRICULAR ACTIVITIES\nSUTD Football Club Jan 22 to Jan 25\nPresident | Team Captain\n• Led a 50-member organisation, managing a $3,500 budget to optimise resource allocation for key initiatives and team\ndevelopment.\n• Represented SUTD as team captain in the annual inter-varsity tournament for three consecutive years.\n• Organised large-scale events and marketing campaigns to promote football, collaborating stakeholders to drive engagement and\ncommunity participation.\nADDITIONAL INFORMATION\n• Programming Languages: Python, SQL, Java Script\n• Machine Learning & Deep Learning: Tensor Flow, Py Torch, Scikit-learn, NLP, Computer Vision, Lang Chain\n• Data Handling & Analysis: Pandas, Numpy, SQL, Matplotlib, Seaborn\n• Tools & Platforms: Git, Vast.ai, Figma, Fast API\n• Fluent in English and Mandarin (spoken & written), and Malay (spoken)",
    "sections": [
      {
        "title": "Chin Wei Ming",
        "content": "M: 8032 5190 | E: weiming1902@gmail.com | LI: linkedin.com/in/weiming-chin | Portfolio: Project portfolio\n"
      },
      {
        "title": "EDUCATION",
        "content": "• Singapore University of Technology and Design Sept 21 to May 2025\n• Bachelor of Science (Design and Artificial Intelligence), Minor in Computer Science and Digital Humanities\n• Recipient of KKH-SUTD scholarship\n"
      },
      {
        "title": "Design Innovation Engineer Intern",
        "content": "• Collaborated with cross-functional teams including engineers, designers, and business stakeholders to translate insights from\n"
      },
      {
        "title": "Design Thinking workshops into actionable product requirements and solutions.",
        "content": "• Developed and implemented dashboard prototypes using Figma, creating KPI visualisations that improved operational\nefficiency through real-time performance tracking.\n• Researched performances of AI technologies, resulting in strategic AI recommendations to internal teams.\n"
      },
      {
        "title": "ACADEMIC PROJECTS",
        "content": "Retrieval-Augmented Generation (RAG) Chatbot | Machine Learning Operations (MLOps) Jan 24 to May 25\n• Built an end-to-end RAG pipeline with Lang Chain for a contextual SUTD website chatbot.\n• Web crawled through SUTD website pages, transformed into Markdown with enriched metadata.\n• Fine-tuned Llama 3.2 1 B model using synthetic QA pairs generated by LLMs to enhance response quality.\n• Evaluated performance using LLM-as-a-judge techniques.\nRe-imagine Public Engagement with Gen AI, Urban Redevelopment Board Sept 24 to Dec 24\n• Our team created a public engagement application leveraging Generative AI tools (image in-painting, prompt upscaling, AI\ncritique, from conceptualisation to execution.\n• Designed and developed the urban-planners facing analytical dashboard using Figma for rapid prototyping and React for front-\nend development.\n• Our application results in 3 x faster completion rate and 68% higher user retention than conventional workflow.\n"
      },
      {
        "title": "Manpower Allocation Dashboard, Central Provident Fund Jan 24 to Apr 24",
        "content": "• Led the design and prototyping of an analytical dashboard to optimise manpower allocation using historical trends.\n• Built and validated ML and mathematical models to forecast daily enquiry demand, reducing the need of manual estimation.\n• Researched and integrated suitable simulation system to support the project’s objective.\n"
      },
      {
        "title": "Skin Lesion Classification, Deep Learning Jan 24 to Apr 24",
        "content": "• Built Capsule Network model with 82.5% accuracy, trained and evaluated on HAM10000 skin lesion dataset.\n• Applied Grad-CAM for model explainability, enhancing the credibility of machine learning based skin lesion diagnostics.\n• Leveraged Generative Adversarial Network (GANs) technique to expand dataset, improving model generalisation.\n• Achieved accuracy comparable to human experts using deep learning, demonstrating a proof of concept for early-stage AI-\nassisted diagnostics.\n"
      },
      {
        "title": "Guiding Hand, an app for woman facing unplanned pregnancy, Gebirah May 23 to Aug 23",
        "content": "• End-to-end development of an AI-powered support app, integrating NLP chatbot, self-harm detector, and chat summarisation\nfor healthcare use.\n• Collaborated with founder of Gebirah, to align technical features with user needs and business goals.\n• Built automated testing scripts using Puppeteer (Javascript), ensuring reliability and functionally across the app.\nCO-CURRICULAR ACTIVITIES\nSUTD Football Club Jan 22 to Jan 25\nPresident | Team Captain\n• Led a 50-member organisation, managing a $3,500 budget to optimise resource allocation for key initiatives and team\ndevelopment.\n• Represented SUTD as team captain in the annual inter-varsity tournament for three consecutive years.\n• Organised large-scale events and marketing campaigns to promote football, collaborating stakeholders to drive engagement and\ncommunity participation.\nADDITIONAL INFORMATION\n• Programming Languages: Python, SQL, Java Script\n• Machine Learning & Deep Learning: Tensor Flow, Py Torch, Scikit-learn, NLP, Computer Vision, Lang Chain\n• Data Handling & Analysis: Pandas, Numpy, SQL, Matplotlib, Seaborn\n• Tools & Platforms: Git, Vast.ai, Figma, Fast API\n• Fluent in English and Mandarin (spoken & written), and Malay (spoken)\n"
      }
    ],
    "metadata": {
      "title": "Chin Wei Ming",
      "category": "resume",
      "file_name": "mingresume",
      "relative_path": "raw/mingresume.pdf",
      "page_count": 2,
      "project_name": null,
      "file_size": 804410,
      "last_modified": 1749023892.4851751
    },
    "word_count": 584,
    "page_count": 2
  },
  {
    "id": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f_Product_Design_Studio",
    "source_file": "/Users/weimingchin/Desktop/weiming_chatbot/data/raw/notion_export/Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/Product_Design_Studio.pdf",
    "type": "pdf",
    "title": "Electric Vehicle Additive Manufacturing(EVAM)Interior Design",
    "category": "resume",
    "raw_content": "\n--- Page 1 ---\nElectricVehicleAdditiveManufacturing(EVAM)InteriorDesign\nAUTHORS STUDENTID\nHeTianli 1006151\nLimYuJie 1005999\nAdityaKumar 1006300\nEthanChooE-Rhen 1006324\nCaitlinDaphneTanChiang 1006537\nChinWeiMing 1006264\nAllteammemberscontributedequallytothetask.\nDesignandArtificialIntelligence,SingaporeUniversityofTechnologyandDesign\n60.003:ProductDesignStudio\nDr.KwanWeiLek,DrEdwinKoh,MrMichaelAlexander Reeves\n24April2023\n\n--- Page 2 ---\n1\nContents\nContents........................................................................................................................................................1\nBackground...................................................................................................................................................3\nUserInterview.........................................................................................................................................3\nAEIOUObservationFramework.............................................................................................................3\nValueProposition.....................................................................................................................................3\nDesignBrief...................................................................................................................................................4\nProblemStatement...................................................................................................................................4\nDesignSpecification................................................................................................................................4\nConceptDevelopment..................................................................................................................................4\nFunctionalAnalysis.................................................................................................................................4\nMoodBoard.............................................................................................................................................6\nConceptGeneration.................................................................................................................................6\nConceptSelection....................................................................................................................................8\nDesignProcess..............................................................................................................................................9\nDashboard................................................................................................................................................9\nScreens..............................................................................................................................................9\nUserInterface..................................................................................................................................10\nSteeringWheel.......................................................................................................................................11\nGettingIn/Out......................................................................................................................................11\nFinalDesign................................................................................................................................................17\nRenders..................................................................................................................................................17\nSystemDiagram....................................................................................................................................19\nEvaluation...................................................................................................................................................20\nBudgeting...............................................................................................................................................20\nDashboard.......................................................................................................................................20\nSteeringWheel................................................................................................................................20\nProductWeight......................................................................................................................................20\nClientFeedback.....................................................................................................................................20\nAreasofImprovement...........................................................................................................................21\nDashboard.......................................................................................................................................21\nSteeringWheel................................................................................................................................21\nGettingin/out..................................................................................................................................21\nReflection....................................................................................................................................................22\nAcknowledgement......................................................................................................................................22\nReferences...................................................................................................................................................23\nAppendixA.................................................................................................................................................24\nAppendixB.................................................................................................................................................25\nAppendixC.................................................................................................................................................26\nAppendixD.................................................................................................................................................27\n\n--- Page 3 ---\n2\nAppendixE.................................................................................................................................................29\nAppendixF..................................................................................................................................................30\nAppendixG.................................................................................................................................................31\nAppendixH.................................................................................................................................................32\nAppendixI...................................................................................................................................................37\nAppendixJ..................................................................................................................................................40\nAppendixK.................................................................................................................................................45\nAppendixL.................................................................................................................................................48\nAppendixM................................................................................................................................................50\n\n--- Page 4 ---\n3\nElectricVehicleAdditiveManufacturing(EVAM)InteriorDesign\nAt the forefront of design and innovation, Singapore University of Technology and Design (SUTD) is\nmanufacturing components for their electric vehicle project, known as Electric Vehicle Additive\nManufacturing (EVAM). As the name suggests, they employfabricatingmethodssuchasSelectiveLaser\nMelting (SLM), Fused Deposition Modeling (FDM), Multi Jet Fusion (MJF) and Stereolithography\n(SLA).\nAs part of our 60.003 Product Design Studio, we have been tasked with designing the interior of the\nvehicle.\nBackground\nFirstly, we would need to understand the current users’ needs for the vehicle. As thevehicleisstillinits\ntesting stage, the users are mainly the engineering staff and students. To better understand future user\nneeds, we conducted user (engineer) interviews, the AEIOU Observation Framework, and a Value\nProposition.\nUserInterview\nWhen we first viewed the vehicle, we prepared some questions for Senior SpecialistMrLiewZhenHui,\nto understand whattheyrequiredustodesign.ThiswasfollowedupwithquestionstoMrMatthewDylan\nWong Jian Xiong for his vision and the concept for thecar.Thesequestionsandanswerscanbefoundin\nAppendix A. In summary, the concept of the car is to demonstrate additive manufacturing capabilities in\nthe manufacturing of a race car. It should be designed to look‘dynamic,aggressiveandsporty’,withthe\nnecessaryfunctionsrequiredforittobedriveninarace.\nAEIOUObservationFramework\nNext, weexploredtheAEIOU1ObservationFrameworkoftheEVAM.AmindmapofourAEIOUcanbe\nseeninAppendixB.\nValueProposition\nLastly, based on our User Interviews and AEIOU, a valuepropositionwasplannedtocategorisepossible\nneeds into demands and wishes.Thedemandsarewhatwecanfullydesignwithinourtimelineandareof\nhighestpriority.Theyareasfollows:\n1. Dashboard. In order to enhance user experience, the users must be able to see all necessary\ninformationaboutthevehicleatalltimes.\n2. Steering wheel. To control more car settings for improved performance, the driver must be able to\nreachbuttons,whilestillfirmlyholdingthewheel.\n3. Mobility. The users should be able to get into andoutofthecarwithease,regardlessoftheirheight\norsize.\nOn the other hand, wishes are additional features that we could provide. They can be foundinAppendix\nC.Astoryboardwasdrawntobettervisualisethevalueproposition,foundinAppendixD.\n1AcronymforActivities,Environment,Objects,InteractionsandUsers\n\n--- Page 5 ---\n4\nDesignBrief\nProblemStatement\nThrough our design process of empathise, define and ideate, we decided to design a means to enhance\nuserexperienceoftheinteriorofthevehiclewithaddedlightness.\nDesignSpecification\nTo accomplish our problem statement efficiently, we merged certain design wishes with the demands to\ncreatethreecohesivedesigngoals:\n1. To design a dashboard, for both driver and passenger, that displays allnecessaryinformationatthe\nappropriate times, depending on its driving modes. These modes include static, demonstration and\nrace mode. The telemetry must, at least, include Speed, Temperature of battery and motor, Battery\nCharge,andmustbeintuitive.\n2. To design a steering wheel, with other controls besides steering input, for customizability of car\nfunctions. The design should be ergonomic so that the wheel is comfortable to hold and grip for\nprolonged periods, and also to reflect the SUTD identity onto the steering wheel design.\nAdditionally,controlsshouldbeeasilyreachablebyfingers,evenwhenthedriverissteeringthecar.\n3. To design a way for the trained driver and any able-bodied passenger to enter and exit thevehicle\nwithease,regardlessoftheirheightsorsizes.\nConceptDevelopment\nThis means that although the final product is the interior of the goal, there is a need to design three\nseparatecomponentsthatcanstillcomplementeachother.\nFunctionalAnalysis\nHence, in our concept development, there is a need to perform a functional analysis for the three\ncomponents. Two types of analysis are the system and process perspectives, with the former shown in\nFigure1,andthelatterinAppendixE.\n\n--- Page 6 ---\n5\nFigure1\nFunctionalAnalysisSystemPerspectiveofDashboard,SteeringWheel,Ingress/Egress\n\n--- Page 7 ---\n6\nMoodBoard\nFor the designofeachcomponenttocomplementeachother,thedesignsshouldbeinspiredbyacommon\nmoodboard,asseeninFigure2.\nFigure2\nDesignMoodBoard\nAdditionally, a mood board was designed by DALL-E2. We wanted to use an AI-generated mood board\nfor inspiration. However, it did not align with our vision so we didnotuseit.Thegeneratedmoodboard\ncanbefoundinAppendixF.\nConceptGeneration\nUsing functional analysis, a morphological chart can be created to aid in concept generation, as seen in\nTable 1. However, due to the complexity of the vehicle, onlyfunctionsrelatedtothesethreecomponents\nwillbeused.\n2 DALL-E is an Artificial Intelligence System that generates images for text input, created by OpenAI. (Johnson,\n2021)\n\n--- Page 8 ---\n7\nTable1\nMorphologicalChartConceptGeneration\nWe selected and combined these functions to form our concepts. Since we have three design goals\n(dashboard, steering, getting in/out), a totalofnineconceptsisneeded.Thefunctionsfortheconceptsare\nhighlightedinTable1,usingthelegendinTable2.\n\n--- Page 9 ---\n8\nTable2\nLegendforFunctionSelectionforConceptGenerationinMorphologicalChart\nConcept1 Concept2 Concept3\nDashboard Red ㅤㅤㅤㅤ Orange ㅤㅤㅤㅤ Yellow ㅤㅤㅤㅤ\nSteeringWheel Green ㅤㅤㅤㅤ Cyan ㅤㅤㅤㅤ Blue ㅤㅤㅤㅤ\nGettingin/out DarkBlue ㅤㅤㅤㅤ Purple ㅤㅤㅤㅤ Pink ㅤㅤㅤㅤ\nConceptSelection\nThese concepts were then put through a selection criteria, seen in Appendix G, and rated among team\nmembers. The ratings are on a scale from 1 to 5, with 5 being the most ideal. With four criteria being\nlooked at, the score for a component ranges from 1 to 20. This issummarisedinTable3.Withsixgroup\nmembers, the total score for a component will range from 6 to 120. The highest scores would then\nundergothedesignprocessinthenextstep.\nTable3\nCriteriaandRankingsofEVAMComponentConcepts\nDashboard Steering Gettingin/out\nTeam\nMember\n1 ㅤ 2 ㅤ 3 ㅤ 1 ㅤ 2 ㅤ 3 ㅤ 1 ㅤ 2 ㅤ 3 ㅤ\nEthan 18 16 9 5 17 19 17 5 8\nCaitlin 20 13 4 11 15 7 10 8 11\nTianLi 17 11 5 14 16 14 11 12 19\nWeiMing 18 15 7 6 15 11 13 20 16\nAditya 17 12 10 15 19 17 15 14 5\nYuJie 19 13 4 5 18 7 10 8 11\nTotal 109 80 39 56 100 75 76 67 70\nNote:Theselectedconceptshavetheir‘Total’boxeshighlightedintheirrespectivecolourcodes.\n\n--- Page 10 ---\n9\nDesignProcess\nOnce the concepts were selected, we began the design process. This process consists of ideation,\niterationsandfurtherdevelopment.\nDashboard\nThe dashboard could be split into two main components: user interface and screens. Each component\nwouldgothroughideation,iteration,anddevelopmentaswell.\nScreens\nInitially, therewereonlyplanstodevelopafrontdashboard.However,takinginspirationfromco-pilotsin\nfighter aircraft, a passenger dashboard was deemed necessary to assist the driver during races. Hence, a\ndashboard was required for driver and passenger, with each displaying different information. This\ninformation can be categorised into two categories: telemetry and camera views, which will be further\nelaboratedonunder‘UserInterface’.\nThe front dashboard only had a 360 by 140 millimetre space while the rear dashboard had a 200by200\nmillimetre space. At therear,therewasalsomorespacebelowthecentralsupportbar,butthatareahadto\nbe clear for the passenger to enter and exit the vehicle. Furthermore, only the upper section of the\ndashboard space could be used to narrow down the driver’s field of view, especially when he needs to\nfocus on the track. Hence, we decided to use two 3.5-inch LCDs and one 7-inch LCD at the front, and\nfive7-inchLCDsattheback.\nAt the front, the dashboard holder was designed more aesthetically than functionally. Taking inspiration\nfrom wind tunnel flow visualisation (Hall, 2021), and our mood board, wavy lines were etched into the\ndashboard, with incrementing depths of two millimetres. These lineswouldcontinuetowardstheinterior\nside panelling of the vehicle.Thefrontscreenholderisalsotiltedfivedegreesupwardstoaccountforthe\nelevated height of the perspective of the driver. Similarly, the screens are tilted five degrees inwards to\nfacethedriver.AnisometricviewofthisdesigncanbefoundinAppendixHFigureH1.\nTo account for ease of manufacturing and theme of additive manufacturing, the screen holder had to be\nbroken up to fit the 3D Printer bed size. In the end, the dashboard was divided into two, with excess\nmaterial trimmed for added lightness. A screen cover was added to seal the screen in the dashboard, as\nwellastohideanyexposedelectronics.ItsassemblycanbefoundinAppendixHFigureH2.\nAt the back, we decided to implement adjustable screen holders. This is because unlike the user of the\nfront seat, who is usually thesametraineddriver,theuserofthebackseatcanbeanyable-bodiedperson,\nregardless of his/her height. This means that different passengers may have different perspectives,hence\nthe screen tilt must be different. Furthermore, as we chose to utilise the space below the central support\nbeam for more screens as well, the screens should be able to tuck away so that itiseasierfortheuserto\ngetinandoutofthevehicle.\nWe based the design of theseadjustablescreensonmonitorstands.Thisusuallyinvolvesarmswithjoints\nor hinges that can be tightened or loosened. We simulated this design by 3D printing arms, which were\nconnected using M10 hex boltsandwingnuts.Thismeansthatthejointscouldbetightenedandloosened\nusing the wing nuts. Similarly, the clamps that held the screen holders to thecentralsupportbeamofthe\nchassis could be tightened and loosened to provide a cylindrical joint for further adjustment by the\npassenger.\nWhile sharing with the engineering team about our design plans,theytoldusthattestersoftenholdonto\nthe central support beam to steady themselves to counter the G-forces during acceleration and\ndeceleration. This means that the initial plan for placement of screens wouldblockthesupportbeam.As\n\n--- Page 11 ---\n10\nwe did not want to eliminate this, we decided to implement grips into the clamps. This would not only\nallow passengers to support themselves during high lateral forces, but also for ease in adjusting the\nclamp’s tilt. The final design can be found in Appendix H Figure H3, with its exploded assembly in\nFigureH4.\nThe parts list for these dashboards can be found in Appendix H Table H1 while the fasteners usedarein\nTableH2.\nUserInterface\nDue to the different modes, two different interfaces must be designed: Race and Demonstration. The\ncodes for this (including all the connections of the user interfaces to the various components of the\nsteeringwheel)canbefoundatthisrepository:https://github.com/CaitlinChiang/EV-Screens\nRace. Inspiration was taken from F1 cars because the car concepts are similar: open wheel racing.\nSpecifically, we took inspiration from the Aston Martin Aramco Cognizant F1 Team, which displayed\ntheir wheel layout and the telemetry for each. The race driver would require telemetry such as speed,\ntyre/brake/motor/battery temperatures, battery charge,racedeltas,laptimings,brakebiasandmigration,\nrace positions, laps done and drinking water remaining. Allthisdataiscolourcoded,withorangebeing\ntoo hot or almost depleted, blue being too cold or half used, and white being optimum temperature or\nfull. The race-standard colours purple, green and white are used forracetimingsaswell.Thisissothat\nthedrivercaninstantaneouslytelliftheirbrakes/tyres/motor/batteryistoowarmorcold.\nAs for the camera views, they were placed at the usual positions that therearviewmirrorswouldbein\nanordinarycar.Thisissothatitismoreintuitive.\nWe tested the designs by printing out the interface onto paper the size of the actual screens. We then\npasted them on the vehicle and tested how readable the interface is, as seen in Appendix I Figure I1.\nInitially,thefontsweretoosmallsoweincreasedthesizes.Additionally,wedecidedtoremovethefront\nfacing camera as thespeedometerwastoolowandwasblockedbythesteeringwheel,sincethespeedis\nofhigherimportancethanthefrontfacingcamera.\nThefinalinterfacecanbefoundinAppendixIFigureI2.\nDemonstration. The first iteration was a simple layout only including the absolute essential telemetry\ngiven to us during the interview, but we realised that a lot of spaceisbeingputtowaste,andthatusers\nactually don’t know that seeing certain data is convenient untilwealreadypresentittothem.Withthat,\nwe iterated another design, this time adding telemetrythatwethinkisnecessarybasedonresearch.The\ntelemetryhereisthesameastheonesusedinracemode,exceptdesignedandpresenteddifferently.\nOn the third iteration, we focused more on the experience of the interfaces and making sure that they\nwere pleasing to look at.Westartedtousecolourstodepictcertainvalues(e.g.usingredwhenacertain\npart is overheating), and placed telemetry in a way that made more sense to the user, allowing it to be\nmore intuitive as well. We also added camera views here, the same as race mode, tothepositionsasto\nwhererearviewmirrorswouldnormallybe.\nThe final iteration includedcreatingsimilarbutdifferentinterfacessuchthattheusercanswitchtothem\nat appropriate times, and therefore be able toseecertaintelemetryatspecificsituationsatalargerview.\nWekeptrefiningthemsuchthatthereisaflowtowhichgetsshownatvariousbuttonpresses.\nThefinalinterfacecanbefoundinAppendixIFigureI3.\n\n--- Page 12 ---\n11\nSteeringWheel\nThe design process for our steering wheel began with a collaborative effort among team members, with\neachcontributingtheirbestideastocreateaninitialconcept.Werefinedthedesignthroughtrialanderror,\nexperimenting with specifications like size and button placement to achievethedesiredfunctionalityand\naestheticswhichcanbeseeninAppendixJFigureJ1andFigureJ2.\nHowever, we realised that we had overlooked important factors such as budget and manufacturing\nlimitations, which led us to revisit the design and incorporate these factors. The iterations of steering\nwheeldesignscanbeseeninAppendixJFigureJ3.\nTo achieve a practical and cost-effective design, we began with a robust and lightweight aluminium\nchassis that provided structural support and stability. We carefully designed the chassis with cutouts for\nbuttons and screws, ensuring a precise and secure fit for all components asseeninAppendixJFigureJ4\nand Figure J5. Another point to note is that, when we designed the steering wheel, we made sure to\nposition the buttons in a way that would allow for easy access by the thumb for an average adult male.\nThe button placements, as well as their functions can be seen in Appendix J Figure J6 and Table J1\nrespectively.\nThe handles, buttons, and button shell were 3D printed using advanced additive manufacturing\ntechniques, allowing for a high degree of customisationandprecision.Thisreducedproductioncostsand\nlead times while also ensuring optimal functionality and aesthetics. The paddle shifters were designed\nwith simplicity andeffectivenessinmind,featuringamicroleverswitchtowhichthepaddleattaches.We\nutilised a magnet instead of a traditional spring to provide haptic feedback, enhancing the tactile\nexperiencefortheuser.ThelistoffastenersusedtoassemblethesteeringwheelcanbefoundinAppendix\nJTableJ3.\nTo cover the wires coming out the back of the steering wheel, we designed and 3D printed a custom\ncasing that also housed the paddle shifters. This provided a clean and professional look while ensuring\nthatthewireswereprotectedfromdamageandinterference.\nOverall, our steering wheel design incorporated a range of advanced manufacturing techniques and\nmaterials, from the precision-cut aluminium chassis to the 3D printed components and custom casings.\nEvery element was carefully considered and optimised for maximum functionality and user experience,\nresultinginahigh-performanceandaestheticallypleasingproductasseeninAppendixJFigureJ7.\nGettingIn/Out\nThis particular aspect of the EVAM’s experience was rather tricky to enhance. Mainly because of the\nspace constraints of the car’s interior,aswellashowthecarwasnotinitiallydesignedforaneasyingress\nand egress in mind. Both this obstacle has created a particularly awkwardwayfortheuserofthevehicle\ntogetinandoutofthecarasillustratedinFigure3below.\nHerearethestepsfortheingressandegressprocesses:\nIngress\n(1)Putonefootonthechassis\n(2)Putbothfeetontotheseat\n(3)Usingthesidechassisassupport,theuserwillslowlylowerhim/herself\n(4)Userwillthensuccessfullygetintothecar\n\n--- Page 13 ---\n12\nEgress\n(1)Holdonthethesidechassis\n(2)Pushhim/herselfup\n(3)Standupontheseat\n(4)Steponefootontothechassis\n(5)Steptheotherfootontothehigherchassisbarandeventuallygetoutofthecar\nFirst, we will illustrate with pictures how an average person enters the back seatofthevehicle.Wehave\ndecided to put our main focusonthebackseatbecausewehavebeeninformedthattheuserprofileofthe\nfrontseatwillbeafitandathleticracer.\nFigure3\nProcessthatusergetsintovehicle(fromlefttoright,toptobottom)\n\n--- Page 14 ---\n13\nAs illustrated in the pictures above, it is extremely uncomfortable and awkward even for a ratherfitand\nathletic person. So imagineifduringashowcaseofthecar,anoldpersonwouldliketositinthecar.With\nthecurrentdesign,itwillbeverydifficultforthattohappen.\nBefore we start going intoideatingwhatadesignsolutionforthiswouldbe.Wefirstanalysewhatarethe\ntouchpoints, by hand and foot, of different users when they get in and out of the car. And eventually\nbringing us to understand in which area of the car is commonlyusedassupportwhentheyenterandexit\nthecar.AnillustrationofthecommontouchpointsisshowninAppendixKFigureK1.\nFurthermore, we had to understand what are the space constraints we are limitedtowithinthecar.Todo\nthis, using Fusion 360, we modelled out the space inside the car, allowing us to know the exact\nmeasurements of space that we could work with. An illustration of the space constraint is shown in\nAppendixKFigureK2.\n\n--- Page 15 ---\n14\nAfter which we exploredmultipleideasasseeninAppendixKFigureK3,whicheventuallywerealisedis\nnot very practical. One of the ideas was to lift the seatuptoacertainlevelthatwillbeeasierforusersto\nget into a sitting position first, then using some sort of mechanism to slowly lower the seat down and\nsame for bringing the seat up. But we were soonnotifiedbyMrLiewthatwearenotabletodisassemble\nthe seat, therefore this idea was removed. Another idea wastohavepanelswhichareabletoflapopenas\nstairs for the user to step intothecarinmultiplelevels,butitwassoonrealisedthattheseflapswouldnot\nbeabletosupporttheweightofanadultperson.\nSoon after, we realised that the most amount of space that is availableinsidethecar,isactuallythesides\nof the seats. Which led us to think of designingamodularsidecompartment,whichservesasanarmrest,\nstorage space, as well as steps toeasetheingressandegressofusers.Italsoactsasaspaceforfunctional\nbuttonsthatwerenotplacedonthesteeringwheel.\nHaving used Fusion 360 to model out what the side compartment looks like, we ensured that the sizeof\nthe armrest will be within the size constraint of the interior of the EVAM. Figure 4 shows the design of\nthemodularsidecompartment,whileFigure5andFigure6showitsfunctionalities.\nFigure4\nDesignsforeasingingressandegress\n\n--- Page 16 ---\n15\nFigure5\nDesignforeasingingress\nFigure6\nDesignforeasingegress\n\n--- Page 17 ---\n16\nAnother issue that we found was that there is no place forthedrivertoresttheirheelwhenoperatingthe\nvehicle. Using Midjourney, an AI image generator, we asked it to show us designs of a F1 heel rest for\ninspiration.GeneratedimagescanbefoundatAppendixKFigureK4.\nWe then proceeded to design a heel rest, which also includes a leg rest that will further aid the driver to\nget inandoutofthecar.Furthermore,whenourteamtriedgettingintothevehicletotestoutthespacewe\nhad, there was one engineer whowasworriedthatourteam,beinginexperiencedwiththecar,wouldstep\non some of the wiringthatwasexposed.Havingaddedalegrest,therewasnowacoverforthewiresand\nnolongertheworryofsteppingonsensitivewiring.\nFigure7belowshowsapictureoftheheelandlegrest.\nFigure7\nDesignforheelandlegrestforfrontseat\nThe design for the heel rest is heavily inspired by the Midjourney generated images. As for the leg rest,\nthe driver is able to ‘feel’ his/her legs into the car and eventually reach thepedals.Wavypatternscanbe\nseen throughout the design of the car, these patterns are made rubbery with very high friction which\nserves as steps users take. A better illustration of the wavy patterns on the rest of the car can be seen at\nAppendixKFigureK5.\n\n--- Page 18 ---\n17\nFinalDesign\nRenders\nFigure8\nOverheadviewofthefinalinteriordesignoftheEVAM\nFigure9\nOverheadviewofthefinalinteriordesignoftheEVAM\n\n--- Page 19 ---\n18\nFigure10\nDriver’sperspectiveofthefinalinteriordesign\nFigure11\nPassenger’sperspectiveofthefinalinteriordesign\n\n--- Page 20 ---\n19\nSystemDiagram\nTo illustrate the new functionsofthedashboardsandsteeringawheel,aSystemDiagramhasbeendrawn\nasseeninFigure12.\nFigure12\nSystemsDiagramofEVAMInteriorDesign\nThiscanalsoberepresentedinastoryboardforthesolutions,asseeninAppendixL.\n\n--- Page 21 ---\n20\nEvaluation\nBudgeting\nFor this project, we were given a budget of $1000fortheproductionofourproduct.Wewillbebreaking\ndown how much was spent in the making of each component in this section, as well as justification of\nwhy these purchases are necessary for our project. The detailedlistofhowmuchwasspentcanbefound\ninAppendixM.\nDashboard\nA total of$737ofthebudgetwasspentoncreatingthedashboard.Itwasparticularlyexpensivebecause\nof the display that is compatible with the raspberry pi module, that is necessary for us to showexactly\nwhatitwouldlooklikeexactlyintheactualcar.Secondreasonbeingthatwewereconstantlyfacedwith\nthe issue of not supplying enough power to the Raspberry Pi and its display. And soon later found out\nthat for each screen and Raspberry Pi,itneededaseparatepowersourcewhichwethenneededtobuya\ncharging plug for each display. For each screen, we also needed a SD card slotted into it for it to\nfunction. Therefore a part of the budget also went to the purchaseofSDcards.Otherthanthat,wealso\nintended 2 of our screens to display the live rear camera view in the car. Therefore we purchased 2\nraspberry pi cameras to connect to the Raspberry Pi which eventually is able to display camera views\nonto the Raspberry Pi display that we purchased. Other than that, we purchased quite afewcablesthat\nare necessary to connectbetweentheRaspberryPimodule,thedisplay,andaswellasthepowersource.\nFor each display, 1x HDMI to HDMI cable, 2x USB to micro USB cable is required. Thereforewith5\ndisplays,weneeded5xHDMItoHDMIcable,and10xUSBtomicroUSBcable.\nSteeringWheel\nA total of $138 of the budget was spent on the production of the steering wheel. When creating the\nbase metal plate of the wheel, we first attempted to purchase our own metal plate and cut it out in the\nFabrication Lab (FabLab), but was soon met with the obstacle of it being too heavy and difficult in\ncutting precisely to scale. We then seeked help from an external source which provided us with alaser\ncutting service as well as a good lightweight material that is most suitable for our use case. Some\namount ofthebudgetwasalsospentonswitchesthatwerenecessaryforustomakefunctionallybuttons\nin the steering wheel. We also spent some amount to purchase the screws and nuts that were not\nprovided by Fablab mainly because it didn't have the length and size that we needed. Lastly, we\npurchased spray paints for the final finish of the steering wheel. Note that someoftheitemspurchased\nlikeclayandnon-slipmatswerefortherapidprototypingphaseofthesteeringwheel,andwerenotused\nin the production final steering wheel. Finally, we also seek help from an external 3D printing service\nfortheproductionofourbackcasingforbetter3Dprintingfinishandquality.\nProductWeight\nAs seen in Appendix H Table H1 and Appendix J Table J2, the total weight of fabricated products is\nestimated to be approximately 3.628 kilograms. This is below our design specification wish of five\nkilograms. This means that additional functionality would have a very minor dentontheperformanceof\nthevehicle.\nClientFeedback\nThe client was particularly impressed with the seamless integration of the steeringwheelanddashboard.\nThey felt that the design concept was innovative and unique, and appreciated the attention to detail that\nwent into creatingacohesiveandaestheticallypleasingproduct.Theclientnotedthatthedesigncreateda\nmore streamlined and ergonomic experienceforthedriver,whichtheyfeltwasasignificantimprovement\noverthetraditionaldesign.\nWhile the client was impressed with the overall design concept and integrationofthesteeringwheeland\ndashboard, they did have some minor critiques. The use of trial-and-error fordeterminingmeasurements\n\n--- Page 22 ---\n21\nwas seen as a potential setback, as the client recommended the use of Anthropometric data for more\nprecise results. Additionally, the client suggested modifying the line width and increasing the infill\ndensity for the 3D prints in order to enhance the durability of the product, especially if it will be used\nwhiledriving.\nDespite these minor setbacks, the client remained highly satisfied with the design and expressed\nexcitementforthepotentialoftheproduct.\nAreasofImprovement\nDashboard\nDue to the lack of time, more iterations of the screen holders could not be fabricated. The front\ndashboard holder should be designed to hide the Raspberry Pi. Similarly, the passenger dashboard\nshould have a way to hide the cables that run out of it. Screen casings should be made, together with\nsealant, to weatherproof the electronics. A more flexible material should be usedtomakethehingesso\nthatthehingescantightenbetter.\nIn terms of improvements in regards to the user interfaces, the various functionalities of the steering\nwheel can have more impact on the displays aside from changing the interface completely from one\nmenu to another.Instead,wecanhavethevaluesonthedisplayitselfchangedependingonthetiltofthe\ncar. Furthermore, we aim to include warning interfaces that have the ability to completely override the\ncurrent interface being displayed to the driver / passenger (with the ability to close it quickly) when a\ncertain vehicle component is overheating or underperforming. This way, the driver and passenger can\noperate more safely and with a better peace of mind, knowing that anything that goes wrong will be\nalertedbeforeincriticalcondition.\nSteeringWheel\nDuring the design and manufacturing process of our steering wheel, we encountered some challenges\nthat revealed areas for improvement. Specifically, we found that our wiring management was lacking\nand our soldering skills needed improvement, resulting in a delicate final product.Additionally,the3D\nprinting process had some flaws that required workarounds. Furthermore, we were unable to complete\nthe setup of the dashboard due to lack of planning and insufficientlylongwires,whichcausedconstant\nreboots. These challenges highlight the need for more comprehensive planning and better execution of\nmanufacturingprocessesinthefuture.\nGettingin/out\nSeeing how the rest of the components blend so seamlessly with the interior of the car. This particular\ncomponent that is aiding the ingress and egress process for the user, specially seemed like a complete\nseparatecomponentanddoesnotentirelyblendintotheinteriorofthevehicle.Therefore,morethoughts\non how these side compartments could blend more seamlessly with thedesignoftherestoftheinterior\nofthecomponentswoulddefinitelyimprovetheoveralldesignofthevehicle’sinterior.\nAlso because this is one of the more obvious components when someone looks into the car, mainly\nbecause of its size compared to other components. We could also put more thoughts on how its\nfunctionality and aesthetic of the design of the sidecompartmentcouldbetterreflecttheSUTDidentity\nof being future-oriented, design-centric, and interdisciplinary. So that when visitors come and visit the\nEVAM, they are constantly reminded that this is specially produced by SUTD students and as well as\ntheSUTDidentity.\n\n--- Page 23 ---\n22\nReflection\nAs mentioned in the client feedback, our group didnotuseprecisemeasurements.Thisledtoissuessuch\nasthebackcasingofoursteeringwheelnotfittingthebaseplateexactly.Assuch,wehadtofiledownthe\nback casing inordertofitittothebaseplate.Havingfacedthisavoidableissue,ourgroupnowknowsthe\nimportanceoftakingprecisemeasurements,especiallyinthepreliminarystagesoftheproject.\nOur group also made use of Dall-E and Midjourney in our project to give us inspiration. While theydid\nhelp us make the design process easier, we understand that it is only a tool to aid us, and should not be\nused as a solution in itself. There is still the need for humans (i.e.ourgroup)togetinvolvedtotestouta\ndesign and see if it works for our project. This can be seen when we decided that the mood board\nprovided by Dall-E did notsuitourvisionandthatwewouldnotwanttouseit.Anotherexampleiswhen\nwe used Midjourney to give usdesigninspirationsforthefootrest.Westillhadtogodowntothevehicle\nand see ifthefootrestwasindeedabletobeimplemented,withtheconstraintmainlybeingtheamountof\nspacewehad.\nEffective and precisecommunicationofideasisverycrucialattheideatingpartofthisproject.Especially\nwhen we needed to explore and compare between different design solutions. Therefore, theskillofhand\nsketching in a 2Ddiagramiscrucial,inthesensewhereweareabletoexplainourideastoeachotherina\nquick and precise manner. After which when this particular idea seems like a good one in the 2D\nperspective, we will then have to model it out in any 3D software available and 3D print it for us tosee\nthe component physically.Wewillthenplaceitintheactualpositioninthevehicleofwhereitisintended\nto be. These steps will then allow us to analyse possible issues that may arise with thisparticulardesign\nsolution,whichwecantheniterateandimproveonuntilwereachourfinaldesignsolution.\nAcknowledgement\nWe would like to express our deepest gratitude to Mr Liew and the rest of theteamfortheirguidancein\nour project, and to the EV Club’s Wei Ming for taking the time to come down totheFabLabtoassistus\nwithmeasurementsandtesting,despiteherbusyschedule.\nWe would also like to acknowledge with gratitude, the support from our professors and supervisors. We\nwould like to thank Mr Michael Alexander Reeves and Dr Edwin Koh, for giving us different\nperspectives on the project, aswellasDrKwanWeiLekforhelpinguswiththeconnectionofelectronics\nandprovidinguswithmultiplemicrocontrollers.\nSpecial thanks to Ms Zerline Tan for ensuring we have a conducive place to work in. Last but not least,\nthank you to our Teaching Assistants (TAs), Ke Wei and Eugene for sparing time to give us insights on\nexistingcardesignswecanuseasinspirationforourproject.\nSpecial mentions to the exterior team consisting of Aaron, Billy, Joshua, Jun Xiang, Valencia and Wei\nXuan. It was thanks to the collaboration between both teams that we managed to showcase the EV as a\nwhole,ratherthanseparateprojects.\nIt is thanks to all of these people that our group managed tocompletethisprojectwithsuchsuccess,and\nimpressourclientswithourfinalphysicalprototypesandproduct.\n\n--- Page 24 ---\n23\nReferences\nHall,N.(2021,May13).SmokeandTuftFlowVisualization.WindTunnelIndex.RetrievedApril24,\n2023,fromhttps://www.grc.nasa.gov/www/k-12/airplane/tunvsmoke.html\nJohnson,K.(2021,January5).OpenAIdebutsDALL-Eforgeneratingimagesfromtext.VentureBeat.\nRetrievedApril21,2023,from\nhttps://venturebeat.com/business/openai-debuts-dall-e-for-generating-images-from-text/\n\n--- Page 25 ---\n24\nAppendixA\nInterviewtranscriptwithEVAMEngineers\nThefollowingquestionswereaskedtoSeniorSpecialistMrLiewZhenHui:\n1. Whatisyourbiggestfocusindesigningtheoperationsofthevehicle?\nThegoalisracing,hencespeedismostimportant.\n2. Whataresomeissuesyoufaceintheoperationofthevehicle?\nThereisnocarinformationcommunicatedtothedriver.\n3. Aretherespecifictelemetriesyouwouldliketosee?\nSpeed,Temperatureofbatteryandmotor,BatteryCharge.\n4. Iscomfortimportant?\nThedrivermustadapttothedesign,butsupporttotheneck,shouldersandlumbarisabonus.\nAs afollowup,wetextmessagedastudentengineerwhohelpeddesignthecar,MrMatthewDylanWong\nJian Xiong (2023), to offer his perspective on the vehicle. We asked what his original vision for the car\nwas:\nThe concept of the caratitscorewastoexploitadditivemanufacturingasmuchaspossibletodothings\nthatarenotconventionallypossibleandtomakethefabricationprocessmoreefficient.\n(1) Explore how additivemanufacturingcanbeusedtofabricatethebodyshellparts.Ahugeproportion\nofthemoneyspentonthecarisgoingintothe3dprintedmetaljointsontheframe.\n(2)Highlightthe3dprintedjointsofthecar.ThisiswhyItriedtoframetheminsizethesidewindow.\n(3) The car was originally meant to be a \"formula-style race car\". It has a low ride height, wide track\nwidthandlowoverallheightthatgivesitanaggressivestance.\n(4) The design ofthecarshouldbetosomeextent,dynamic,aggressiveandsporty.Itshouldlooklikeit\ncan go fast even when it's not moving. At thesametime,thereshouldbeaerodynamicelementstosend\nthemessagethatthiscarismeantforperformance.\n\n--- Page 26 ---\n25\nAppendixB\nAEIOUFrameworkObservationMindmap\nNote:Thenodes’relationshipsarerepresentedbythedashedlines.\n\n--- Page 27 ---\n26\nAppendixC\nValuePropositionWishes\n1. Weight\nThe additional components are lightweight (below 5 kg) to enhance performance despite additional\nfunctions.\n2. Hydration\nThe users should be able to have access to water easily so they can sustain themselves during\noperationofthevehicle.\n3. Safe\nUsersmustbeabletoberemovedfromthevehicleswiftlyandsafelyifanaccidentweretohappen.\n4. Communication\nUsersneedtobeabletocommunicatewitheachotherdespitethenoise.\n5. Comfort\nThecockpitshouldstaycool,andtheseatingpositionsshouldbecomfortable.\n6. Fabrication\nThe components should be easy and inexpensive to manufacture, preferably done using additive\nmanufacturingtosupporttheoriginalconceptofthevehicle.\n7. Affordable\nControlsareintuitive.\n8. Enjoyable\nUserswillhaveapositiveexperiencefromridinginthevehicle.\n\n--- Page 28 ---\n27\nAppendixD\nStoryboard(Problems)\n\n--- Page 29 ---\n28\n\n--- Page 30 ---\n29\nAppendixE\nWorkflowofFunctionalAnalysisProcessPerspective\n1. Initialisethevehicle\na. Receiveuser\nb. Receiveinputtostartthevehicle\nc. Initiatestartupprocess\nd. Receiveinputsonusersettings,ifany\n2. Operatethevehicle\na. Receiveinputsfordrive/reversemode\nb. Receiveinputsonusersettingchanges,ifany\nc. Displayinformationoftheuser’ssettings\nd. Receiveinputsonvehicle’sstatus,ifany\ne. Displayinformationonthevehicle’sstatus\nf. Physicallysupportuser\ng. Hydrateuser\nh. Cooluser\ni. Receivevoiceinputfromdriver/passenger\nj. Relayvoiceoutputtodriver/passenger\n3. Deactivatethevehicle\na. Receiveinputforemergencystop,ifany\nb. Receiveinputtoshutdownthevehicle\nc. Initiateshutdownprocess\nd. Savecurrentinputsofusersettings\n\n--- Page 31 ---\n30\nAppendixF\nAIGeneratedMoodBoard\n\n--- Page 32 ---\n31\nAppendixG\nCriteriaforscoringandrankingdesignconcepts\nCriteria Score\nFunctionality 1 2 3 4 5\nEaseofFabrication 1 2 3 4 5\nEaseofuse 1 2 3 4 5\nCost 1 2 3 4 5\nTotal\nNotesfromselection\nDashboard\n1. Using the Raspberry Pi computer was most ideal because our team could borrow them from our\ninstitution,SUTD,allowingustostaywithinourgivenbudget.\n2. Fordisplayinginformation,theLCDscreenswerechosenoverTabletDevicesastheywouldbetoo\nexpensive, and over e-ink as it cannotrespondfastenoughforhighstresssituationssuchasracing.\nLastly, a rocker switch was chosen over its other concepts because it gives a tactile sensation to\nstartingandstoppingthecar,withtwoalternativeoptions:startandstop.\nSteering\n1. Buttons were chosen over all other options for the steering as in a racing scenario, timing to\nobtain required information is extremely important, and buttons are the most effective in\nminimising the movement and time needed to obtain the desired dashboard information output,\nmakingitthebestchoice.\nGettingin/out\n1. We chose to use groovedpadsoverrubberpadsbecausegroovedpadsprovidemoregripstrength\non the shoe to prevent any slipping. We can also incorporate awavydesignintoitforaesthetics,\nwhichiswhatwewantasshowninourmoodboard.\n2. Steps have been chosen over leg rests as we realised that the height difference when getting in\nand out of the car is very deep. By adding steps, it serves asacomfortableintermediarypointof\nmanoeuvringtomaketheprocesssmoother.\n3. We decided to use handles for the back seat as we observed that it is easier for drivers to push\nthemselves upwards as compared to pulling themselves up by holding onto the chassis when\ngettingoutofthecar.\n4. We have also implemented grooved heel rests for the front seat as during our investigation, we\nrealised that there is a gap between the heels of the driver when their feet rests on the pedals,\nmaking it tiring to hold the driving position for long periods of time. Additionally, whengetting\nout of the car,driversalwayspushthemselvesupusingtheirlegs,andbyaddingheelrests,italso\nservesasagripmakingiteasiertoexitthecar.\n\n--- Page 33 ---\n32\nAppendixH\nDashboardsScreensSchematics\nFigureH1\nInitialdesignofFrontDashboard\nFigureH2\nFinalexplodedassemblyviewofFrontDashboard\n\n--- Page 34 ---\n33\nFigureH3\nFinalfullyassembledPassengerDashboard\nNote.Redarrowsindicatepossiblemovementofjoints.Notalljointscanbeseen.\n\n--- Page 35 ---\n34\nFigureH4\nFinalexplodedassemblyviewofPassengerDashboard\n\n--- Page 36 ---\n35\nTableH1\nPartslistusedinDashboards\nLabel Part Weight(kg)\nA MainDashboardBodies 0.347\nB RaspberryPi3ModelB3\nC Waveshare3.5inchRPiLCD(B)4\nD Waveshare7inchResistiveTouchScreenLCD,1024×600,HDMI,\nIPS5\nE ScreenCover\nF TopScreenRockerArms 2.700\nG TopClampArm\nH TopMiddleClamp\nI BottomMiddleClamp\nJ TopLeftClamp\nK BottomLeftClamp\nL RightTopClamp\nM RightBottomClamp\nN BottomArm\nO BottomHinge\nP ScreenHolder\nQ Waveshare7inchResistiveTouchScreenLCD,1024×600,HDMI,IPS\n3CADModeladaptedfrom\nhttps://grabcad.com/library/raspberry-pi-3-model-b-reference-design-solidworks-cad-raspberry-pi-raspberrypi-rpi-\n1\n4CADModeladaptedfrom:https://grabcad.com/library/raspberry-lcd-3-5inch-1\n5CADModeladaptedfrom:https://www.waveshare.com/wiki/File:7inch_HDMI_LCD_3D_Drawing.zip\n\n--- Page 37 ---\n36\nTableH2\nFastenerlistusedinDashboards\nS/N Fastener Quantity Joint\n1 M8WingBolt75mm 4 A(ScrewClamp)\n2 M8HexNut 8 A(ScrewClamp)\n3 M2Screw20mm+Nut 4 BandA\n4 M3Screw20mm+Nut 4 A,DandE\n5 M10HexBolt+WingNut+SpringLockWasher 13 H and I, JandK,LandM,\n(90mm), 6 F/O and P, G/N and F/O,\n(40mm) H/I/K/MandG/N\n6 M6 Hex Bolt 40mm + Hex Nut + Spring Lock 8 H/J/LandI/K/M\nWasher\n7 M3Screw20mm+Nut 20 PandQ\n\n--- Page 38 ---\n37\nAppendixI\nUserInterfaceforScreens\nFigureI1\nPhysicalTestingoftheUserInterface\n\n--- Page 39 ---\n38\nFigureI2\nFinalUserInterfaceforRaceMode\nFigureI3\nFinalUserInterfacesforDemonstration&StaticMode\n\n--- Page 40 ---\n39\n\n--- Page 41 ---\n40\nAppendixJ\nFigureJ1\nInitialsketchesofsteeringwheeldesigns.\nFigureJ2\nIterationsoffinalsteeringwheeldesigns\n\n--- Page 42 ---\n41\nFigureJ3 FigureJ4\nSteeringwheelphysicalcutouts IsometricViewofFullyAssembledSteeringWheel\nFigureJ5\nFinalexplodedassemblyviewofSteeringWheel\n\n--- Page 43 ---\n42\nFigureJ6\nButtonLayoutforSteeringwheel\nFigureJ7\nFinaliterationofthesteeringwheel\n\n--- Page 44 ---\n43\nTableJ1\nButtonFunctionsforSteeringWheel\nButton Function\nA Switchtheuserinterfaceonthescreentoshowtheleftcameraview.\nB Switchtheuserinterfacetothemainmenuofracemode(driver).\nC Turnontheleftlightindicatorsofthevehicle(theoretically,notimplemented).\nD Turnontheheadlightsofthevehicle.\nE Turnontheheadlightsofthevehicle.\nF Turnontherightlightindicatorsofthevehicle.\nG Switchtheuserinterfacetothemainmenuofracemode(passenger).\nH Switchtheuserinterfaceonthescreentoshowtherightcameraview.\nTableJ2\nPartslistusedinsteeringwheel\nLabel Part Weight(kg)\nA SteeringwheelChassis 0.581\nB Rightbuttoncasing\nC Leftbuttoncasing\nD Backcasingforthesteeringwheel\nE Rightpaddleshifter\nF Leftpaddleshifter\nG Quickreleasemechanism\n\n--- Page 45 ---\n44\nTableJ3\nFastenerlistusedintheSteeringWheel\nS/N Fastener Quantity Joint\n1 M3Screw20mm+Nut 2 ConnectingthePaddle\nShifterstotheMicroLever\nSwitch\n2 M3Screw25mm+Nut 4 AttachingthePaddle\nshifterstothebackcasing\n3 M5Screw20mm+Nut 6 BindtheQuickReleaseto\ntheSteeringWheel\n4 M5Screws25mm+Nut 6 Boltingthe3Dprinted\nHandlestotheSteering\nWheel\n5 NeodymiumMagnets(2mm) 4 Providingthenecessary\nforceforpaddleshifter\nfeedback\n6 M3Screws25mm+Nut 6 TighteningtheButton\nCasingtotheSteering\nWheel\n\n--- Page 46 ---\n45\nAppendixK\nFigureK1\nTouchpointanalysationforingressandegressofthevehicle\nFigureK2\nSpaceconstraintswithintheEVAM\n\n--- Page 47 ---\n46\nFigureK3\nDesignexplorationforingressandegress\nFigureK4\nMidjourneygeneratedheelrestimages\n\n--- Page 48 ---\n47\nFigureK5\nIllustrationofstepsthroughoutthecar.\n\n--- Page 49 ---\n48\nAppendixL\nStoryboard(Solutions)\n\n--- Page 50 ---\n49\n\n--- Page 51 ---\n50\nAppendixM\nBudgetSpending\nPriceper\nNo. Item Qty piece TotalPrice\nRaspberryPi7\"HDMILCD(C)IPS1024x600Capacitive\n1 Touch 3 85.70 257.10\nRaspberryPi3.5\"HDMILCD(B)IPS480x320Resistive\n2 Touch 2 57.50 115.00\n3 Magnet 3 3.50 10.50\n4 MicroSDCard32GB100MB/sClass10/SandiskwithOS 3 24.00 72.00\n5 RaspberryPiCamera5MP 3 14.00 42.00\n6 MomentaryPushSwitch12mm/Red 2 1.50 3.00\n7 MomentaryPushSwitch12mm/Green 2 1.50 3.00\n8 MomentaryPushSwitch12mm/Blue 2 1.50 3.00\n9 SPDTLimitSwitch13mm 6 1.00 6.00\n10 MicroHDMItoStandardHDMI(A/M)Cable/1m/Black 3 15.00 45.00\n11 BrassSpacerMale-FemaleScrew-NutM310mm4pcs 1 1.30 1.30\n12 BrassSpacerMale-FemaleScrew-NutM425mm4pcs 1 2.60 2.60\n13 Screw-NutM3x205pcs 2 1.00 2.00\n14 Screw-NutM3x255pcs 1 1.00 1.00\n15 M10Nut 4 - 1.00\n16 M8x40Bolt+WingNut 6 1.00 6.00\n17 4x50Bolt 4 - 2.00\n18 5x50Bolt 4 - 2.00\nx1.08(GST)\n19 MicroSDCard32GB100MB/sClass10/SandiskwithOS 2 24.00 48.00\n20 MLip&EyeM/UpR70ml 1 12.50 12.50\n21 SBH.DutyRefill1S 1 4.20 4.20\n22 MicroSDCard32GB100MB/sClass10/SandiskwithOS 2 24.00 48.00\n23 Lasercutaluminiumplate265mmx148mm 1 40.00 40.00\nx1.08(GST)\nChargingandDataTransferCable-ForSmartphones-1m-\n24 2A 3 2.16 6.48\n25 Pearl&MetallicPaint 1 5.90 5.90\n26 CarNonSlipMat50x150 1 8.20 8.20\n27 AluminiumPlate 1 8.00 8.00\n\n--- Page 52 ---\n51\n28 SoftClayBlack 2 2.16 4.32\n29 SoftClayBlack 2 2.16 4.32\n30 HTHexBoltGR838DIN933M10-1.5x40BlueZN 6 0.40 2.40\n31 HTHexBoltGR838DIN933M10-1.5x90BlueZN 13 0.70 9.10\n32 STLWingNutM10-1.5NP 19 0.60 11.40\nx1.08(GST)\n33 Switch 6 - 12.50\n34 VCEVEAH024in1USB3.0HubBlack 1 18.90 18.90\n35 ChargingandTransferCablemicroB1m2.4AAluminium 2 2.16 4.32\nChargingandTransferCablemicroB1m2.4AAluminium\n36 Mesh 1 2.16 2.16\n37 7CFSprayPaint#4 1 5.20 5.20\n38 PGP66014S4Way2MPowerExtensionSocket 1 16.90 16.90\nOmar'sOMWC013TypeC+USBPort20WWallCharger\n39 Black 4 19.90 79.60\n40 BrintsCo.3DPrintingServices 1 32.00 32.00\n964.81\nAccurateasof16April2023\n",
    "cleaned_content": "[PAGE BREAK]\nElectric Vehicle Additive Manufacturing(EVAM)Interior Design\nAUTHORS STUDENTID\nHe Tianli 1006151\nLim Yu Jie 1005999\nAditya Kumar 1006300\nEthan Choo E-Rhen 1006324\nCaitlin Daphne Tan Chiang 1006537\nChin Wei Ming 1006264\nAllteammemberscontributedequallytothetask.\nDesignand Artificial Intelligence,Singapore Universityof Technologyand Design\n60.003:Product Design Studio\nDr.Kwan Wei Lek,Dr Edwin Koh,Mr Michael Alexander Reeves\n24 April2023\n[PAGE BREAK]\nContents\nContents........................................................................................................................................................1\nBackground...................................................................................................................................................3\nUser Interview.........................................................................................................................................3\nAEIOUObservation Framework.............................................................................................................3\nValue Proposition.....................................................................................................................................3\nDesign Brief...................................................................................................................................................4\nProblem Statement...................................................................................................................................4\nDesign Specification................................................................................................................................4\nConcept Development..................................................................................................................................4\nFunctional Analysis.................................................................................................................................4\nMood Board.............................................................................................................................................6\nConcept Generation.................................................................................................................................6\nConcept Selection....................................................................................................................................8\nDesign Process..............................................................................................................................................9\nDashboard................................................................................................................................................9\nScreens..............................................................................................................................................9\nUser Interface..................................................................................................................................10\nSteering Wheel.......................................................................................................................................11\nGetting In/Out......................................................................................................................................11\nFinal Design................................................................................................................................................17\nRenders..................................................................................................................................................17\nSystem Diagram....................................................................................................................................19\nEvaluation...................................................................................................................................................20\nBudgeting...............................................................................................................................................20\nDashboard.......................................................................................................................................20\nSteering Wheel................................................................................................................................20\nProduct Weight......................................................................................................................................20\nClient Feedback.....................................................................................................................................20\nAreasof Improvement...........................................................................................................................21\nDashboard.......................................................................................................................................21\nSteering Wheel................................................................................................................................21\nGettingin/out..................................................................................................................................21\nReflection....................................................................................................................................................22\nAcknowledgement......................................................................................................................................22\nReferences...................................................................................................................................................23\nAppendix A.................................................................................................................................................24\nAppendix B.................................................................................................................................................25\nAppendix C.................................................................................................................................................26\nAppendix D.................................................................................................................................................27\n[PAGE BREAK]\nAppendix E.................................................................................................................................................29\nAppendix F..................................................................................................................................................30\nAppendix G.................................................................................................................................................31\nAppendix H.................................................................................................................................................32\nAppendix I...................................................................................................................................................37\nAppendix J..................................................................................................................................................40\nAppendix K.................................................................................................................................................45\nAppendix L.................................................................................................................................................48\nAppendix M................................................................................................................................................50\n[PAGE BREAK]\nElectric Vehicle Additive Manufacturing(EVAM)Interior Design\nAt the forefront of design and innovation, Singapore University of Technology and Design (SUTD) is\nmanufacturing components for their electric vehicle project, known as Electric Vehicle Additive\nManufacturing (EVAM). As the name suggests, they employfabricatingmethodssuchas Selective Laser\nMelting (SLM), Fused Deposition Modeling (FDM), Multi Jet Fusion (MJF) and Stereolithography\n(SLA).\nAs part of our 60.003 Product Design Studio, we have been tasked with designing the interior of the\nvehicle.\nBackground\nFirstly, we would need to understand the current users’ needs for the vehicle. As thevehicleisstillinits\ntesting stage, the users are mainly the engineering staff and students. To better understand future user\nneeds, we conducted user (engineer) interviews, the AEIOU Observation Framework, and a Value\nProposition.\nUser Interview\nWhen we first viewed the vehicle, we prepared some questions for Senior Specialist Mr Liew Zhen Hui,\nto understand whattheyrequiredustodesign.Thiswasfollowedupwithquestionsto Mr Matthew Dylan\nWong Jian Xiong for his vision and the concept for thecar.Thesequestionsandanswerscanbefoundin\nAppendix A. In summary, the concept of the car is to demonstrate additive manufacturing capabilities in\nthe manufacturing of a race car. It should be designed to look‘dynamic,aggressiveandsporty’,withthe\nnecessaryfunctionsrequiredforittobedriveninarace.\nAEIOUObservation Framework\nNext, weexploredthe AEIOU1 Observation Frameworkofthe EVAM.Amindmapofour AEIOUcanbe\nseenin Appendix B.\nValue Proposition\nLastly, based on our User Interviews and AEIOU, a valuepropositionwasplannedtocategorisepossible\nneeds into demands and wishes.Thedemandsarewhatwecanfullydesignwithinourtimelineandareof\nhighestpriority.Theyareasfollows:\n1. Dashboard. In order to enhance user experience, the users must be able to see all necessary\ninformationaboutthevehicleatalltimes.\n2. Steering wheel. To control more car settings for improved performance, the driver must be able to\nreachbuttons,whilestillfirmlyholdingthewheel.\n3. Mobility. The users should be able to get into andoutofthecarwithease,regardlessoftheirheight\norsize.\nOn the other hand, wishes are additional features that we could provide. They can be foundin Appendix\nC.Astoryboardwasdrawntobettervisualisethevalueproposition,foundin Appendix D.\n1 Acronymfor Activities,Environment,Objects,Interactionsand Users\n[PAGE BREAK]\nDesign Brief\nProblem Statement\nThrough our design process of empathise, define and ideate, we decided to design a means to enhance\nuserexperienceoftheinteriorofthevehiclewithaddedlightness.\nDesign Specification\nTo accomplish our problem statement efficiently, we merged certain design wishes with the demands to\ncreatethreecohesivedesigngoals:\n1. To design a dashboard, for both driver and passenger, that displays allnecessaryinformationatthe\nappropriate times, depending on its driving modes. These modes include static, demonstration and\nrace mode. The telemetry must, at least, include Speed, Temperature of battery and motor, Battery\nCharge,andmustbeintuitive.\n2. To design a steering wheel, with other controls besides steering input, for customizability of car\nfunctions. The design should be ergonomic so that the wheel is comfortable to hold and grip for\nprolonged periods, and also to reflect the SUTD identity onto the steering wheel design.\nAdditionally,controlsshouldbeeasilyreachablebyfingers,evenwhenthedriverissteeringthecar.\n3. To design a way for the trained driver and any able-bodied passenger to enter and exit thevehicle\nwithease,regardlessoftheirheightsorsizes.\nConcept Development\nThis means that although the final product is the interior of the goal, there is a need to design three\nseparatecomponentsthatcanstillcomplementeachother.\nFunctional Analysis\nHence, in our concept development, there is a need to perform a functional analysis for the three\ncomponents. Two types of analysis are the system and process perspectives, with the former shown in\nFigure1,andthelatterin Appendix E.\n[PAGE BREAK]\nFigure1\nFunctional Analysis System Perspectiveof Dashboard,Steering Wheel,Ingress/Egress\n[PAGE BREAK]\nMood Board\nFor the designofeachcomponenttocomplementeachother,thedesignsshouldbeinspiredbyacommon\nmoodboard,asseenin Figure2.\nFigure2\nDesign Mood Board\nAdditionally, a mood board was designed by DALL-E2. We wanted to use an AI-generated mood board\nfor inspiration. However, it did not align with our vision so we didnotuseit.Thegeneratedmoodboard\ncanbefoundin Appendix F.\nConcept Generation\nUsing functional analysis, a morphological chart can be created to aid in concept generation, as seen in\nTable 1. However, due to the complexity of the vehicle, onlyfunctionsrelatedtothesethreecomponents\nwillbeused.\n2 DALL-E is an Artificial Intelligence System that generates images for text input, created by Open AI. (Johnson,\n2021)\n[PAGE BREAK]\nTable1\nMorphological Chart Concept Generation\nWe selected and combined these functions to form our concepts. Since we have three design goals\n(dashboard, steering, getting in/out), a totalofnineconceptsisneeded.Thefunctionsfortheconceptsare\nhighlightedin Table1,usingthelegendin Table2.\n[PAGE BREAK]\nTable2\nLegendfor Function Selectionfor Concept Generationin Morphological Chart\nConcept1 Concept2 Concept3\nDashboard Red ㅤㅤㅤㅤ Orange ㅤㅤㅤㅤ Yellow ㅤㅤㅤㅤ\nSteering Wheel Green ㅤㅤㅤㅤ Cyan ㅤㅤㅤㅤ Blue ㅤㅤㅤㅤ\nGettingin/out Dark Blue ㅤㅤㅤㅤ Purple ㅤㅤㅤㅤ Pink ㅤㅤㅤㅤ\nConcept Selection\nThese concepts were then put through a selection criteria, seen in Appendix G, and rated among team\nmembers. The ratings are on a scale from 1 to 5, with 5 being the most ideal. With four criteria being\nlooked at, the score for a component ranges from 1 to 20. This issummarisedin Table3.Withsixgroup\nmembers, the total score for a component will range from 6 to 120. The highest scores would then\nundergothedesignprocessinthenextstep.\nTable3\nCriteriaand Rankingsof EVAMComponent Concepts\nDashboard Steering Gettingin/out\nTeam\nMember\n1 ㅤ 2 ㅤ 3 ㅤ 1 ㅤ 2 ㅤ 3 ㅤ 1 ㅤ 2 ㅤ 3 ㅤ\nEthan 18 16 9 5 17 19 17 5 8\nCaitlin 20 13 4 11 15 7 10 8 11\nTian Li 17 11 5 14 16 14 11 12 19\nWei Ming 18 15 7 6 15 11 13 20 16\nAditya 17 12 10 15 19 17 15 14 5\nYu Jie 19 13 4 5 18 7 10 8 11\nTotal 109 80 39 56 100 75 76 67 70\nNote:Theselectedconceptshavetheir‘Total’boxeshighlightedintheirrespectivecolourcodes.\n[PAGE BREAK]\nDesign Process\nOnce the concepts were selected, we began the design process. This process consists of ideation,\niterationsandfurtherdevelopment.\nDashboard\nThe dashboard could be split into two main components: user interface and screens. Each component\nwouldgothroughideation,iteration,anddevelopmentaswell.\nScreens\nInitially, therewereonlyplanstodevelopafrontdashboard.However,takinginspirationfromco-pilotsin\nfighter aircraft, a passenger dashboard was deemed necessary to assist the driver during races. Hence, a\ndashboard was required for driver and passenger, with each displaying different information. This\ninformation can be categorised into two categories: telemetry and camera views, which will be further\nelaboratedonunder‘User Interface’.\nThe front dashboard only had a 360 by 140 millimetre space while the rear dashboard had a 200 by200\nmillimetre space. At therear,therewasalsomorespacebelowthecentralsupportbar,butthatareahadto\nbe clear for the passenger to enter and exit the vehicle. Furthermore, only the upper section of the\ndashboard space could be used to narrow down the driver’s field of view, especially when he needs to\nfocus on the track. Hence, we decided to use two 3.5-inch LCDs and one 7-inch LCD at the front, and\nfive7-inch LCDsattheback.\nAt the front, the dashboard holder was designed more aesthetically than functionally. Taking inspiration\nfrom wind tunnel flow visualisation (Hall, 2021), and our mood board, wavy lines were etched into the\ndashboard, with incrementing depths of two millimetres. These lineswouldcontinuetowardstheinterior\nside panelling of the vehicle.Thefrontscreenholderisalsotiltedfivedegreesupwardstoaccountforthe\nelevated height of the perspective of the driver. Similarly, the screens are tilted five degrees inwards to\nfacethedriver.Anisometricviewofthisdesigncanbefoundin Appendix HFigure H1.\nTo account for ease of manufacturing and theme of additive manufacturing, the screen holder had to be\nbroken up to fit the 3 D Printer bed size. In the end, the dashboard was divided into two, with excess\nmaterial trimmed for added lightness. A screen cover was added to seal the screen in the dashboard, as\nwellastohideanyexposedelectronics.Itsassemblycanbefoundin Appendix HFigure H2.\nAt the back, we decided to implement adjustable screen holders. This is because unlike the user of the\nfront seat, who is usually thesametraineddriver,theuserofthebackseatcanbeanyable-bodiedperson,\nregardless of his/her height. This means that different passengers may have different perspectives,hence\nthe screen tilt must be different. Furthermore, as we chose to utilise the space below the central support\nbeam for more screens as well, the screens should be able to tuck away so that itiseasierfortheuserto\ngetinandoutofthevehicle.\nWe based the design of theseadjustablescreensonmonitorstands.Thisusuallyinvolvesarmswithjoints\nor hinges that can be tightened or loosened. We simulated this design by 3 D printing arms, which were\nconnected using M10 hex boltsandwingnuts.Thismeansthatthejointscouldbetightenedandloosened\nusing the wing nuts. Similarly, the clamps that held the screen holders to thecentralsupportbeamofthe\nchassis could be tightened and loosened to provide a cylindrical joint for further adjustment by the\npassenger.\nWhile sharing with the engineering team about our design plans,theytoldusthattestersoftenholdonto\nthe central support beam to steady themselves to counter the G-forces during acceleration and\ndeceleration. This means that the initial plan for placement of screens wouldblockthesupportbeam.As\n[PAGE BREAK]\nwe did not want to eliminate this, we decided to implement grips into the clamps. This would not only\nallow passengers to support themselves during high lateral forces, but also for ease in adjusting the\nclamp’s tilt. The final design can be found in Appendix H Figure H3, with its exploded assembly in\nFigure H4.\nThe parts list for these dashboards can be found in Appendix H Table H1 while the fasteners usedarein\nTable H2.\nUser Interface\nDue to the different modes, two different interfaces must be designed: Race and Demonstration. The\ncodes for this (including all the connections of the user interfaces to the various components of the\nsteeringwheel)canbefoundatthisrepository:https://github.com/Caitlin Chiang/EV-Screens\nRace. Inspiration was taken from F1 cars because the car concepts are similar: open wheel racing.\nSpecifically, we took inspiration from the Aston Martin Aramco Cognizant F1 Team, which displayed\ntheir wheel layout and the telemetry for each. The race driver would require telemetry such as speed,\ntyre/brake/motor/battery temperatures, battery charge,racedeltas,laptimings,brakebiasandmigration,\nrace positions, laps done and drinking water remaining. Allthisdataiscolourcoded,withorangebeing\ntoo hot or almost depleted, blue being too cold or half used, and white being optimum temperature or\nfull. The race-standard colours purple, green and white are used forracetimingsaswell.Thisissothat\nthedrivercaninstantaneouslytelliftheirbrakes/tyres/motor/batteryistoowarmorcold.\nAs for the camera views, they were placed at the usual positions that therearviewmirrorswouldbein\nanordinarycar.Thisissothatitismoreintuitive.\nWe tested the designs by printing out the interface onto paper the size of the actual screens. We then\npasted them on the vehicle and tested how readable the interface is, as seen in Appendix I Figure I1.\nInitially,thefontsweretoosmallsoweincreasedthesizes.Additionally,wedecidedtoremovethefront\nfacing camera as thespeedometerwastoolowandwasblockedbythesteeringwheel,sincethespeedis\nofhigherimportancethanthefrontfacingcamera.\nThefinalinterfacecanbefoundin Appendix IFigure I2.\nDemonstration. The first iteration was a simple layout only including the absolute essential telemetry\ngiven to us during the interview, but we realised that a lot of spaceisbeingputtowaste,andthatusers\nactually don’t know that seeing certain data is convenient untilwealreadypresentittothem.Withthat,\nwe iterated another design, this time adding telemetrythatwethinkisnecessarybasedonresearch.The\ntelemetryhereisthesameastheonesusedinracemode,exceptdesignedandpresenteddifferently.\nOn the third iteration, we focused more on the experience of the interfaces and making sure that they\nwere pleasing to look at.Westartedtousecolourstodepictcertainvalues(e.g.usingredwhenacertain\npart is overheating), and placed telemetry in a way that made more sense to the user, allowing it to be\nmore intuitive as well. We also added camera views here, the same as race mode, tothepositionsasto\nwhererearviewmirrorswouldnormallybe.\nThe final iteration includedcreatingsimilarbutdifferentinterfacessuchthattheusercanswitchtothem\nat appropriate times, and therefore be able toseecertaintelemetryatspecificsituationsatalargerview.\nWekeptrefiningthemsuchthatthereisaflowtowhichgetsshownatvariousbuttonpresses.\nThefinalinterfacecanbefoundin Appendix IFigure I3.\n[PAGE BREAK]\nSteering Wheel\nThe design process for our steering wheel began with a collaborative effort among team members, with\neachcontributingtheirbestideastocreateaninitialconcept.Werefinedthedesignthroughtrialanderror,\nexperimenting with specifications like size and button placement to achievethedesiredfunctionalityand\naestheticswhichcanbeseenin Appendix JFigure J1 and Figure J2.\nHowever, we realised that we had overlooked important factors such as budget and manufacturing\nlimitations, which led us to revisit the design and incorporate these factors. The iterations of steering\nwheeldesignscanbeseenin Appendix JFigure J3.\nTo achieve a practical and cost-effective design, we began with a robust and lightweight aluminium\nchassis that provided structural support and stability. We carefully designed the chassis with cutouts for\nbuttons and screws, ensuring a precise and secure fit for all components asseenin Appendix JFigure J4\nand Figure J5. Another point to note is that, when we designed the steering wheel, we made sure to\nposition the buttons in a way that would allow for easy access by the thumb for an average adult male.\nThe button placements, as well as their functions can be seen in Appendix J Figure J6 and Table J1\nrespectively.\nThe handles, buttons, and button shell were 3 D printed using advanced additive manufacturing\ntechniques, allowing for a high degree of customisationandprecision.Thisreducedproductioncostsand\nlead times while also ensuring optimal functionality and aesthetics. The paddle shifters were designed\nwith simplicity andeffectivenessinmind,featuringamicroleverswitchtowhichthepaddleattaches.We\nutilised a magnet instead of a traditional spring to provide haptic feedback, enhancing the tactile\nexperiencefortheuser.Thelistoffastenersusedtoassemblethesteeringwheelcanbefoundin Appendix\nJTable J3.\nTo cover the wires coming out the back of the steering wheel, we designed and 3 D printed a custom\ncasing that also housed the paddle shifters. This provided a clean and professional look while ensuring\nthatthewireswereprotectedfromdamageandinterference.\nOverall, our steering wheel design incorporated a range of advanced manufacturing techniques and\nmaterials, from the precision-cut aluminium chassis to the 3 D printed components and custom casings.\nEvery element was carefully considered and optimised for maximum functionality and user experience,\nresultinginahigh-performanceandaestheticallypleasingproductasseenin Appendix JFigure J7.\nGetting In/Out\nThis particular aspect of the EVAM’s experience was rather tricky to enhance. Mainly because of the\nspace constraints of the car’s interior,aswellashowthecarwasnotinitiallydesignedforaneasyingress\nand egress in mind. Both this obstacle has created a particularly awkwardwayfortheuserofthevehicle\ntogetinandoutofthecarasillustratedin Figure3 below.\nHerearethestepsfortheingressandegressprocesses:\nIngress\n(1)Putonefootonthechassis\n(2)Putbothfeetontotheseat\n(3)Usingthesidechassisassupport,theuserwillslowlylowerhim/herself\n(4)Userwillthensuccessfullygetintothecar\n[PAGE BREAK]\nEgress\n(1)Holdonthethesidechassis\n(2)Pushhim/herselfup\n(3)Standupontheseat\n(4)Steponefootontothechassis\n(5)Steptheotherfootontothehigherchassisbarandeventuallygetoutofthecar\nFirst, we will illustrate with pictures how an average person enters the back seatofthevehicle.Wehave\ndecided to put our main focusonthebackseatbecausewehavebeeninformedthattheuserprofileofthe\nfrontseatwillbeafitandathleticracer.\nFigure3\nProcessthatusergetsintovehicle(fromlefttoright,toptobottom)\n[PAGE BREAK]\nAs illustrated in the pictures above, it is extremely uncomfortable and awkward even for a ratherfitand\nathletic person. So imagineifduringashowcaseofthecar,anoldpersonwouldliketositinthecar.With\nthecurrentdesign,itwillbeverydifficultforthattohappen.\nBefore we start going intoideatingwhatadesignsolutionforthiswouldbe.Wefirstanalysewhatarethe\ntouchpoints, by hand and foot, of different users when they get in and out of the car. And eventually\nbringing us to understand in which area of the car is commonlyusedassupportwhentheyenterandexit\nthecar.Anillustrationofthecommontouchpointsisshownin Appendix KFigure K1.\nFurthermore, we had to understand what are the space constraints we are limitedtowithinthecar.Todo\nthis, using Fusion 360, we modelled out the space inside the car, allowing us to know the exact\nmeasurements of space that we could work with. An illustration of the space constraint is shown in\nAppendix KFigure K2.\n[PAGE BREAK]\nAfter which we exploredmultipleideasasseenin Appendix KFigure K3,whicheventuallywerealisedis\nnot very practical. One of the ideas was to lift the seatuptoacertainlevelthatwillbeeasierforusersto\nget into a sitting position first, then using some sort of mechanism to slowly lower the seat down and\nsame for bringing the seat up. But we were soonnotifiedby Mr Liewthatwearenotabletodisassemble\nthe seat, therefore this idea was removed. Another idea wastohavepanelswhichareabletoflapopenas\nstairs for the user to step intothecarinmultiplelevels,butitwassoonrealisedthattheseflapswouldnot\nbeabletosupporttheweightofanadultperson.\nSoon after, we realised that the most amount of space that is availableinsidethecar,isactuallythesides\nof the seats. Which led us to think of designingamodularsidecompartment,whichservesasanarmrest,\nstorage space, as well as steps toeasetheingressandegressofusers.Italsoactsasaspaceforfunctional\nbuttonsthatwerenotplacedonthesteeringwheel.\nHaving used Fusion 360 to model out what the side compartment looks like, we ensured that the sizeof\nthe armrest will be within the size constraint of the interior of the EVAM. Figure 4 shows the design of\nthemodularsidecompartment,while Figure5 and Figure6 showitsfunctionalities.\nFigure4\nDesignsforeasingingressandegress\n[PAGE BREAK]\nFigure5\nDesignforeasingingress\nFigure6\nDesignforeasingegress\n[PAGE BREAK]\nAnother issue that we found was that there is no place forthedrivertoresttheirheelwhenoperatingthe\nvehicle. Using Midjourney, an AI image generator, we asked it to show us designs of a F1 heel rest for\ninspiration.Generatedimagescanbefoundat Appendix KFigure K4.\nWe then proceeded to design a heel rest, which also includes a leg rest that will further aid the driver to\nget inandoutofthecar.Furthermore,whenourteamtriedgettingintothevehicletotestoutthespacewe\nhad, there was one engineer whowasworriedthatourteam,beinginexperiencedwiththecar,wouldstep\non some of the wiringthatwasexposed.Havingaddedalegrest,therewasnowacoverforthewiresand\nnolongertheworryofsteppingonsensitivewiring.\nFigure7 belowshowsapictureoftheheelandlegrest.\nFigure7\nDesignforheelandlegrestforfrontseat\nThe design for the heel rest is heavily inspired by the Midjourney generated images. As for the leg rest,\nthe driver is able to ‘feel’ his/her legs into the car and eventually reach thepedals.Wavypatternscanbe\nseen throughout the design of the car, these patterns are made rubbery with very high friction which\nserves as steps users take. A better illustration of the wavy patterns on the rest of the car can be seen at\nAppendix KFigure K5.\n[PAGE BREAK]\nFinal Design\nRenders\nFigure8\nOverheadviewofthefinalinteriordesignofthe EVAM\nFigure9\nOverheadviewofthefinalinteriordesignofthe EVAM\n[PAGE BREAK]\nFigure10\nDriver’sperspectiveofthefinalinteriordesign\nFigure11\nPassenger’sperspectiveofthefinalinteriordesign\n[PAGE BREAK]\nSystem Diagram\nTo illustrate the new functionsofthedashboardsandsteeringawheel,a System Diagramhasbeendrawn\nasseenin Figure12.\nFigure12\nSystems Diagramof EVAMInterior Design\nThiscanalsoberepresentedinastoryboardforthesolutions,asseenin Appendix L.\n[PAGE BREAK]\nEvaluation\nBudgeting\nFor this project, we were given a budget of $1000 fortheproductionofourproduct.Wewillbebreaking\ndown how much was spent in the making of each component in this section, as well as justification of\nwhy these purchases are necessary for our project. The detailedlistofhowmuchwasspentcanbefound\nin Appendix M.\nDashboard\nA total of$737 ofthebudgetwasspentoncreatingthedashboard.Itwasparticularlyexpensivebecause\nof the display that is compatible with the raspberry pi module, that is necessary for us to showexactly\nwhatitwouldlooklikeexactlyintheactualcar.Secondreasonbeingthatwewereconstantlyfacedwith\nthe issue of not supplying enough power to the Raspberry Pi and its display. And soon later found out\nthat for each screen and Raspberry Pi,itneededaseparatepowersourcewhichwethenneededtobuya\ncharging plug for each display. For each screen, we also needed a SD card slotted into it for it to\nfunction. Therefore a part of the budget also went to the purchaseof SDcards.Otherthanthat,wealso\nintended 2 of our screens to display the live rear camera view in the car. Therefore we purchased 2\nraspberry pi cameras to connect to the Raspberry Pi which eventually is able to display camera views\nonto the Raspberry Pi display that we purchased. Other than that, we purchased quite afewcablesthat\nare necessary to connectbetweenthe Raspberry Pimodule,thedisplay,andaswellasthepowersource.\nFor each display, 1 x HDMI to HDMI cable, 2 x USB to micro USB cable is required. Thereforewith5\ndisplays,weneeded5 x HDMIto HDMIcable,and10 x USBtomicro USBcable.\nSteering Wheel\nA total of $138 of the budget was spent on the production of the steering wheel. When creating the\nbase metal plate of the wheel, we first attempted to purchase our own metal plate and cut it out in the\nFabrication Lab (Fab Lab), but was soon met with the obstacle of it being too heavy and difficult in\ncutting precisely to scale. We then seeked help from an external source which provided us with alaser\ncutting service as well as a good lightweight material that is most suitable for our use case. Some\namount ofthebudgetwasalsospentonswitchesthatwerenecessaryforustomakefunctionallybuttons\nin the steering wheel. We also spent some amount to purchase the screws and nuts that were not\nprovided by Fablab mainly because it didn't have the length and size that we needed. Lastly, we\npurchased spray paints for the final finish of the steering wheel. Note that someoftheitemspurchased\nlikeclayandnon-slipmatswerefortherapidprototypingphaseofthesteeringwheel,andwerenotused\nin the production final steering wheel. Finally, we also seek help from an external 3 D printing service\nfortheproductionofourbackcasingforbetter3 Dprintingfinishandquality.\nProduct Weight\nAs seen in Appendix H Table H1 and Appendix J Table J2, the total weight of fabricated products is\nestimated to be approximately 3.628 kilograms. This is below our design specification wish of five\nkilograms. This means that additional functionality would have a very minor dentontheperformanceof\nthevehicle.\nClient Feedback\nThe client was particularly impressed with the seamless integration of the steeringwheelanddashboard.\nThey felt that the design concept was innovative and unique, and appreciated the attention to detail that\nwent into creatingacohesiveandaestheticallypleasingproduct.Theclientnotedthatthedesigncreateda\nmore streamlined and ergonomic experienceforthedriver,whichtheyfeltwasasignificantimprovement\noverthetraditionaldesign.\nWhile the client was impressed with the overall design concept and integrationofthesteeringwheeland\ndashboard, they did have some minor critiques. The use of trial-and-error fordeterminingmeasurements\n[PAGE BREAK]\nwas seen as a potential setback, as the client recommended the use of Anthropometric data for more\nprecise results. Additionally, the client suggested modifying the line width and increasing the infill\ndensity for the 3 D prints in order to enhance the durability of the product, especially if it will be used\nwhiledriving.\nDespite these minor setbacks, the client remained highly satisfied with the design and expressed\nexcitementforthepotentialoftheproduct.\nAreasof Improvement\nDashboard\nDue to the lack of time, more iterations of the screen holders could not be fabricated. The front\ndashboard holder should be designed to hide the Raspberry Pi. Similarly, the passenger dashboard\nshould have a way to hide the cables that run out of it. Screen casings should be made, together with\nsealant, to weatherproof the electronics. A more flexible material should be usedtomakethehingesso\nthatthehingescantightenbetter.\nIn terms of improvements in regards to the user interfaces, the various functionalities of the steering\nwheel can have more impact on the displays aside from changing the interface completely from one\nmenu to another.Instead,wecanhavethevaluesonthedisplayitselfchangedependingonthetiltofthe\ncar. Furthermore, we aim to include warning interfaces that have the ability to completely override the\ncurrent interface being displayed to the driver / passenger (with the ability to close it quickly) when a\ncertain vehicle component is overheating or underperforming. This way, the driver and passenger can\noperate more safely and with a better peace of mind, knowing that anything that goes wrong will be\nalertedbeforeincriticalcondition.\nSteering Wheel\nDuring the design and manufacturing process of our steering wheel, we encountered some challenges\nthat revealed areas for improvement. Specifically, we found that our wiring management was lacking\nand our soldering skills needed improvement, resulting in a delicate final product.Additionally,the3 D\nprinting process had some flaws that required workarounds. Furthermore, we were unable to complete\nthe setup of the dashboard due to lack of planning and insufficientlylongwires,whichcausedconstant\nreboots. These challenges highlight the need for more comprehensive planning and better execution of\nmanufacturingprocessesinthefuture.\nGettingin/out\nSeeing how the rest of the components blend so seamlessly with the interior of the car. This particular\ncomponent that is aiding the ingress and egress process for the user, specially seemed like a complete\nseparatecomponentanddoesnotentirelyblendintotheinteriorofthevehicle.Therefore,morethoughts\non how these side compartments could blend more seamlessly with thedesignoftherestoftheinterior\nofthecomponentswoulddefinitelyimprovetheoveralldesignofthevehicle’sinterior.\nAlso because this is one of the more obvious components when someone looks into the car, mainly\nbecause of its size compared to other components. We could also put more thoughts on how its\nfunctionality and aesthetic of the design of the sidecompartmentcouldbetterreflectthe SUTDidentity\nof being future-oriented, design-centric, and interdisciplinary. So that when visitors come and visit the\nEVAM, they are constantly reminded that this is specially produced by SUTD students and as well as\nthe SUTDidentity.\n[PAGE BREAK]\nReflection\nAs mentioned in the client feedback, our group didnotuseprecisemeasurements.Thisledtoissuessuch\nasthebackcasingofoursteeringwheelnotfittingthebaseplateexactly.Assuch,wehadtofiledownthe\nback casing inordertofitittothebaseplate.Havingfacedthisavoidableissue,ourgroupnowknowsthe\nimportanceoftakingprecisemeasurements,especiallyinthepreliminarystagesoftheproject.\nOur group also made use of Dall-E and Midjourney in our project to give us inspiration. While theydid\nhelp us make the design process easier, we understand that it is only a tool to aid us, and should not be\nused as a solution in itself. There is still the need for humans (i.e.ourgroup)togetinvolvedtotestouta\ndesign and see if it works for our project. This can be seen when we decided that the mood board\nprovided by Dall-E did notsuitourvisionandthatwewouldnotwanttouseit.Anotherexampleiswhen\nwe used Midjourney to give usdesigninspirationsforthefootrest.Westillhadtogodowntothevehicle\nand see ifthefootrestwasindeedabletobeimplemented,withtheconstraintmainlybeingtheamountof\nspacewehad.\nEffective and precisecommunicationofideasisverycrucialattheideatingpartofthisproject.Especially\nwhen we needed to explore and compare between different design solutions. Therefore, theskillofhand\nsketching in a 2 Ddiagramiscrucial,inthesensewhereweareabletoexplainourideastoeachotherina\nquick and precise manner. After which when this particular idea seems like a good one in the 2 D\nperspective, we will then have to model it out in any 3 D software available and 3 D print it for us tosee\nthe component physically.Wewillthenplaceitintheactualpositioninthevehicleofwhereitisintended\nto be. These steps will then allow us to analyse possible issues that may arise with thisparticulardesign\nsolution,whichwecantheniterateandimproveonuntilwereachourfinaldesignsolution.\nAcknowledgement\nWe would like to express our deepest gratitude to Mr Liew and the rest of theteamfortheirguidancein\nour project, and to the EV Club’s Wei Ming for taking the time to come down tothe Fab Labtoassistus\nwithmeasurementsandtesting,despiteherbusyschedule.\nWe would also like to acknowledge with gratitude, the support from our professors and supervisors. We\nwould like to thank Mr Michael Alexander Reeves and Dr Edwin Koh, for giving us different\nperspectives on the project, aswellas Dr Kwan Wei Lekforhelpinguswiththeconnectionofelectronics\nandprovidinguswithmultiplemicrocontrollers.\nSpecial thanks to Ms Zerline Tan for ensuring we have a conducive place to work in. Last but not least,\nthank you to our Teaching Assistants (TAs), Ke Wei and Eugene for sparing time to give us insights on\nexistingcardesignswecanuseasinspirationforourproject.\nSpecial mentions to the exterior team consisting of Aaron, Billy, Joshua, Jun Xiang, Valencia and Wei\nXuan. It was thanks to the collaboration between both teams that we managed to showcase the EV as a\nwhole,ratherthanseparateprojects.\nIt is thanks to all of these people that our group managed tocompletethisprojectwithsuchsuccess,and\nimpressourclientswithourfinalphysicalprototypesandproduct.\n[PAGE BREAK]\nReferences\nHall,N.(2021,May13).Smokeand Tuft Flow Visualization.Wind Tunnel Index.Retrieved April24,\n2023,fromhttps://www.grc.nasa.gov/www/k-12/airplane/tunvsmoke.html\nJohnson,K.(2021,January5).Open AIdebuts DALL-Eforgeneratingimagesfromtext.Venture Beat.\nRetrieved April21,2023,from\nhttps://venturebeat.com/business/openai-debuts-dall-e-for-generating-images-from-text/\n[PAGE BREAK]\nAppendix A\nInterviewtranscriptwith EVAMEngineers\nThefollowingquestionswereaskedto Senior Specialist Mr Liew Zhen Hui:\n1. Whatisyourbiggestfocusindesigningtheoperationsofthevehicle?\nThegoalisracing,hencespeedismostimportant.\n2. Whataresomeissuesyoufaceintheoperationofthevehicle?\nThereisnocarinformationcommunicatedtothedriver.\n3. Aretherespecifictelemetriesyouwouldliketosee?\nSpeed,Temperatureofbatteryandmotor,Battery Charge.\n4. Iscomfortimportant?\nThedrivermustadapttothedesign,butsupporttotheneck,shouldersandlumbarisabonus.\nAs afollowup,wetextmessagedastudentengineerwhohelpeddesignthecar,Mr Matthew Dylan Wong\nJian Xiong (2023), to offer his perspective on the vehicle. We asked what his original vision for the car\nwas:\nThe concept of the caratitscorewastoexploitadditivemanufacturingasmuchaspossibletodothings\nthatarenotconventionallypossibleandtomakethefabricationprocessmoreefficient.\n(1) Explore how additivemanufacturingcanbeusedtofabricatethebodyshellparts.Ahugeproportion\nofthemoneyspentonthecarisgoingintothe3 dprintedmetaljointsontheframe.\n(2)Highlightthe3 dprintedjointsofthecar.Thisiswhy Itriedtoframetheminsizethesidewindow.\n(3) The car was originally meant to be a \"formula-style race car\". It has a low ride height, wide track\nwidthandlowoverallheightthatgivesitanaggressivestance.\n(4) The design ofthecarshouldbetosomeextent,dynamic,aggressiveandsporty.Itshouldlooklikeit\ncan go fast even when it's not moving. At thesametime,thereshouldbeaerodynamicelementstosend\nthemessagethatthiscarismeantforperformance.\n[PAGE BREAK]\nAppendix B\nAEIOUFramework Observation Mindmap\nNote:Thenodes’relationshipsarerepresentedbythedashedlines.\n[PAGE BREAK]\nAppendix C\nValue Proposition Wishes\n1. Weight\nThe additional components are lightweight (below 5 kg) to enhance performance despite additional\nfunctions.\n2. Hydration\nThe users should be able to have access to water easily so they can sustain themselves during\noperationofthevehicle.\n3. Safe\nUsersmustbeabletoberemovedfromthevehicleswiftlyandsafelyifanaccidentweretohappen.\n4. Communication\nUsersneedtobeabletocommunicatewitheachotherdespitethenoise.\n5. Comfort\nThecockpitshouldstaycool,andtheseatingpositionsshouldbecomfortable.\n6. Fabrication\nThe components should be easy and inexpensive to manufacture, preferably done using additive\nmanufacturingtosupporttheoriginalconceptofthevehicle.\n7. Affordable\nControlsareintuitive.\n8. Enjoyable\nUserswillhaveapositiveexperiencefromridinginthevehicle.\n[PAGE BREAK]\nAppendix D\nStoryboard(Problems)\n[PAGE BREAK]\n[PAGE BREAK]\nAppendix E\nWorkflowof Functional Analysis Process Perspective\n1. Initialisethevehicle\na. Receiveuser\nb. Receiveinputtostartthevehicle\nc. Initiatestartupprocess\nd. Receiveinputsonusersettings,ifany\n2. Operatethevehicle\na. Receiveinputsfordrive/reversemode\nb. Receiveinputsonusersettingchanges,ifany\nc. Displayinformationoftheuser’ssettings\nd. Receiveinputsonvehicle’sstatus,ifany\ne. Displayinformationonthevehicle’sstatus\nf. Physicallysupportuser\ng. Hydrateuser\nh. Cooluser\ni. Receivevoiceinputfromdriver/passenger\nj. Relayvoiceoutputtodriver/passenger\n3. Deactivatethevehicle\na. Receiveinputforemergencystop,ifany\nb. Receiveinputtoshutdownthevehicle\nc. Initiateshutdownprocess\nd. Savecurrentinputsofusersettings\n[PAGE BREAK]\nAppendix F\nAIGenerated Mood Board\n[PAGE BREAK]\nAppendix G\nCriteriaforscoringandrankingdesignconcepts\nCriteria Score\nFunctionality 1 2 3 4 5\nEaseof Fabrication 1 2 3 4 5\nEaseofuse 1 2 3 4 5\nCost 1 2 3 4 5\nTotal\nNotesfromselection\nDashboard\n1. Using the Raspberry Pi computer was most ideal because our team could borrow them from our\ninstitution,SUTD,allowingustostaywithinourgivenbudget.\n2. Fordisplayinginformation,the LCDscreenswerechosenover Tablet Devicesastheywouldbetoo\nexpensive, and over e-ink as it cannotrespondfastenoughforhighstresssituationssuchasracing.\nLastly, a rocker switch was chosen over its other concepts because it gives a tactile sensation to\nstartingandstoppingthecar,withtwoalternativeoptions:startandstop.\nSteering\n1. Buttons were chosen over all other options for the steering as in a racing scenario, timing to\nobtain required information is extremely important, and buttons are the most effective in\nminimising the movement and time needed to obtain the desired dashboard information output,\nmakingitthebestchoice.\nGettingin/out\n1. We chose to use groovedpadsoverrubberpadsbecausegroovedpadsprovidemoregripstrength\non the shoe to prevent any slipping. We can also incorporate awavydesignintoitforaesthetics,\nwhichiswhatwewantasshowninourmoodboard.\n2. Steps have been chosen over leg rests as we realised that the height difference when getting in\nand out of the car is very deep. By adding steps, it serves asacomfortableintermediarypointof\nmanoeuvringtomaketheprocesssmoother.\n3. We decided to use handles for the back seat as we observed that it is easier for drivers to push\nthemselves upwards as compared to pulling themselves up by holding onto the chassis when\ngettingoutofthecar.\n4. We have also implemented grooved heel rests for the front seat as during our investigation, we\nrealised that there is a gap between the heels of the driver when their feet rests on the pedals,\nmaking it tiring to hold the driving position for long periods of time. Additionally, whengetting\nout of the car,driversalwayspushthemselvesupusingtheirlegs,andbyaddingheelrests,italso\nservesasagripmakingiteasiertoexitthecar.\n[PAGE BREAK]\nAppendix H\nDashboards Screens Schematics\nFigure H1\nInitialdesignof Front Dashboard\nFigure H2\nFinalexplodedassemblyviewof Front Dashboard\n[PAGE BREAK]\nFigure H3\nFinalfullyassembled Passenger Dashboard\nNote.Redarrowsindicatepossiblemovementofjoints.Notalljointscanbeseen.\n[PAGE BREAK]\nFigure H4\nFinalexplodedassemblyviewof Passenger Dashboard\n[PAGE BREAK]\nTable H1\nPartslistusedin Dashboards\nLabel Part Weight(kg)\nA Main Dashboard Bodies 0.347\nB Raspberry Pi3 Model B3\nC Waveshare3.5 inch RPi LCD(B)4\nD Waveshare7 inch Resistive Touch Screen LCD,1024×600,HDMI,\nIPS5\nE Screen Cover\nF Top Screen Rocker Arms 2.700\nG Top Clamp Arm\nH Top Middle Clamp\nI Bottom Middle Clamp\nJ Top Left Clamp\nK Bottom Left Clamp\nL Right Top Clamp\nM Right Bottom Clamp\nN Bottom Arm\nO Bottom Hinge\nP Screen Holder\nQ Waveshare7 inch Resistive Touch Screen LCD,1024×600,HDMI,IPS\n3 CADModeladaptedfrom\nhttps://grabcad.com/library/raspberry-pi-3-model-b-reference-design-solidworks-cad-raspberry-pi-raspberrypi-rpi-\n4 CADModeladaptedfrom:https://grabcad.com/library/raspberry-lcd-3-5 inch-1\n5 CADModeladaptedfrom:https://www.waveshare.com/wiki/File:7 inch_HDMI_LCD_3 D_Drawing.zip\n[PAGE BREAK]\nTable H2\nFastenerlistusedin Dashboards\nS/N Fastener Quantity Joint\n1 M8 Wing Bolt75 mm 4 A(Screw Clamp)\n2 M8 Hex Nut 8 A(Screw Clamp)\n3 M2 Screw20 mm+Nut 4 Band A\n4 M3 Screw20 mm+Nut 4 A,Dand E\n5 M10 Hex Bolt+Wing Nut+Spring Lock Washer 13 H and I, Jand K,Land M,\n(90 mm), 6 F/O and P, G/N and F/O,\n(40 mm) H/I/K/Mand G/N\n6 M6 Hex Bolt 40 mm + Hex Nut + Spring Lock 8 H/J/Land I/K/M\nWasher\n7 M3 Screw20 mm+Nut 20 Pand Q\n[PAGE BREAK]\nAppendix I\nUser Interfacefor Screens\nFigure I1\nPhysical Testingofthe User Interface\n[PAGE BREAK]\nFigure I2\nFinal User Interfacefor Race Mode\nFigure I3\nFinal User Interfacesfor Demonstration&Static Mode\n[PAGE BREAK]\n[PAGE BREAK]\nAppendix J\nFigure J1\nInitialsketchesofsteeringwheeldesigns.\nFigure J2\nIterationsoffinalsteeringwheeldesigns\n[PAGE BREAK]\nFigure J3 Figure J4\nSteeringwheelphysicalcutouts Isometric Viewof Fully Assembled Steering Wheel\nFigure J5\nFinalexplodedassemblyviewof Steering Wheel\n[PAGE BREAK]\nFigure J6\nButton Layoutfor Steeringwheel\nFigure J7\nFinaliterationofthesteeringwheel\n[PAGE BREAK]\nTable J1\nButton Functionsfor Steering Wheel\nButton Function\nA Switchtheuserinterfaceonthescreentoshowtheleftcameraview.\nB Switchtheuserinterfacetothemainmenuofracemode(driver).\nC Turnontheleftlightindicatorsofthevehicle(theoretically,notimplemented).\nD Turnontheheadlightsofthevehicle.\nE Turnontheheadlightsofthevehicle.\nF Turnontherightlightindicatorsofthevehicle.\nG Switchtheuserinterfacetothemainmenuofracemode(passenger).\nH Switchtheuserinterfaceonthescreentoshowtherightcameraview.\nTable J2\nPartslistusedinsteeringwheel\nLabel Part Weight(kg)\nA Steeringwheel Chassis 0.581\nB Rightbuttoncasing\nC Leftbuttoncasing\nD Backcasingforthesteeringwheel\nE Rightpaddleshifter\nF Leftpaddleshifter\nG Quickreleasemechanism\n[PAGE BREAK]\nTable J3\nFastenerlistusedinthe Steering Wheel\nS/N Fastener Quantity Joint\n1 M3 Screw20 mm+Nut 2 Connectingthe Paddle\nShifterstothe Micro Lever\nSwitch\n2 M3 Screw25 mm+Nut 4 Attachingthe Paddle\nshifterstothebackcasing\n3 M5 Screw20 mm+Nut 6 Bindthe Quick Releaseto\nthe Steering Wheel\n4 M5 Screws25 mm+Nut 6 Boltingthe3 Dprinted\nHandlestothe Steering\nWheel\n5 Neodymium Magnets(2 mm) 4 Providingthenecessary\nforceforpaddleshifter\nfeedback\n6 M3 Screws25 mm+Nut 6 Tighteningthe Button\nCasingtothe Steering\nWheel\n[PAGE BREAK]\nAppendix K\nFigure K1\nTouchpointanalysationforingressandegressofthevehicle\nFigure K2\nSpaceconstraintswithinthe EVAM\n[PAGE BREAK]\nFigure K3\nDesignexplorationforingressandegress\nFigure K4\nMidjourneygeneratedheelrestimages\n[PAGE BREAK]\nFigure K5\nIllustrationofstepsthroughoutthecar.\n[PAGE BREAK]\nAppendix L\nStoryboard(Solutions)\n[PAGE BREAK]\n[PAGE BREAK]\nAppendix M\nBudget Spending\nPriceper\nNo. Item Qty piece Total Price\nRaspberry Pi7\"HDMILCD(C)IPS1024 x600 Capacitive\n1 Touch 3 85.70 257.10\nRaspberry Pi3.5\"HDMILCD(B)IPS480 x320 Resistive\n2 Touch 2 57.50 115.00\n3 Magnet 3 3.50 10.50\n4 Micro SDCard32 GB100 MB/s Class10/Sandiskwith OS 3 24.00 72.00\n5 Raspberry Pi Camera5 MP 3 14.00 42.00\n6 Momentary Push Switch12 mm/Red 2 1.50 3.00\n7 Momentary Push Switch12 mm/Green 2 1.50 3.00\n8 Momentary Push Switch12 mm/Blue 2 1.50 3.00\n9 SPDTLimit Switch13 mm 6 1.00 6.00\n10 Micro HDMIto Standard HDMI(A/M)Cable/1 m/Black 3 15.00 45.00\n11 Brass Spacer Male-Female Screw-Nut M310 mm4 pcs 1 1.30 1.30\n12 Brass Spacer Male-Female Screw-Nut M425 mm4 pcs 1 2.60 2.60\n13 Screw-Nut M3 x205 pcs 2 1.00 2.00\n14 Screw-Nut M3 x255 pcs 1 1.00 1.00\n15 M10 Nut 4 - 1.00\n16 M8 x40 Bolt+Wing Nut 6 1.00 6.00\n17 4 x50 Bolt 4 - 2.00\n18 5 x50 Bolt 4 - 2.00\nx1.08(GST)\n19 Micro SDCard32 GB100 MB/s Class10/Sandiskwith OS 2 24.00 48.00\n20 MLip&Eye M/Up R70 ml 1 12.50 12.50\n21 SBH.Duty Refill1 S 1 4.20 4.20\n22 Micro SDCard32 GB100 MB/s Class10/Sandiskwith OS 2 24.00 48.00\n23 Lasercutaluminiumplate265 mmx148 mm 1 40.00 40.00\nx1.08(GST)\nChargingand Data Transfer Cable-For Smartphones-1 m-\n24 2 A 3 2.16 6.48\n25 Pearl&Metallic Paint 1 5.90 5.90\n26 Car Non Slip Mat50 x150 1 8.20 8.20\n27 Aluminium Plate 1 8.00 8.00\n[PAGE BREAK]\n28 Soft Clay Black 2 2.16 4.32\n29 Soft Clay Black 2 2.16 4.32\n30 HTHex Bolt GR838 DIN933 M10-1.5 x40 Blue ZN 6 0.40 2.40\n31 HTHex Bolt GR838 DIN933 M10-1.5 x90 Blue ZN 13 0.70 9.10\n32 STLWing Nut M10-1.5 NP 19 0.60 11.40\nx1.08(GST)\n33 Switch 6 - 12.50\n34 VCEVEAH024 in1 USB3.0 Hub Black 1 18.90 18.90\n35 Chargingand Transfer Cablemicro B1 m2.4 AAluminium 2 2.16 4.32\nChargingand Transfer Cablemicro B1 m2.4 AAluminium\n36 Mesh 1 2.16 2.16\n37 7 CFSpray Paint#4 1 5.20 5.20\n38 PGP66014 S4 Way2 MPower Extension Socket 1 16.90 16.90\nOmar's OMWC013 Type C+USBPort20 WWall Charger\n39 Black 4 19.90 79.60\n40 Brints Co.3 DPrinting Services 1 32.00 32.00\n964.81\nAccurateasof16 April2023",
    "sections": [
      {
        "title": "Chin Wei Ming 1006264",
        "content": "Allteammemberscontributedequallytothetask.\n"
      },
      {
        "title": "Designand Artificial Intelligence,Singapore Universityof Technologyand Design",
        "content": "60.003:Product Design Studio\nDr.Kwan Wei Lek,Dr Edwin Koh,Mr Michael Alexander Reeves\n24 April2023\n"
      },
      {
        "title": "Content",
        "content": "Contents\nContents........................................................................................................................................................1\nBackground...................................................................................................................................................3\n"
      },
      {
        "title": "User Interview.........................................................................................................................................3",
        "content": "AEIOUObservation Framework.............................................................................................................3\n"
      },
      {
        "title": "Design Process..............................................................................................................................................9",
        "content": "Dashboard................................................................................................................................................9\nScreens..............................................................................................................................................9\n"
      },
      {
        "title": "Final Design................................................................................................................................................17",
        "content": "Renders..................................................................................................................................................17\n"
      },
      {
        "title": "System Diagram....................................................................................................................................19",
        "content": "Evaluation...................................................................................................................................................20\nBudgeting...............................................................................................................................................20\nDashboard.......................................................................................................................................20\n"
      },
      {
        "title": "Areasof Improvement...........................................................................................................................21",
        "content": "Dashboard.......................................................................................................................................21\n"
      },
      {
        "title": "Steering Wheel................................................................................................................................21",
        "content": "Gettingin/out..................................................................................................................................21\nReflection....................................................................................................................................................22\nAcknowledgement......................................................................................................................................22\nReferences...................................................................................................................................................23\nAppendix A.................................................................................................................................................24\nAppendix B.................................................................................................................................................25\nAppendix C.................................................................................................................................................26\nAppendix D.................................................................................................................................................27\n"
      },
      {
        "title": "Content",
        "content": "Appendix E.................................................................................................................................................29\nAppendix F..................................................................................................................................................30\nAppendix G.................................................................................................................................................31\nAppendix H.................................................................................................................................................32\nAppendix I...................................................................................................................................................37\nAppendix J..................................................................................................................................................40\nAppendix K.................................................................................................................................................45\nAppendix L.................................................................................................................................................48\nAppendix M................................................................................................................................................50\n"
      },
      {
        "title": "Electric Vehicle Additive Manufacturing(EVAM)Interior Design",
        "content": "At the forefront of design and innovation, Singapore University of Technology and Design (SUTD) is\nmanufacturing components for their electric vehicle project, known as Electric Vehicle Additive\nManufacturing (EVAM). As the name suggests, they employfabricatingmethodssuchas Selective Laser\nMelting (SLM), Fused Deposition Modeling (FDM), Multi Jet Fusion (MJF) and Stereolithography\n(SLA).\nAs part of our 60.003 Product Design Studio, we have been tasked with designing the interior of the\nvehicle.\n"
      },
      {
        "title": "Background",
        "content": "Firstly, we would need to understand the current users’ needs for the vehicle. As thevehicleisstillinits\ntesting stage, the users are mainly the engineering staff and students. To better understand future user\nneeds, we conducted user (engineer) interviews, the AEIOU Observation Framework, and a Value\nProposition.\n"
      },
      {
        "title": "User Interview",
        "content": "When we first viewed the vehicle, we prepared some questions for Senior Specialist Mr Liew Zhen Hui,\nto understand whattheyrequiredustodesign.Thiswasfollowedupwithquestionsto Mr Matthew Dylan\n"
      },
      {
        "title": "Wong Jian Xiong for his vision and the concept for thecar.Thesequestionsandanswerscanbefoundin",
        "content": "Appendix A. In summary, the concept of the car is to demonstrate additive manufacturing capabilities in\nthe manufacturing of a race car. It should be designed to look‘dynamic,aggressiveandsporty’,withthe\nnecessaryfunctionsrequiredforittobedriveninarace.\nAEIOUObservation Framework\nNext, weexploredthe AEIOU1 Observation Frameworkofthe EVAM.Amindmapofour AEIOUcanbe\nseenin Appendix B.\n"
      },
      {
        "title": "Value Proposition",
        "content": "Lastly, based on our User Interviews and AEIOU, a valuepropositionwasplannedtocategorisepossible\nneeds into demands and wishes.Thedemandsarewhatwecanfullydesignwithinourtimelineandareof\nhighestpriority.Theyareasfollows:\n"
      },
      {
        "title": "1. Dashboard. In order to enhance user experience, the users must be able to see all necessary",
        "content": "informationaboutthevehicleatalltimes.\n"
      },
      {
        "title": "2. Steering wheel. To control more car settings for improved performance, the driver must be able to",
        "content": "reachbuttons,whilestillfirmlyholdingthewheel.\n"
      },
      {
        "title": "3. Mobility. The users should be able to get into andoutofthecarwithease,regardlessoftheirheight",
        "content": "orsize.\nOn the other hand, wishes are additional features that we could provide. They can be foundin Appendix\nC.Astoryboardwasdrawntobettervisualisethevalueproposition,foundin Appendix D.\n1 Acronymfor Activities,Environment,Objects,Interactionsand Users\n"
      },
      {
        "title": "Problem Statement",
        "content": "Through our design process of empathise, define and ideate, we decided to design a means to enhance\nuserexperienceoftheinteriorofthevehiclewithaddedlightness.\n"
      },
      {
        "title": "Design Specification",
        "content": "To accomplish our problem statement efficiently, we merged certain design wishes with the demands to\ncreatethreecohesivedesigngoals:\n"
      },
      {
        "title": "1. To design a dashboard, for both driver and passenger, that displays allnecessaryinformationatthe",
        "content": "appropriate times, depending on its driving modes. These modes include static, demonstration and\nrace mode. The telemetry must, at least, include Speed, Temperature of battery and motor, Battery\nCharge,andmustbeintuitive.\n"
      },
      {
        "title": "2. To design a steering wheel, with other controls besides steering input, for customizability of car",
        "content": "functions. The design should be ergonomic so that the wheel is comfortable to hold and grip for\nprolonged periods, and also to reflect the SUTD identity onto the steering wheel design.\nAdditionally,controlsshouldbeeasilyreachablebyfingers,evenwhenthedriverissteeringthecar.\n"
      },
      {
        "title": "3. To design a way for the trained driver and any able-bodied passenger to enter and exit thevehicle",
        "content": "withease,regardlessoftheirheightsorsizes.\n"
      },
      {
        "title": "Concept Development",
        "content": "This means that although the final product is the interior of the goal, there is a need to design three\nseparatecomponentsthatcanstillcomplementeachother.\n"
      },
      {
        "title": "Functional Analysis",
        "content": "Hence, in our concept development, there is a need to perform a functional analysis for the three\ncomponents. Two types of analysis are the system and process perspectives, with the former shown in\nFigure1,andthelatterin Appendix E.\n"
      },
      {
        "title": "Content",
        "content": "Figure1\n"
      },
      {
        "title": "Mood Board",
        "content": "For the designofeachcomponenttocomplementeachother,thedesignsshouldbeinspiredbyacommon\nmoodboard,asseenin Figure2.\nFigure2\n"
      },
      {
        "title": "Design Mood Board",
        "content": "Additionally, a mood board was designed by DALL-E2. We wanted to use an AI-generated mood board\nfor inspiration. However, it did not align with our vision so we didnotuseit.Thegeneratedmoodboard\ncanbefoundin Appendix F.\n"
      },
      {
        "title": "Concept Generation",
        "content": "Using functional analysis, a morphological chart can be created to aid in concept generation, as seen in\nTable 1. However, due to the complexity of the vehicle, onlyfunctionsrelatedtothesethreecomponents\nwillbeused.\n2 DALL-E is an Artificial Intelligence System that generates images for text input, created by Open AI. (Johnson,\n2021)\n"
      },
      {
        "title": "Content",
        "content": "Table1\n"
      },
      {
        "title": "Morphological Chart Concept Generation",
        "content": "We selected and combined these functions to form our concepts. Since we have three design goals\n(dashboard, steering, getting in/out), a totalofnineconceptsisneeded.Thefunctionsfortheconceptsare\nhighlightedin Table1,usingthelegendin Table2.\n"
      },
      {
        "title": "Content",
        "content": "Table2\n"
      },
      {
        "title": "Legendfor Function Selectionfor Concept Generationin Morphological Chart",
        "content": "Concept1 Concept2 Concept3\n"
      },
      {
        "title": "Steering Wheel Green ㅤㅤㅤㅤ Cyan ㅤㅤㅤㅤ Blue ㅤㅤㅤㅤ",
        "content": "Gettingin/out Dark Blue ㅤㅤㅤㅤ Purple ㅤㅤㅤㅤ Pink ㅤㅤㅤㅤ\n"
      },
      {
        "title": "Concept Selection",
        "content": "These concepts were then put through a selection criteria, seen in Appendix G, and rated among team\nmembers. The ratings are on a scale from 1 to 5, with 5 being the most ideal. With four criteria being\nlooked at, the score for a component ranges from 1 to 20. This issummarisedin Table3.Withsixgroup\nmembers, the total score for a component will range from 6 to 120. The highest scores would then\nundergothedesignprocessinthenextstep.\nTable3\n"
      },
      {
        "title": "Dashboard Steering Gettingin/out",
        "content": "Team\nMember\n1 ㅤ 2 ㅤ 3 ㅤ 1 ㅤ 2 ㅤ 3 ㅤ 1 ㅤ 2 ㅤ 3 ㅤ\nEthan 18 16 9 5 17 19 17 5 8\nCaitlin 20 13 4 11 15 7 10 8 11\n"
      },
      {
        "title": "Wei Ming 18 15 7 6 15 11 13 20 16",
        "content": "Aditya 17 12 10 15 19 17 15 14 5\n"
      },
      {
        "title": "Yu Jie 19 13 4 5 18 7 10 8 11",
        "content": "Total 109 80 39 56 100 75 76 67 70\nNote:Theselectedconceptshavetheir‘Total’boxeshighlightedintheirrespectivecolourcodes.\n"
      },
      {
        "title": "Design Process",
        "content": "Once the concepts were selected, we began the design process. This process consists of ideation,\niterationsandfurtherdevelopment.\nDashboard\nThe dashboard could be split into two main components: user interface and screens. Each component\nwouldgothroughideation,iteration,anddevelopmentaswell.\nScreens\nInitially, therewereonlyplanstodevelopafrontdashboard.However,takinginspirationfromco-pilotsin\nfighter aircraft, a passenger dashboard was deemed necessary to assist the driver during races. Hence, a\ndashboard was required for driver and passenger, with each displaying different information. This\ninformation can be categorised into two categories: telemetry and camera views, which will be further\nelaboratedonunder‘User Interface’.\nThe front dashboard only had a 360 by 140 millimetre space while the rear dashboard had a 200 by200\nmillimetre space. At therear,therewasalsomorespacebelowthecentralsupportbar,butthatareahadto\nbe clear for the passenger to enter and exit the vehicle. Furthermore, only the upper section of the\ndashboard space could be used to narrow down the driver’s field of view, especially when he needs to\nfocus on the track. Hence, we decided to use two 3.5-inch LCDs and one 7-inch LCD at the front, and\nfive7-inch LCDsattheback.\nAt the front, the dashboard holder was designed more aesthetically than functionally. Taking inspiration\nfrom wind tunnel flow visualisation (Hall, 2021), and our mood board, wavy lines were etched into the\ndashboard, with incrementing depths of two millimetres. These lineswouldcontinuetowardstheinterior\nside panelling of the vehicle.Thefrontscreenholderisalsotiltedfivedegreesupwardstoaccountforthe\nelevated height of the perspective of the driver. Similarly, the screens are tilted five degrees inwards to\nfacethedriver.Anisometricviewofthisdesigncanbefoundin Appendix HFigure H1.\nTo account for ease of manufacturing and theme of additive manufacturing, the screen holder had to be\nbroken up to fit the 3 D Printer bed size. In the end, the dashboard was divided into two, with excess\nmaterial trimmed for added lightness. A screen cover was added to seal the screen in the dashboard, as\nwellastohideanyexposedelectronics.Itsassemblycanbefoundin Appendix HFigure H2.\nAt the back, we decided to implement adjustable screen holders. This is because unlike the user of the\nfront seat, who is usually thesametraineddriver,theuserofthebackseatcanbeanyable-bodiedperson,\nregardless of his/her height. This means that different passengers may have different perspectives,hence\nthe screen tilt must be different. Furthermore, as we chose to utilise the space below the central support\nbeam for more screens as well, the screens should be able to tuck away so that itiseasierfortheuserto\ngetinandoutofthevehicle.\nWe based the design of theseadjustablescreensonmonitorstands.Thisusuallyinvolvesarmswithjoints\nor hinges that can be tightened or loosened. We simulated this design by 3 D printing arms, which were\nconnected using M10 hex boltsandwingnuts.Thismeansthatthejointscouldbetightenedandloosened\nusing the wing nuts. Similarly, the clamps that held the screen holders to thecentralsupportbeamofthe\nchassis could be tightened and loosened to provide a cylindrical joint for further adjustment by the\npassenger.\nWhile sharing with the engineering team about our design plans,theytoldusthattestersoftenholdonto\nthe central support beam to steady themselves to counter the G-forces during acceleration and\ndeceleration. This means that the initial plan for placement of screens wouldblockthesupportbeam.As\n"
      },
      {
        "title": "Content",
        "content": "we did not want to eliminate this, we decided to implement grips into the clamps. This would not only\nallow passengers to support themselves during high lateral forces, but also for ease in adjusting the\nclamp’s tilt. The final design can be found in Appendix H Figure H3, with its exploded assembly in\nFigure H4.\nThe parts list for these dashboards can be found in Appendix H Table H1 while the fasteners usedarein\nTable H2.\n"
      },
      {
        "title": "User Interface",
        "content": "Due to the different modes, two different interfaces must be designed: Race and Demonstration. The\ncodes for this (including all the connections of the user interfaces to the various components of the\nsteeringwheel)canbefoundatthisrepository:https://github.com/Caitlin Chiang/EV-Screens\nRace. Inspiration was taken from F1 cars because the car concepts are similar: open wheel racing.\nSpecifically, we took inspiration from the Aston Martin Aramco Cognizant F1 Team, which displayed\ntheir wheel layout and the telemetry for each. The race driver would require telemetry such as speed,\ntyre/brake/motor/battery temperatures, battery charge,racedeltas,laptimings,brakebiasandmigration,\nrace positions, laps done and drinking water remaining. Allthisdataiscolourcoded,withorangebeing\ntoo hot or almost depleted, blue being too cold or half used, and white being optimum temperature or\nfull. The race-standard colours purple, green and white are used forracetimingsaswell.Thisissothat\nthedrivercaninstantaneouslytelliftheirbrakes/tyres/motor/batteryistoowarmorcold.\nAs for the camera views, they were placed at the usual positions that therearviewmirrorswouldbein\nanordinarycar.Thisissothatitismoreintuitive.\nWe tested the designs by printing out the interface onto paper the size of the actual screens. We then\npasted them on the vehicle and tested how readable the interface is, as seen in Appendix I Figure I1.\nInitially,thefontsweretoosmallsoweincreasedthesizes.Additionally,wedecidedtoremovethefront\nfacing camera as thespeedometerwastoolowandwasblockedbythesteeringwheel,sincethespeedis\nofhigherimportancethanthefrontfacingcamera.\n"
      },
      {
        "title": "Thefinalinterfacecanbefoundin Appendix IFigure I2.",
        "content": "Demonstration. The first iteration was a simple layout only including the absolute essential telemetry\ngiven to us during the interview, but we realised that a lot of spaceisbeingputtowaste,andthatusers\nactually don’t know that seeing certain data is convenient untilwealreadypresentittothem.Withthat,\nwe iterated another design, this time adding telemetrythatwethinkisnecessarybasedonresearch.The\ntelemetryhereisthesameastheonesusedinracemode,exceptdesignedandpresenteddifferently.\nOn the third iteration, we focused more on the experience of the interfaces and making sure that they\nwere pleasing to look at.Westartedtousecolourstodepictcertainvalues(e.g.usingredwhenacertain\npart is overheating), and placed telemetry in a way that made more sense to the user, allowing it to be\nmore intuitive as well. We also added camera views here, the same as race mode, tothepositionsasto\nwhererearviewmirrorswouldnormallybe.\nThe final iteration includedcreatingsimilarbutdifferentinterfacessuchthattheusercanswitchtothem\nat appropriate times, and therefore be able toseecertaintelemetryatspecificsituationsatalargerview.\nWekeptrefiningthemsuchthatthereisaflowtowhichgetsshownatvariousbuttonpresses.\n"
      },
      {
        "title": "Steering Wheel",
        "content": "The design process for our steering wheel began with a collaborative effort among team members, with\neachcontributingtheirbestideastocreateaninitialconcept.Werefinedthedesignthroughtrialanderror,\nexperimenting with specifications like size and button placement to achievethedesiredfunctionalityand\naestheticswhichcanbeseenin Appendix JFigure J1 and Figure J2.\nHowever, we realised that we had overlooked important factors such as budget and manufacturing\nlimitations, which led us to revisit the design and incorporate these factors. The iterations of steering\nwheeldesignscanbeseenin Appendix JFigure J3.\nTo achieve a practical and cost-effective design, we began with a robust and lightweight aluminium\nchassis that provided structural support and stability. We carefully designed the chassis with cutouts for\nbuttons and screws, ensuring a precise and secure fit for all components asseenin Appendix JFigure J4\nand Figure J5. Another point to note is that, when we designed the steering wheel, we made sure to\nposition the buttons in a way that would allow for easy access by the thumb for an average adult male.\nThe button placements, as well as their functions can be seen in Appendix J Figure J6 and Table J1\nrespectively.\nThe handles, buttons, and button shell were 3 D printed using advanced additive manufacturing\ntechniques, allowing for a high degree of customisationandprecision.Thisreducedproductioncostsand\nlead times while also ensuring optimal functionality and aesthetics. The paddle shifters were designed\nwith simplicity andeffectivenessinmind,featuringamicroleverswitchtowhichthepaddleattaches.We\nutilised a magnet instead of a traditional spring to provide haptic feedback, enhancing the tactile\nexperiencefortheuser.Thelistoffastenersusedtoassemblethesteeringwheelcanbefoundin Appendix\nJTable J3.\nTo cover the wires coming out the back of the steering wheel, we designed and 3 D printed a custom\ncasing that also housed the paddle shifters. This provided a clean and professional look while ensuring\nthatthewireswereprotectedfromdamageandinterference.\nOverall, our steering wheel design incorporated a range of advanced manufacturing techniques and\nmaterials, from the precision-cut aluminium chassis to the 3 D printed components and custom casings.\nEvery element was carefully considered and optimised for maximum functionality and user experience,\nresultinginahigh-performanceandaestheticallypleasingproductasseenin Appendix JFigure J7.\n"
      },
      {
        "title": "Getting In/Out",
        "content": "This particular aspect of the EVAM’s experience was rather tricky to enhance. Mainly because of the\nspace constraints of the car’s interior,aswellashowthecarwasnotinitiallydesignedforaneasyingress\nand egress in mind. Both this obstacle has created a particularly awkwardwayfortheuserofthevehicle\ntogetinandoutofthecarasillustratedin Figure3 below.\nHerearethestepsfortheingressandegressprocesses:\nIngress\n(1)Putonefootonthechassis\n(2)Putbothfeetontotheseat\n(3)Usingthesidechassisassupport,theuserwillslowlylowerhim/herself\n(4)Userwillthensuccessfullygetintothecar\n"
      },
      {
        "title": "Content",
        "content": "Egress\n(1)Holdonthethesidechassis\n(2)Pushhim/herselfup\n(3)Standupontheseat\n(4)Steponefootontothechassis\n(5)Steptheotherfootontothehigherchassisbarandeventuallygetoutofthecar\nFirst, we will illustrate with pictures how an average person enters the back seatofthevehicle.Wehave\ndecided to put our main focusonthebackseatbecausewehavebeeninformedthattheuserprofileofthe\nfrontseatwillbeafitandathleticracer.\nFigure3\nProcessthatusergetsintovehicle(fromlefttoright,toptobottom)\n"
      },
      {
        "title": "Content",
        "content": "As illustrated in the pictures above, it is extremely uncomfortable and awkward even for a ratherfitand\nathletic person. So imagineifduringashowcaseofthecar,anoldpersonwouldliketositinthecar.With\nthecurrentdesign,itwillbeverydifficultforthattohappen.\nBefore we start going intoideatingwhatadesignsolutionforthiswouldbe.Wefirstanalysewhatarethe\ntouchpoints, by hand and foot, of different users when they get in and out of the car. And eventually\nbringing us to understand in which area of the car is commonlyusedassupportwhentheyenterandexit\nthecar.Anillustrationofthecommontouchpointsisshownin Appendix KFigure K1.\nFurthermore, we had to understand what are the space constraints we are limitedtowithinthecar.Todo\nthis, using Fusion 360, we modelled out the space inside the car, allowing us to know the exact\nmeasurements of space that we could work with. An illustration of the space constraint is shown in\nAppendix KFigure K2.\n"
      },
      {
        "title": "Content",
        "content": "After which we exploredmultipleideasasseenin Appendix KFigure K3,whicheventuallywerealisedis\nnot very practical. One of the ideas was to lift the seatuptoacertainlevelthatwillbeeasierforusersto\nget into a sitting position first, then using some sort of mechanism to slowly lower the seat down and\nsame for bringing the seat up. But we were soonnotifiedby Mr Liewthatwearenotabletodisassemble\nthe seat, therefore this idea was removed. Another idea wastohavepanelswhichareabletoflapopenas\nstairs for the user to step intothecarinmultiplelevels,butitwassoonrealisedthattheseflapswouldnot\nbeabletosupporttheweightofanadultperson.\nSoon after, we realised that the most amount of space that is availableinsidethecar,isactuallythesides\nof the seats. Which led us to think of designingamodularsidecompartment,whichservesasanarmrest,\nstorage space, as well as steps toeasetheingressandegressofusers.Italsoactsasaspaceforfunctional\nbuttonsthatwerenotplacedonthesteeringwheel.\nHaving used Fusion 360 to model out what the side compartment looks like, we ensured that the sizeof\nthe armrest will be within the size constraint of the interior of the EVAM. Figure 4 shows the design of\nthemodularsidecompartment,while Figure5 and Figure6 showitsfunctionalities.\nFigure4\nDesignsforeasingingressandegress\n"
      },
      {
        "title": "Content",
        "content": "Figure5\nDesignforeasingingress\nFigure6\nDesignforeasingegress\n"
      },
      {
        "title": "Content",
        "content": "Another issue that we found was that there is no place forthedrivertoresttheirheelwhenoperatingthe\nvehicle. Using Midjourney, an AI image generator, we asked it to show us designs of a F1 heel rest for\ninspiration.Generatedimagescanbefoundat Appendix KFigure K4.\nWe then proceeded to design a heel rest, which also includes a leg rest that will further aid the driver to\nget inandoutofthecar.Furthermore,whenourteamtriedgettingintothevehicletotestoutthespacewe\nhad, there was one engineer whowasworriedthatourteam,beinginexperiencedwiththecar,wouldstep\non some of the wiringthatwasexposed.Havingaddedalegrest,therewasnowacoverforthewiresand\nnolongertheworryofsteppingonsensitivewiring.\nFigure7 belowshowsapictureoftheheelandlegrest.\nFigure7\nDesignforheelandlegrestforfrontseat\nThe design for the heel rest is heavily inspired by the Midjourney generated images. As for the leg rest,\nthe driver is able to ‘feel’ his/her legs into the car and eventually reach thepedals.Wavypatternscanbe\nseen throughout the design of the car, these patterns are made rubbery with very high friction which\nserves as steps users take. A better illustration of the wavy patterns on the rest of the car can be seen at\nAppendix KFigure K5.\n"
      },
      {
        "title": "Final Design",
        "content": "Renders\nFigure8\nOverheadviewofthefinalinteriordesignofthe EVAM\nFigure9\nOverheadviewofthefinalinteriordesignofthe EVAM\n"
      },
      {
        "title": "Content",
        "content": "Figure10\nDriver’sperspectiveofthefinalinteriordesign\nFigure11\nPassenger’sperspectiveofthefinalinteriordesign\n"
      },
      {
        "title": "System Diagram",
        "content": "To illustrate the new functionsofthedashboardsandsteeringawheel,a System Diagramhasbeendrawn\nasseenin Figure12.\nFigure12\n"
      },
      {
        "title": "Systems Diagramof EVAMInterior Design",
        "content": "Thiscanalsoberepresentedinastoryboardforthesolutions,asseenin Appendix L.\n"
      },
      {
        "title": "Evaluation",
        "content": "Budgeting\nFor this project, we were given a budget of $1000 fortheproductionofourproduct.Wewillbebreaking\ndown how much was spent in the making of each component in this section, as well as justification of\nwhy these purchases are necessary for our project. The detailedlistofhowmuchwasspentcanbefound\nin Appendix M.\nDashboard\nA total of$737 ofthebudgetwasspentoncreatingthedashboard.Itwasparticularlyexpensivebecause\nof the display that is compatible with the raspberry pi module, that is necessary for us to showexactly\nwhatitwouldlooklikeexactlyintheactualcar.Secondreasonbeingthatwewereconstantlyfacedwith\nthe issue of not supplying enough power to the Raspberry Pi and its display. And soon later found out\nthat for each screen and Raspberry Pi,itneededaseparatepowersourcewhichwethenneededtobuya\ncharging plug for each display. For each screen, we also needed a SD card slotted into it for it to\nfunction. Therefore a part of the budget also went to the purchaseof SDcards.Otherthanthat,wealso\nintended 2 of our screens to display the live rear camera view in the car. Therefore we purchased 2\nraspberry pi cameras to connect to the Raspberry Pi which eventually is able to display camera views\nonto the Raspberry Pi display that we purchased. Other than that, we purchased quite afewcablesthat\nare necessary to connectbetweenthe Raspberry Pimodule,thedisplay,andaswellasthepowersource.\nFor each display, 1 x HDMI to HDMI cable, 2 x USB to micro USB cable is required. Thereforewith5\ndisplays,weneeded5 x HDMIto HDMIcable,and10 x USBtomicro USBcable.\n"
      },
      {
        "title": "Steering Wheel",
        "content": "A total of $138 of the budget was spent on the production of the steering wheel. When creating the\nbase metal plate of the wheel, we first attempted to purchase our own metal plate and cut it out in the\n"
      },
      {
        "title": "Fabrication Lab (Fab Lab), but was soon met with the obstacle of it being too heavy and difficult in",
        "content": "cutting precisely to scale. We then seeked help from an external source which provided us with alaser\ncutting service as well as a good lightweight material that is most suitable for our use case. Some\namount ofthebudgetwasalsospentonswitchesthatwerenecessaryforustomakefunctionallybuttons\nin the steering wheel. We also spent some amount to purchase the screws and nuts that were not\nprovided by Fablab mainly because it didn't have the length and size that we needed. Lastly, we\npurchased spray paints for the final finish of the steering wheel. Note that someoftheitemspurchased\nlikeclayandnon-slipmatswerefortherapidprototypingphaseofthesteeringwheel,andwerenotused\nin the production final steering wheel. Finally, we also seek help from an external 3 D printing service\nfortheproductionofourbackcasingforbetter3 Dprintingfinishandquality.\n"
      },
      {
        "title": "Product Weight",
        "content": "As seen in Appendix H Table H1 and Appendix J Table J2, the total weight of fabricated products is\nestimated to be approximately 3.628 kilograms. This is below our design specification wish of five\nkilograms. This means that additional functionality would have a very minor dentontheperformanceof\nthevehicle.\n"
      },
      {
        "title": "Client Feedback",
        "content": "The client was particularly impressed with the seamless integration of the steeringwheelanddashboard.\nThey felt that the design concept was innovative and unique, and appreciated the attention to detail that\nwent into creatingacohesiveandaestheticallypleasingproduct.Theclientnotedthatthedesigncreateda\nmore streamlined and ergonomic experienceforthedriver,whichtheyfeltwasasignificantimprovement\noverthetraditionaldesign.\nWhile the client was impressed with the overall design concept and integrationofthesteeringwheeland\ndashboard, they did have some minor critiques. The use of trial-and-error fordeterminingmeasurements\n"
      },
      {
        "title": "Content",
        "content": "was seen as a potential setback, as the client recommended the use of Anthropometric data for more\nprecise results. Additionally, the client suggested modifying the line width and increasing the infill\ndensity for the 3 D prints in order to enhance the durability of the product, especially if it will be used\nwhiledriving.\nDespite these minor setbacks, the client remained highly satisfied with the design and expressed\nexcitementforthepotentialoftheproduct.\n"
      },
      {
        "title": "Areasof Improvement",
        "content": "Dashboard\nDue to the lack of time, more iterations of the screen holders could not be fabricated. The front\ndashboard holder should be designed to hide the Raspberry Pi. Similarly, the passenger dashboard\nshould have a way to hide the cables that run out of it. Screen casings should be made, together with\nsealant, to weatherproof the electronics. A more flexible material should be usedtomakethehingesso\nthatthehingescantightenbetter.\nIn terms of improvements in regards to the user interfaces, the various functionalities of the steering\nwheel can have more impact on the displays aside from changing the interface completely from one\nmenu to another.Instead,wecanhavethevaluesonthedisplayitselfchangedependingonthetiltofthe\ncar. Furthermore, we aim to include warning interfaces that have the ability to completely override the\ncurrent interface being displayed to the driver / passenger (with the ability to close it quickly) when a\ncertain vehicle component is overheating or underperforming. This way, the driver and passenger can\noperate more safely and with a better peace of mind, knowing that anything that goes wrong will be\nalertedbeforeincriticalcondition.\n"
      },
      {
        "title": "Steering Wheel",
        "content": "During the design and manufacturing process of our steering wheel, we encountered some challenges\nthat revealed areas for improvement. Specifically, we found that our wiring management was lacking\nand our soldering skills needed improvement, resulting in a delicate final product.Additionally,the3 D\nprinting process had some flaws that required workarounds. Furthermore, we were unable to complete\nthe setup of the dashboard due to lack of planning and insufficientlylongwires,whichcausedconstant\nreboots. These challenges highlight the need for more comprehensive planning and better execution of\nmanufacturingprocessesinthefuture.\nGettingin/out\nSeeing how the rest of the components blend so seamlessly with the interior of the car. This particular\ncomponent that is aiding the ingress and egress process for the user, specially seemed like a complete\nseparatecomponentanddoesnotentirelyblendintotheinteriorofthevehicle.Therefore,morethoughts\non how these side compartments could blend more seamlessly with thedesignoftherestoftheinterior\nofthecomponentswoulddefinitelyimprovetheoveralldesignofthevehicle’sinterior.\nAlso because this is one of the more obvious components when someone looks into the car, mainly\nbecause of its size compared to other components. We could also put more thoughts on how its\nfunctionality and aesthetic of the design of the sidecompartmentcouldbetterreflectthe SUTDidentity\nof being future-oriented, design-centric, and interdisciplinary. So that when visitors come and visit the\nEVAM, they are constantly reminded that this is specially produced by SUTD students and as well as\nthe SUTDidentity.\n"
      },
      {
        "title": "Content",
        "content": "Reflection\nAs mentioned in the client feedback, our group didnotuseprecisemeasurements.Thisledtoissuessuch\nasthebackcasingofoursteeringwheelnotfittingthebaseplateexactly.Assuch,wehadtofiledownthe\nback casing inordertofitittothebaseplate.Havingfacedthisavoidableissue,ourgroupnowknowsthe\nimportanceoftakingprecisemeasurements,especiallyinthepreliminarystagesoftheproject.\nOur group also made use of Dall-E and Midjourney in our project to give us inspiration. While theydid\nhelp us make the design process easier, we understand that it is only a tool to aid us, and should not be\nused as a solution in itself. There is still the need for humans (i.e.ourgroup)togetinvolvedtotestouta\ndesign and see if it works for our project. This can be seen when we decided that the mood board\nprovided by Dall-E did notsuitourvisionandthatwewouldnotwanttouseit.Anotherexampleiswhen\nwe used Midjourney to give usdesigninspirationsforthefootrest.Westillhadtogodowntothevehicle\nand see ifthefootrestwasindeedabletobeimplemented,withtheconstraintmainlybeingtheamountof\nspacewehad.\nEffective and precisecommunicationofideasisverycrucialattheideatingpartofthisproject.Especially\nwhen we needed to explore and compare between different design solutions. Therefore, theskillofhand\nsketching in a 2 Ddiagramiscrucial,inthesensewhereweareabletoexplainourideastoeachotherina\nquick and precise manner. After which when this particular idea seems like a good one in the 2 D\nperspective, we will then have to model it out in any 3 D software available and 3 D print it for us tosee\nthe component physically.Wewillthenplaceitintheactualpositioninthevehicleofwhereitisintended\nto be. These steps will then allow us to analyse possible issues that may arise with thisparticulardesign\nsolution,whichwecantheniterateandimproveonuntilwereachourfinaldesignsolution.\nAcknowledgement\nWe would like to express our deepest gratitude to Mr Liew and the rest of theteamfortheirguidancein\nour project, and to the EV Club’s Wei Ming for taking the time to come down tothe Fab Labtoassistus\nwithmeasurementsandtesting,despiteherbusyschedule.\nWe would also like to acknowledge with gratitude, the support from our professors and supervisors. We\nwould like to thank Mr Michael Alexander Reeves and Dr Edwin Koh, for giving us different\nperspectives on the project, aswellas Dr Kwan Wei Lekforhelpinguswiththeconnectionofelectronics\nandprovidinguswithmultiplemicrocontrollers.\nSpecial thanks to Ms Zerline Tan for ensuring we have a conducive place to work in. Last but not least,\nthank you to our Teaching Assistants (TAs), Ke Wei and Eugene for sparing time to give us insights on\nexistingcardesignswecanuseasinspirationforourproject.\nSpecial mentions to the exterior team consisting of Aaron, Billy, Joshua, Jun Xiang, Valencia and Wei\nXuan. It was thanks to the collaboration between both teams that we managed to showcase the EV as a\nwhole,ratherthanseparateprojects.\nIt is thanks to all of these people that our group managed tocompletethisprojectwithsuchsuccess,and\nimpressourclientswithourfinalphysicalprototypesandproduct.\n"
      },
      {
        "title": "Content",
        "content": "References\nHall,N.(2021,May13).Smokeand Tuft Flow Visualization.Wind Tunnel Index.Retrieved April24,\n2023,fromhttps://www.grc.nasa.gov/www/k-12/airplane/tunvsmoke.html\nJohnson,K.(2021,January5).Open AIdebuts DALL-Eforgeneratingimagesfromtext.Venture Beat.\n"
      },
      {
        "title": "Retrieved April21,2023,from",
        "content": "https://venturebeat.com/business/openai-debuts-dall-e-for-generating-images-from-text/\n"
      },
      {
        "title": "Content",
        "content": "Appendix A\nInterviewtranscriptwith EVAMEngineers\n"
      },
      {
        "title": "1. Whatisyourbiggestfocusindesigningtheoperationsofthevehicle?",
        "content": "Thegoalisracing,hencespeedismostimportant.\n"
      },
      {
        "title": "2. Whataresomeissuesyoufaceintheoperationofthevehicle?",
        "content": "Thereisnocarinformationcommunicatedtothedriver.\n"
      },
      {
        "title": "3. Aretherespecifictelemetriesyouwouldliketosee?",
        "content": "Speed,Temperatureofbatteryandmotor,Battery Charge.\n"
      },
      {
        "title": "4. Iscomfortimportant?",
        "content": "Thedrivermustadapttothedesign,butsupporttotheneck,shouldersandlumbarisabonus.\nAs afollowup,wetextmessagedastudentengineerwhohelpeddesignthecar,Mr Matthew Dylan Wong\n"
      },
      {
        "title": "Jian Xiong (2023), to offer his perspective on the vehicle. We asked what his original vision for the car",
        "content": "was:\nThe concept of the caratitscorewastoexploitadditivemanufacturingasmuchaspossibletodothings\nthatarenotconventionallypossibleandtomakethefabricationprocessmoreefficient.\n(1) Explore how additivemanufacturingcanbeusedtofabricatethebodyshellparts.Ahugeproportion\nofthemoneyspentonthecarisgoingintothe3 dprintedmetaljointsontheframe.\n(2)Highlightthe3 dprintedjointsofthecar.Thisiswhy Itriedtoframetheminsizethesidewindow.\n(3) The car was originally meant to be a \"formula-style race car\". It has a low ride height, wide track\nwidthandlowoverallheightthatgivesitanaggressivestance.\n(4) The design ofthecarshouldbetosomeextent,dynamic,aggressiveandsporty.Itshouldlooklikeit\ncan go fast even when it's not moving. At thesametime,thereshouldbeaerodynamicelementstosend\nthemessagethatthiscarismeantforperformance.\n"
      },
      {
        "title": "Content",
        "content": "Appendix B\nAEIOUFramework Observation Mindmap\nNote:Thenodes’relationshipsarerepresentedbythedashedlines.\n"
      },
      {
        "title": "Content",
        "content": "Appendix C\n"
      },
      {
        "title": "1. Weight",
        "content": "The additional components are lightweight (below 5 kg) to enhance performance despite additional\nfunctions.\n"
      },
      {
        "title": "2. Hydration",
        "content": "The users should be able to have access to water easily so they can sustain themselves during\noperationofthevehicle.\n"
      },
      {
        "title": "3. Safe",
        "content": "Usersmustbeabletoberemovedfromthevehicleswiftlyandsafelyifanaccidentweretohappen.\n"
      },
      {
        "title": "4. Communication",
        "content": "Usersneedtobeabletocommunicatewitheachotherdespitethenoise.\n"
      },
      {
        "title": "5. Comfort",
        "content": "Thecockpitshouldstaycool,andtheseatingpositionsshouldbecomfortable.\n"
      },
      {
        "title": "6. Fabrication",
        "content": "The components should be easy and inexpensive to manufacture, preferably done using additive\nmanufacturingtosupporttheoriginalconceptofthevehicle.\n"
      },
      {
        "title": "7. Affordable",
        "content": "Controlsareintuitive.\n"
      },
      {
        "title": "8. Enjoyable",
        "content": "Userswillhaveapositiveexperiencefromridinginthevehicle.\n"
      },
      {
        "title": "Content",
        "content": "Appendix D\nStoryboard(Problems)\n"
      },
      {
        "title": "Content",
        "content": "Appendix E\n"
      },
      {
        "title": "1. Initialisethevehicle",
        "content": "a. Receiveuser\nb. Receiveinputtostartthevehicle\nc. Initiatestartupprocess\nd. Receiveinputsonusersettings,ifany\n"
      },
      {
        "title": "2. Operatethevehicle",
        "content": "a. Receiveinputsfordrive/reversemode\nb. Receiveinputsonusersettingchanges,ifany\nc. Displayinformationoftheuser’ssettings\nd. Receiveinputsonvehicle’sstatus,ifany\ne. Displayinformationonthevehicle’sstatus\nf. Physicallysupportuser\ng. Hydrateuser\nh. Cooluser\ni. Receivevoiceinputfromdriver/passenger\nj. Relayvoiceoutputtodriver/passenger\n"
      },
      {
        "title": "3. Deactivatethevehicle",
        "content": "a. Receiveinputforemergencystop,ifany\nb. Receiveinputtoshutdownthevehicle\nc. Initiateshutdownprocess\nd. Savecurrentinputsofusersettings\n"
      },
      {
        "title": "Content",
        "content": "Appendix F\nAIGenerated Mood Board\n"
      },
      {
        "title": "Content",
        "content": "Appendix G\nCriteriaforscoringandrankingdesignconcepts\n"
      },
      {
        "title": "Criteria Score",
        "content": "Functionality 1 2 3 4 5\n"
      },
      {
        "title": "Easeof Fabrication 1 2 3 4 5",
        "content": "Easeofuse 1 2 3 4 5\nCost 1 2 3 4 5\nTotal\nNotesfromselection\nDashboard\n"
      },
      {
        "title": "1. Using the Raspberry Pi computer was most ideal because our team could borrow them from our",
        "content": "institution,SUTD,allowingustostaywithinourgivenbudget.\n"
      },
      {
        "title": "2. Fordisplayinginformation,the LCDscreenswerechosenover Tablet Devicesastheywouldbetoo",
        "content": "expensive, and over e-ink as it cannotrespondfastenoughforhighstresssituationssuchasracing.\nLastly, a rocker switch was chosen over its other concepts because it gives a tactile sensation to\nstartingandstoppingthecar,withtwoalternativeoptions:startandstop.\nSteering\n"
      },
      {
        "title": "1. Buttons were chosen over all other options for the steering as in a racing scenario, timing to",
        "content": "obtain required information is extremely important, and buttons are the most effective in\nminimising the movement and time needed to obtain the desired dashboard information output,\nmakingitthebestchoice.\nGettingin/out\n"
      },
      {
        "title": "1. We chose to use groovedpadsoverrubberpadsbecausegroovedpadsprovidemoregripstrength",
        "content": "on the shoe to prevent any slipping. We can also incorporate awavydesignintoitforaesthetics,\nwhichiswhatwewantasshowninourmoodboard.\n"
      },
      {
        "title": "2. Steps have been chosen over leg rests as we realised that the height difference when getting in",
        "content": "and out of the car is very deep. By adding steps, it serves asacomfortableintermediarypointof\nmanoeuvringtomaketheprocesssmoother.\n"
      },
      {
        "title": "3. We decided to use handles for the back seat as we observed that it is easier for drivers to push",
        "content": "themselves upwards as compared to pulling themselves up by holding onto the chassis when\ngettingoutofthecar.\n"
      },
      {
        "title": "4. We have also implemented grooved heel rests for the front seat as during our investigation, we",
        "content": "realised that there is a gap between the heels of the driver when their feet rests on the pedals,\nmaking it tiring to hold the driving position for long periods of time. Additionally, whengetting\nout of the car,driversalwayspushthemselvesupusingtheirlegs,andbyaddingheelrests,italso\nservesasagripmakingiteasiertoexitthecar.\n"
      },
      {
        "title": "Content",
        "content": "Appendix H\n"
      },
      {
        "title": "Dashboards Screens Schematics",
        "content": "Figure H1\n"
      },
      {
        "title": "Initialdesignof Front Dashboard",
        "content": "Figure H2\n"
      },
      {
        "title": "Content",
        "content": "Figure H3\n"
      },
      {
        "title": "Finalfullyassembled Passenger Dashboard",
        "content": "Note.Redarrowsindicatepossiblemovementofjoints.Notalljointscanbeseen.\n"
      },
      {
        "title": "Content",
        "content": "Figure H4\n"
      },
      {
        "title": "Content",
        "content": "Table H1\n"
      },
      {
        "title": "Label Part Weight(kg)",
        "content": "A Main Dashboard Bodies 0.347\nB Raspberry Pi3 Model B3\nC Waveshare3.5 inch RPi LCD(B)4\nD Waveshare7 inch Resistive Touch Screen LCD,1024×600,HDMI,\nIPS5\nE Screen Cover\nF Top Screen Rocker Arms 2.700\nG Top Clamp Arm\nH Top Middle Clamp\nI Bottom Middle Clamp\nJ Top Left Clamp\nK Bottom Left Clamp\nL Right Top Clamp\nM Right Bottom Clamp\nN Bottom Arm\nO Bottom Hinge\nP Screen Holder\nQ Waveshare7 inch Resistive Touch Screen LCD,1024×600,HDMI,IPS\n3 CADModeladaptedfrom\nhttps://grabcad.com/library/raspberry-pi-3-model-b-reference-design-solidworks-cad-raspberry-pi-raspberrypi-rpi-\n4 CADModeladaptedfrom:https://grabcad.com/library/raspberry-lcd-3-5 inch-1\n5 CADModeladaptedfrom:https://www.waveshare.com/wiki/File:7 inch_HDMI_LCD_3 D_Drawing.zip\n"
      },
      {
        "title": "Content",
        "content": "Table H2\n"
      },
      {
        "title": "Fastenerlistusedin Dashboards",
        "content": "S/N Fastener Quantity Joint\n1 M8 Wing Bolt75 mm 4 A(Screw Clamp)\n2 M8 Hex Nut 8 A(Screw Clamp)\n3 M2 Screw20 mm+Nut 4 Band A\n4 M3 Screw20 mm+Nut 4 A,Dand E\n5 M10 Hex Bolt+Wing Nut+Spring Lock Washer 13 H and I, Jand K,Land M,\n(90 mm), 6 F/O and P, G/N and F/O,\n(40 mm) H/I/K/Mand G/N\n6 M6 Hex Bolt 40 mm + Hex Nut + Spring Lock 8 H/J/Land I/K/M\nWasher\n7 M3 Screw20 mm+Nut 20 Pand Q\n"
      },
      {
        "title": "Content",
        "content": "Appendix I\n"
      },
      {
        "title": "User Interfacefor Screens",
        "content": "Figure I1\n"
      },
      {
        "title": "Content",
        "content": "Figure I2\n"
      },
      {
        "title": "Final User Interfacefor Race Mode",
        "content": "Figure I3\n"
      },
      {
        "title": "Content",
        "content": "Appendix J\nFigure J1\nInitialsketchesofsteeringwheeldesigns.\nFigure J2\nIterationsoffinalsteeringwheeldesigns\n"
      },
      {
        "title": "Content",
        "content": "Figure J3 Figure J4\n"
      },
      {
        "title": "Steeringwheelphysicalcutouts Isometric Viewof Fully Assembled Steering Wheel",
        "content": "Figure J5\n"
      },
      {
        "title": "Content",
        "content": "Figure J6\n"
      },
      {
        "title": "Button Layoutfor Steeringwheel",
        "content": "Figure J7\nFinaliterationofthesteeringwheel\n"
      },
      {
        "title": "Content",
        "content": "Table J1\n"
      },
      {
        "title": "Button Function",
        "content": "A Switchtheuserinterfaceonthescreentoshowtheleftcameraview.\nB Switchtheuserinterfacetothemainmenuofracemode(driver).\nC Turnontheleftlightindicatorsofthevehicle(theoretically,notimplemented).\nD Turnontheheadlightsofthevehicle.\nE Turnontheheadlightsofthevehicle.\nF Turnontherightlightindicatorsofthevehicle.\nG Switchtheuserinterfacetothemainmenuofracemode(passenger).\nH Switchtheuserinterfaceonthescreentoshowtherightcameraview.\nTable J2\nPartslistusedinsteeringwheel\n"
      },
      {
        "title": "Label Part Weight(kg)",
        "content": "A Steeringwheel Chassis 0.581\nB Rightbuttoncasing\nC Leftbuttoncasing\nD Backcasingforthesteeringwheel\nE Rightpaddleshifter\nF Leftpaddleshifter\nG Quickreleasemechanism\n"
      },
      {
        "title": "Content",
        "content": "Table J3\n"
      },
      {
        "title": "Fastenerlistusedinthe Steering Wheel",
        "content": "S/N Fastener Quantity Joint\n1 M3 Screw20 mm+Nut 2 Connectingthe Paddle\n"
      },
      {
        "title": "Shifterstothe Micro Lever",
        "content": "Switch\n2 M3 Screw25 mm+Nut 4 Attachingthe Paddle\nshifterstothebackcasing\n3 M5 Screw20 mm+Nut 6 Bindthe Quick Releaseto\nthe Steering Wheel\n4 M5 Screws25 mm+Nut 6 Boltingthe3 Dprinted\n"
      },
      {
        "title": "Handlestothe Steering",
        "content": "Wheel\n5 Neodymium Magnets(2 mm) 4 Providingthenecessary\nforceforpaddleshifter\nfeedback\n6 M3 Screws25 mm+Nut 6 Tighteningthe Button\n"
      },
      {
        "title": "Casingtothe Steering",
        "content": "Wheel\n"
      },
      {
        "title": "Content",
        "content": "Appendix K\nFigure K1\nTouchpointanalysationforingressandegressofthevehicle\nFigure K2\nSpaceconstraintswithinthe EVAM\n"
      },
      {
        "title": "Content",
        "content": "Figure K3\nDesignexplorationforingressandegress\nFigure K4\nMidjourneygeneratedheelrestimages\n"
      },
      {
        "title": "Content",
        "content": "Figure K5\nIllustrationofstepsthroughoutthecar.\n"
      },
      {
        "title": "Content",
        "content": "Appendix L\nStoryboard(Solutions)\n"
      },
      {
        "title": "Content",
        "content": "Appendix M\n"
      },
      {
        "title": "Budget Spending",
        "content": "Priceper\nNo. Item Qty piece Total Price\n"
      },
      {
        "title": "Raspberry Pi7\"HDMILCD(C)IPS1024 x600 Capacitive",
        "content": "1 Touch 3 85.70 257.10\n"
      },
      {
        "title": "Raspberry Pi3.5\"HDMILCD(B)IPS480 x320 Resistive",
        "content": "2 Touch 2 57.50 115.00\n3 Magnet 3 3.50 10.50\n4 Micro SDCard32 GB100 MB/s Class10/Sandiskwith OS 3 24.00 72.00\n5 Raspberry Pi Camera5 MP 3 14.00 42.00\n6 Momentary Push Switch12 mm/Red 2 1.50 3.00\n7 Momentary Push Switch12 mm/Green 2 1.50 3.00\n8 Momentary Push Switch12 mm/Blue 2 1.50 3.00\n9 SPDTLimit Switch13 mm 6 1.00 6.00\n10 Micro HDMIto Standard HDMI(A/M)Cable/1 m/Black 3 15.00 45.00\n11 Brass Spacer Male-Female Screw-Nut M310 mm4 pcs 1 1.30 1.30\n12 Brass Spacer Male-Female Screw-Nut M425 mm4 pcs 1 2.60 2.60\n13 Screw-Nut M3 x205 pcs 2 1.00 2.00\n14 Screw-Nut M3 x255 pcs 1 1.00 1.00\n15 M10 Nut 4 - 1.00\n16 M8 x40 Bolt+Wing Nut 6 1.00 6.00\n17 4 x50 Bolt 4 - 2.00\n18 5 x50 Bolt 4 - 2.00\nx1.08(GST)\n19 Micro SDCard32 GB100 MB/s Class10/Sandiskwith OS 2 24.00 48.00\n20 MLip&Eye M/Up R70 ml 1 12.50 12.50\n21 SBH.Duty Refill1 S 1 4.20 4.20\n22 Micro SDCard32 GB100 MB/s Class10/Sandiskwith OS 2 24.00 48.00\n23 Lasercutaluminiumplate265 mmx148 mm 1 40.00 40.00\nx1.08(GST)\n"
      },
      {
        "title": "Chargingand Data Transfer Cable-For Smartphones-1 m-",
        "content": "24 2 A 3 2.16 6.48\n25 Pearl&Metallic Paint 1 5.90 5.90\n26 Car Non Slip Mat50 x150 1 8.20 8.20\n27 Aluminium Plate 1 8.00 8.00\n"
      },
      {
        "title": "Content",
        "content": "28 Soft Clay Black 2 2.16 4.32\n29 Soft Clay Black 2 2.16 4.32\n30 HTHex Bolt GR838 DIN933 M10-1.5 x40 Blue ZN 6 0.40 2.40\n31 HTHex Bolt GR838 DIN933 M10-1.5 x90 Blue ZN 13 0.70 9.10\n32 STLWing Nut M10-1.5 NP 19 0.60 11.40\nx1.08(GST)\n33 Switch 6 - 12.50\n34 VCEVEAH024 in1 USB3.0 Hub Black 1 18.90 18.90\n35 Chargingand Transfer Cablemicro B1 m2.4 AAluminium 2 2.16 4.32\n"
      },
      {
        "title": "Chargingand Transfer Cablemicro B1 m2.4 AAluminium",
        "content": "36 Mesh 1 2.16 2.16\n37 7 CFSpray Paint#4 1 5.20 5.20\n38 PGP66014 S4 Way2 MPower Extension Socket 1 16.90 16.90\nOmar's OMWC013 Type C+USBPort20 WWall Charger\n39 Black 4 19.90 79.60\n40 Brints Co.3 DPrinting Services 1 32.00 32.00\n964.81\nAccurateasof16 April2023\n"
      }
    ],
    "metadata": {
      "title": "Electric Vehicle Additive Manufacturing(EVAM)Interior Design",
      "category": "resume",
      "file_name": "Product_Design_Studio",
      "relative_path": "Chin Wei Ming’s Portfolio 28d2a0713ab449fa82d8851c163d4b1f/Product_Design_Studio.pdf",
      "page_count": 53,
      "project_name": null,
      "file_size": 2603967,
      "last_modified": 1749024948.5401397
    },
    "word_count": 5329,
    "page_count": 53
  }
]